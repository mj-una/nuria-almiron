<br><br>

# La Verdadera Revolución:<br>Historia del nacimiento del interface gráfico<br>y la posterior humanización de la informática.

### ¿De dónde salen el PC con Windows, el Macintosh o la World Wide Web de Internet?

### _Núria Almiron_<br>_1998_

<br><br>

## INDICE

[Ir a INTRODUCCION](#introduccion)<br>
La necesaria humanización de la informática

[Ir a CAPITULO 1](#capitulo-1)<br>
¿De dónde sale el ordenador actual?

[Ir a CAPITULO 2](#capitulo-2)<br>
Vannebar Bush y el conocimiento acumulado por la humanidad

[Ir a CAPITULO 3](#capitulo-3)<br>
Douglas Engelbart y el marco de trabajo de los obreros de la información

[Ir a CAPITULO 4](#capitulo-4)<br>
Ivan Sutherland y el Sketchpad

[Ir a CAPITULO 5](#capitulo-5)<br>
Ted Nelson y el concepto del hipertexto

[Ir a CAPITULO 6](#capitulo-6)<br>
Alan Kay y la metáfora del escritorio

[Ir a CAPITULO 7](#capitulo-7)<br>
El PARC o cómo Xerox quiso inventar el futuro

[Ir a CAPITULO 8](#capitulo-8)<br>
Butler W. Lampson y el Alto

[Ir a CAPITULO 9](#capitulo-9)<br>
El Altair y el Homebrew Computer Club

[Ir a CAPITULO 10](#capitulo-10)<br>
Dan Bricklin, Bob Frankston y el VisiCalc

[Ir a CAPITULO 11](#capitulo-11)<br>
El segundo intento de Xerox: El Star

[Ir a CAPITULO 12](#capitulo-12)<br>
Apple y el Lisa

[Ir a CAPITULO 13](#capitulo-13)<br>
De la interfaz gráfica a la interfaz humana: el Macintosh

[Ir a CAPITULO 14](#capitulo-14)<br>
Los grandes olvidados: Atari ST y Commodore Amiga

[Ir a CAPITULO 15](#capitulo-15)<br>
Bill Atkinson y la primera herramienta multimedia: HyperCard

[Ir a CAPITULO 16](#capitulo-16)<br>
Y el resto del mundo cambió: Windows e Internet

[Ir a CAPITULO 17](#capitulo-17)<br>
De la antiinterfaz a la interfaz humana pasando por la interfaz gráfica: Síntesis

[Ir a CAPITULO 18](#capitulo-18)<br>
¿Y después qué? Claves para entender el futuro

[Ir a CRONOLOGIA](#cronologia)<br>

[Ir a BIBLIOGRAFIA](#bibliografia)<br>

<br><br>

## INTRODUCCION

### La necesaria humanización de la informática

_Los ordenadores no sirven para nada, sólo pueden darnos respuestas._<br>
Pablo Picasso

Cuando nacieron los primeros ordenadores, que más bien debiéramos llamar simplemente "calculadoras", estos no disponían de pantallas ni monitores a través de los cuales controlar sus acciones (que en los albores de la informática eran más bien pocas). Estas máquinas iniciales eran programadas y utilizadas mediante resortes y palancas externas, en un principio, y mediante tarjetas perforadas, después ­la interacción mediante teclados y pantallas a partir de códigos y comandos no llegaría hasta un estadio posterior. Fue durante esa primera generación de ordenadores que a los operadores informáticos se les adscribió ese áurea de misticismo y admiración característica de los expertos en profesiones cuya comprensión desborda al ciudadano común, áurea que aún arrastran actualmente en ciertos ámbitos. Y no era para menos, programar y utilizar esos primeros aparatos no estaba al alcance de cualquiera y durante muchos años seguiría sin estarlo.

Cuando nació la informática personal, hace escasamente dos décadas, los ordenadores aun arrastraban una complejidad y dificultad heredada de las anteriores etapas y no se erigían como las herramientas precisamente más adecuadas para las necesidades de la mayoría de personas. No sería hasta la aparición de las primeras interfaces gráficas que la cosa daría un gran vuelco. La interfaz es el programa mediante el cual el usuario se relaciona con la máquina, es algo así como la cara visible del sistema operativo, del software básico que hace funcionar al ordenador. Una interfaz es de hecho una forma de comunicación. Las primeras interfaces no tenían nada de gráficas y sólo funcionaban introduciendo la correcta cadena de caracteres a cada momento necesaria para realizar una acción. Pero de esos complejos comandos de las primeras pantallas de fósforo verde poco a poco se pasaría a sistemas gráficos, con iconos y menús con todas las opciones a vista, y mucho más intuitivos.

Sin embargo, la interfaz gráfica del usuario (IGU o GUI si utilizamos las siglas inglesas) arrancó con lentitud en un mercado dominado por los sistemas no gráficos. Cuando apareció el primer ordenador con algo parecido a un IGU, la informática no intuitiva estaba demasiado asentada y este fue uno de los motivos por los cuales, a pesar del potencial demostrado por el primer sistema gráfico que se comercializó, muchos no le dieron ninguna credibilidad. De hecho, la primera reacción fue considerarlo una mera banalización y simplificación de lo que se consideraba eran sistemas potentes y con muchas posibilidades. Los más reacios le auguraron pocas probabilidades de supervivencia y la comunidad mundial de usuarios de informática hizo suya la opinión de la mayoría de la industria. Ésta se limitaba a defender un estadio de evolución consolidado, y a conservar una cultura informática que la respetaba precisamente por los méritos que defendía: el poseer unos conocimientos fundamentados en la complejidad de los sistemas informáticos. La perspectiva de unas máquinas sencillas pero potentes desmontaba buena parte de los mismos cimientos de esa cultura informática nacida para y por la industria, y los que en ella trabajaban o de algún modo estaban relacionados con ella tuvieron sus lógicas reticencias desde un principio, reticencias que sorprendentemente no han desaparecido del todo de ciertos reductos profesionales. Algo que, por otro lado, es comprensible, pues lo que estaba proponiendo ese nuevo concepto de informática barría de un manotazo los miles de horas dedicadas al tortuoso aprendizaje y control de esos sistemas informáticos, y lo que es peor, los desproveía de su principal valor, la exclusividad y omnipotencia sobre el manejo de esas máquinas, descentralizando su poder sobre ellas.

Mientras tanto, la mayoría de ciudadanos contemplábamos perplejos estos cambios. Pero no se tardaría mucho en reconocer de forma generalizada la bondad de todas esas novedades, especialmente en nuestro beneficio, porque el mayor logro de la "humanización" de los ordenadores ha sido precisamente el poner al alcance de cualquier persona un sinfín de potencialidades que hasta ese momento sólo estaban disponibles para unos pocos. La aparición de estos sistemas gráficos más intuitivos democratizó, en síntesis, la informática en general y abrió el camino hacia un mayor aprovechamiento de las nuevas tecnologías de la información.

### Una revolución dentro de otra revolución

Si el nacimiento de la microinformática ya fue de por sí toda una revolución que desembocaría en el ordenador personal, es decir, en la normalización de la idea de que los ordenadores también podían ser una herramienta de uso individual, fue, sin embargo, la aparición de la interfaz gráfica lo que realmente la hizo avanzar. De esta idea surge este libro, porque la humanización de la informática que los entornos gráficos han permitido ha sido lo que ha llevado a dar este gran salto de lo inaccesible y alejado a algo de uso cotidiano por parte de millones de personas. Podríamos decir que es la verdadera revolución de esta nueva era de la información.

Esto no quiere decir, sin embargo, que actualmente no sigan coexistiendo sistemas operativos diversos, algunos de los cuales no tienen nada de intuitivo y más bien parecen pruebas de resistencia cada vez que nos enfrentamos a ellos. Sistemas que siguen siendo utilizados por cientos de miles de personas, en algunos casos reticentes al cambio, en otros reticentes a tirar a la papelera tantos años de aprendizaje y experiencia acumulada o a perder esa parcela de poder que nos otorga el saber utilizar algo difícil, oscuro, complicado. No por nada, hasta hace bien poco, el "saber informático" era algo muy valorado en nuestra sociedad, algo que confería prestigio y reconocimiento social y es lógico que los poseedores de tal exclusiva no siempre vieran con buenos ojos la pérdida de esta categoría. Sin embargo, la fuerza de esta nueva visión de la informática moderna debía ser notable pues logró ir enraizando poco a poco en el sector, y a pesar de enfrentarse a la mayoría de valores de una industria más que consolidada, consiguió decantar ésta a su favor elevando ese "saber utilizar las tecnologías de la información", como bien podríamos definir al "saber informático", a un ámbito de uso y consumo masivo.

Con todo, ello no significa ni mucho menos que se haya alcanzado el estadio ideal en lo que a sistemas operativos concierne. Aun queda un larguísimo camino por recorrer, posiblemente mucho mayor que el transcurrido hasta ahora. El mecanismo de interacción con las máquinas aún dista mucho de ser perfecto. Los programas, la configuración de las máquinas, la compatibilidad entre sistemas, el lenguaje con el que nos dirijamos a ellos, todo tiene que simplificarse muchísimo más. Estamos sólo en los albores de los sistemas informáticos intuitivos o, como dicen los americanos, de los sistemas "human-centered", centrados en el ser humano, en las personas. Pero el gran salto que se dio a mitad de la década de los ochenta bien merece un análisis porque a él le debemos la forma en cómo trabajamos hoy en día y el haber pasado de tener simples máquinas "ordenadoras" encima de la mesa a tener herramientas que estimulan nuestra imaginación.

### El ordenador como medio, no como fin

En el actual escenario informático de euforia de Internet e idolatría universalizada a Bill Gates, nadie recuerda mucho más allá de Windows y el Macintosh o de la denostada IBM. Pero ni Bill Gates ha inventado la interfaz gráfica ni Apple fue la artífice de todos los cambios que se produjeron en el emocionante paso de una informática anticuada a otra de moderna (y como todo el mundo sabe, ni IBM ni ninguna gran corporación de los años sesenta y setenta tuvo nada que ver con la revolución de la informática personal. En 1977, el presidente de Digital, Ken Olson, aún se permitía afirmar, por ejemplo, que no veía "ningún motivo por el que nadie quisiera tener un ordenador en casa") . Sin embargo, instituciones como el SRI (Standfor Research Institute) o el PARC de Xerox fueron a unirse al trabajo de muchos pioneros en este campo y es a todos ellos que va dedicado este libro. A todos los que individualmente o en grupo han transformado la informática desde que nació hasta nuestros días para convertirla en algo mucho más intuitivo e imaginativo de lo que nunca se creyó posible. Desde que hace cincuenta años naciera el primer ordenador, o lo que se ha considerado mayoritariamente como primer ordenador, el ENIAC (siglas en inglés para "Integrador Numérico Electrónico y Ordenador"), hasta nuestros días, la industria informática ha pasado por muchas fases y etapas, algunas para llevarla por derroteros complejos y alejados de los intereses de los usuarios, otras para hacerla cada vez más accesible al común de los mortales. El gran éxito de ello ha sido el triunfo de estas últimas tendencias. Por ello, estas páginas no son ninguna historia de la informática, ni siquiera de la informática personal, porque obvian todos los pasos encaminados a complicar la informática, sino que se trata de un intento de hacer justicia con el trabajo de todos los investigadores, científicos, genios, o simplemente aficionados que, con sus ideas, y en muchos casos, con su empeño, ayudaron a crear una informática personal, primero, y una herramienta intuitiva después.

Gracias a todos ellos, la informática es cada vez más un medio y no un fin, una herramienta de la que no es necesario ser experto en su funcionamiento como tampoco lo somos en el funcionamiento del automóvil que conducimos, de la licuadora que nos hace los zumos por la mañana o del teléfono que utilizamos a diario. Como toda herramienta, lo importante es lo que vamos a hacer con ella, no cómo funciona sino para qué la vamos a utilizar; en el caso del ordenador bien sea para desarrollar una tesis científica, para planificar una campaña de márqueting, para dibujar los planos de un edificio, para crear obras de arte, para imprimir facturas o para crear un complejo sistema de dosificación de medicamentos. Nuestro tiempo y conocimientos los podemos dedicar al objetivo no a la herramienta. Ello no significa que no deban coexistir con nosotros profesionales dedicados a crear este "medio" ­los diseñadores de hardware, de circuitos, de periféricos y de programas­, siempre tiene que haber alguien que entienda de coches, licuadoras y teléfonos para poderlos fabricar y reparar­ sin embargo, la mayoría de personas los utilizamos sólo como un medio y nuestra profesión es otra, de ahí la importancia de poseer ordenadores que sean simples herramientas.

Todo esto, que puede parecer una obviedad, ha necesitado recorrer un largo trecho para admitirse como tal y llegar hasta donde nos encontramos actualmente. Y aunque aún queda mucho por hacer, podemos felicitarnos porque la dirección hacia sistemas centrados en las personas y no en las máquinas parece irreversible, y esto es algo que no estaba nada claro hace tan sólo una docena de años.

De los primeros mecanismos de cálculo manual utilizados desde hacía siglos, a los posteriores sistemas con tarjetas perforadas que permitían manejar grandes volúmenes de datos, pasando por los primeros grandes ordenadores funcionando con tubos de vacío, hasta a mediados de este siglo con el invento del transistor que permitiría el nacimiento de la microinformática, se ha producido una revolución en la tecnología informática que ha permitido el despegue de la actual era de la información. Pero dentro de ella se produciría otra revolución, muy posiblemente la verdadera revolución digital, la que será recordada como causante del mayor cambio acontecido en este siglo que estamos abandonando.

Fin introducción.<br>
[Volver a Indice](#indice)

<br><br>
 
## CAPITULO 1

### ¿De dónde sale el ordenador actual?

_El avance de la innovación ha vestido a la humanidad de poderes con los que hace un siglo ni la imaginación más audaz hubiera soñado_<br>
Henry George

Para comprender la envergadura del cambio que se ha producido en los últimos veinte años y, principalmente, en esta última década, nada mejor que recordar brevemente de donde venimos para poder valorar donde estamos.

### Del Ábaco a las tarjetas perforadas

Los ordenadores, tal y como los conocemos ahora, tienen sus primeros precursores en las calculadoras. De hecho, el primer ordenador del mundo puede considerarse que fue el Ábaco, el rudimentario tablero con cuerdas y bolas que se hacían pasar de un lado a otro y que resolvían operaciones aritméticas simples si se conseguían memorizar una serie de reglas de "programación". ¡El ábaco se utilizó durante más de 2000 años! Fuentes diversas consideran también que fue en 1642 cuando Blaise Pascal construyó la primera calculadora "digital", sólo era capaz de sumar pero en 1791 Wilhelm Gottfried Leibniz inventó un modelo que además de sumar ya era capaz de multiplicar. Los prototipos de estos dos autores no se utilizaron prácticamente en su momento pero inspirarían, más de un siglo después, la creación de la primera calculadora mecánica que fuera un éxito comercial. La construyó Tomas of Colmar en 1820 y podía sumar, restar, multiplicar y dividir.

### Charles Babbage y las primeras calculadoras mecánicas

Pero todos los avances en este campo culminaron en las máquinas diferencial y analítica de Charles Babbage. Charles Babbage es considerado por muchos como el "padre de la informática", un precursor de las ciencias informáticas no sólo por haber diseñado estas máquinas pioneras sino también por establecer los conceptos teóricos en que se basa actualmente la arquitectura informática. Babbage partió de la idea de construir una máquina capaz de recopilar las tablas de logaritmos, que en esos momentos no superaban el siglo de antigüedad. Así, en 1823, Babbage, gracias a una subvención del gobierno británico, pudo construir un primer artefacto completamente automático. Este matemático e ingeniero se dio cuenta que la mayoría de operaciones matemáticas que se requerían eran repetitivas y que podían realizarse automáticamente. Su primer modelo funcionaba a vapor, imprimía las tablas resultantes y estaba controlado por un programa de instrucciones fijo. Esta máquina, que Babbage denominaría "diferencial", sería abandonada sin terminar en 1833 por su siguiente proyecto, la máquina analítica, lo que algunos han catalogado de primer ordenador digital mecánicamente automatizado y completamente controlado mediante programas. El diseño previsto para este mecanismo analítico poseía cinco características cruciales de los ordenadores modernos: un dispositivo de entrada de datos, un dispositivo de almacenamiento para almacenar los números que iban a ser procesados, un procesador o calculadora, una unidad de control para guiar los procesos y un dispositivo de salida. La máquina analítica de Babbage estaba diseñada para recibir datos de hasta cincuenta dígitos y dar resultados de hasta 100 dígitos en formatos impreso y gráfico (todo ello cuando aún faltaban cincuenta años para que se inventaran las máquinas de escribir) además de en tarjeta perforada , tarjetas perforadas que supondrían un paso adelante hacia la informática automatizada especialmente a partir de 1890 con Herman Hollerith y James Powers. Un artilugio como este desgraciadamente era imposible de conseguir con los medios de la época y sin la ayuda de la electricidad ni de la electrónica, pero sus principios básicos han sido considerados como precursores de las calculadoras digitales que precederían a los ordenadores.

### De las calculadoras mecánicas a los aparatos electrónicos

A finales de 1930, las calculadoras (u ordenadores) con tarjetas perforadas alcanzaron cierto grado de consolidación. IBM por ejemplo dedicó grandes esfuerzos y capital a esta tecnología que en esos momentos ya no era precisamente incipiente sino más bien un legado del siglo anterior. También por esa época, Vannevar Bush (al que posteriormente reencontraremos en este libro) construyó un integrador diferencial en el MIT (Massachusetts Institute of Technology), pero se trataba de un paso intermedio, electromecánico, insuficiente para las necesidades que tendría la sociedad en esos años.

La segunda guerra mundial sería el detonante que impulsaría muchos descubrimientos y la informática no sería una excepción. Todas las calculadores ­llamémosles ya a partir de ahora ordenadores­ que existían hasta el momento eran completamente insuficientes para conseguir el nivel de cálculos que la industria militar necesitaba para sus sistemas armamentísticos (cálculos balísticos, de trayectoria, etc.). Así, en 1942, J. Presper Eckert y John W. Mauchly de la escuela Moore de Ingeniería eléctrica de la Universidad de Pensilvania se pusieron en contacto con el ejército para desarrollar su propio integrador diferencial que acabaría dando lugar a la primera máquina electrónica de la historia de la informática: el ENIAC (Electronic Numerical Integrator And Computer). Este primer ordenador electrónico marcaría un punto y a parte en la historia de los ordenadores. A partir del ENIAC y hasta ahora, hemos avanzado en cincuenta años tecnológicamente mil veces más que lo hiciéramos en los dos siglos anteriores.

### El ENIAC o la primera generación de ordenadores

La fabricación de un ordenador electrónico no fue posible hasta que se dispuso de tubos de vacío, inventados en 1904. También denominados válvulas al vacío, se trata de unas lámparas de vidrio con filamentos en su interior que, al calentarse, producen electrones. Al principio, los tubos de vacío se utilizaron como amplificadores en los primeros ordenadores o, por ejemplo, en las radios. Pero a principios de los años cuarenta se aprovechó su capacidad de transmitir o bloquear la corriente eléctrica para representar el código binario de los ordenadores funcionando así como meros interruptores de encendido donde 0 no deja pasar la corriente y 1 sí (apagado/encendido). Su primera implementación efectiva con esta función sería en el ENIAC en 1946.

La finalidad del ENIAC, el objetivo para el cual nació, era automatizar los cálculos de la artillería durante la guerra, por ello, la precisión y fiabilidad en los mismos debía ser muy elevada. Se trataba en realidad de una calculadora gigante (no almacenaba ni programas ni datos) cuya construcción no terminó hasta después de la guerra pero que el ejercito norteamericano utilizaría durante muchos años. El proyecto puede considerarse que fue todo un éxito. A pesar de la dificultad de su manejo, esta máquina electrónica era mucho más rápida que las anteriores disponibles (básicamente los artilugios electromecánicos que funcionaban con tarjetas perforadas y que se habían estado utilizando durante cuarenta años antes de que apareciera el ENIAC). Aunque también era una máquina de treinta toneladas con más de 18.000 válvulas electrónicas y kilómetros de cables que había que desenchufar y volver a enchufar para cada tipo de operación, algo terriblemente engorroso y que, por increíble que parezca, funcionaba. De hecho, el ENIAC funcionó a la perfección para lo que se había creado (aunque el mantenimiento del ordenador era muy costoso por lo mucho que se calentaba y las constantes explosiones de los tubos al vacío). El ENIAC fue operativo de 1946 a 1955, utilizaba tarjetas perforadas y convenció al mundo de la viabilidad de los ordenadores electrónicos. Y de todo esto hace sólo cincuenta años.

### Aparece el transistor y los circuitos integrados

Desde la aparición del ENIAC, la industria informática ha sufrido grandes avances cuya importancia ha ido incrementándose con el paso de los años. Porque, si bien es cierto que en estos últimos cincuenta años el desarrollo ha sido inmensamente superior que en los doscientos anteriores, igual progresión puede aplicarse a estos últimos diez años. Desde el primer ordenador automático electrónico hasta los actuales sistemas informáticos, hemos vivido una evolución que no siempre parecía estar a favor del individuo usuario. La reducción del tamaño de los ordenadores y el aumento de su velocidad de cálculo son dos de las principales características del desarrollo informático a partir del ENIAC.

Cuando en 1947 William Shockley de los Laboratorios Telefónicos Bell desarrolló el transistor, se estaba abriendo la puerta a la microinformática. Los transistores son pequeños bloques de material semiconductor, como el silicio, que pasaron a sustituir a los tubos de vacío de la primera generación de ordenadores. Ello permitió reducir el tamaño y evitar el sobrecalentamiento de las máquinas consiguiendo fabricar ordenadores más pequeños y más baratos. Cuando se consiguieron imprimir varios transistores en un mismo trozo de silicio nació el circuito integrado, los coinventores del circuito integrado serían Robert Noyce y Jack Kilby, curiosamente trabajando en empresas separadas.

Pero la primera gran revolución dentro de esta joven informática se produciría al inventarse un circuito integrado llamado microprocesador a principios de los años setenta y que haría efectivo el concepto ya avanzado por algunos del "ordenador en un chip". Hasta esas fechas no puede hablarse aún de la existencia de ordenadores personales, lo dominante eran los miniordenadores (que recibían este nombre por su menor tamaño con respecto a los anteriores mastodontes con tubos al vacío). De hecho, toda la informática realizada durante esos años sería completamente inaplicable a las necesidades de las personas individuales en tanto en cuanto estaba enfocada completamente hacia el mercado corporativo, militar o académico. El microprocesador, nacido en Intel en 1971, permitiría dar el gran salto. A principios de los 70, los ordenadores seguían aún utilizándose exclusivamente como calculadoras y las máquinas de escribir de IBM y las fotocopiadoras de Xerox llenaban el hueco restante, pero cuando en 1972 apareció el primer chip capaz de procesar 8 bits, suficiente para representar los números y letras del alfabeto, se abrieron de par en par las puertas del futuro para el ordenador personal.

### La cuarta generación de ordenadores

Con la posibilidad de colocar todos los circuitos en un sólo chip y la aparición de los microprocesadores capaces de manejar números y letras se produjo la primera gran revolución: el nacimiento de la informática personal. En esta cuarta generación de ordenadores se inscriben el Altair, el EMC II, el Tandy TRS-80, el Atari, el PET de Commodore, el Apple II, el PC de IBM y el Macintosh como principales artífices de los posteriores avances tecnológicos. En esta cuarta generación tendría explosión la era de la informática personal, la era del PC (Personal Computer), por más que estas siglas en un principio sólo se identificaran con una plataforma concreta. La era de la informática personal revolucionaría a un mundo que, sólo unos años atrás, veía inconcebible la posibilidad de tener en cualquier hogar una de esas máquinas antaño voluminosas, caras y cada vez más potentes. Pero dentro de esta revolución se produjo una revolución aún mayor, presente desde muy pronto pero sólo consolidada hasta más recientemente.

En la era de la informática personal, el usuario no ha sido siempre el principal punto de referencia. Cuando aparecieron los primeros ordenadores personales estos no estaban pensados para el gran público sino para los locos de la electrónica, los amantes de las nuevas tecnologías y del "bricolage informático". Sólo los fanáticos de los ordenadores ­aquellos que estaban dispuestos a montarse físicamente su propio ordenador y a aprender los lenguajes que fueran necesarios para comunicarse con él­ se volcaron a comprar los primeros ordenadores. Los usuarios de aquellos tiempos nada tenían que ver con los usuarios actuales, cómodamente pertrechados tras sus máquinas repletas de programas y de instrucciones para utilizarlos, aquellos primeros usuarios tenían más curiosidad y avidez por la tecnología que miedo a los imponderables, eran una especie de "aventureros" tecnológicos. No podía ser de otro modo porque los primeros productos que se comercializaron no eran aptos para otro tipo de público. Por otro lado, en la informática personal, ha habido diversas tendencias y en un principio llegaron a imponerse entornos nada intuitivos. El sistema operativo DOS, heredero directo en muchos aspectos del primitivo CP/M, dominante en los años setenta hasta que llegó el sistema de Microsoft, se extendió como la pólvora gracias a la inteligente política de esta empresa al regalar una copia de este sistema con todos los PCs y aun hoy en día conserva una gran porción del mercado, especialmente en los países subdesarrollados donde sigue vendiéndose profusamente. Algunos otros fabricantes se atrevieron también con sus propios sistemas operativos pero con muy poco éxito en general o con un éxito, en el mejor de los casos, efímero. En resumidas cuentas, dentro de esta revolución de la informática personal no todo fue tan revolucionario. Pero las ideas de algunos y la pericia de otros combinada con muchas dosis de talento, e infinitas horas de trabajo, acabaron por ganarle la batalla a la parte más oscura de la informática, la que algunos han querido representar como el orwelliano "Gran Hermano" que en su momento se asoció incluso con empresas concretas (IBM, durante mucho tiempo). Los artífices de la verdadera revolución de la informática personal son a los que nos dedicamos en las siguientes páginas.

### QUIÉN ES...

### Charles Babbage<br>(Londres, 26 de diciembre de 1792 - Londres, 18 de octubre de 1871)

Hijo de un banquero londinense, fue desde muy joven el propio instructor de álgebra de su padre. Tal era su pasión por el álgebra y las matemáticas que cuando llegó al Trinity College en Cambridge en 1811 sabía mucho más al respecto que sus profesores. Junto con Herschel, Peacock y otros, Babbage fundó la Analytical Society para promocionar la reforma de las matemáticas newtonianas que aún se enseñaban en la universidad. A los veintipocos años empezó a trabajar como matemático especialmente en cálculos de funciones. Fue elegido Fellow of the Royal Society en 1816 y jugó un papel importante en la fundación de la Astronomical Society en 1820. Fue en esa época que Babbage se empezó a interesar en lo que se convertiría en su máxima pasión para el resto de su vida: las máquinas calculadoras.

A pesar de sus muchos logros, el poco apoyo institucional que recibió y las limitaciones de la tecnología de la época le impidieron terminar ninguna de sus máquinas diferencial o analítica lo cual lo convirtió en un individuo frustrado y amargado. A lo largo de toda su vida trabajó en diversos campos y publicó diversos trabajos entre los cuales destacan A comparative View of the Various Institutions for the Assurance of Lives (1826), Table of Logarithms of the Natural Numbers from 1 to 108.000 (1827), Reflections on the Decline of Science in England (1830), On the Economy of Machinery and Manufactures (1832), Ninth Bridgewater Treatise (1837) y la autobiografía Passages from the Life of a Philosopher (1864).

### Y TAMBIÉN...

### Un precursor con un cráter en la luna

Charles Babbage es considerado actualmente como el padre de la informática sin embargo, cuando murió en 1871 a la edad de 79 años en Londres, poca gente conocía su nombre o su trabajo. Sus diseños nunca serían completamente implementados debido a las limitaciones tecnológicas de la época y sus ideas no serían valoradas hasta mucho después. La máquina analítica, por ejemplo, nunca se fabricaría. Pero en 1991, en el bicentenario de su nacimiento, científicos británicos construyeron, siguiendo su segundo diseño y a modo de demostración de lo acertado de sus ideas, una máquina diferencial que funciona y está expuesta en el Museo de la Ciencia de Londres. Hoy en día, incluso un cráter del polo norte de la luna recibe el nombre de Charles Babbage en su honor.

### Made in USA

Charles Babbage era británico pero a nadie sorprenderá descubrir que el 99% de los protagonistas de los que hablaremos a continuación son norteamericanos, algo lógico teniendo en cuenta que fue allí donde se gestó la primera generación de ordenadores fruto del empuje de la industria militar de la segunda guerra mundial. Fue en Estados Unidos donde nació, creció y maduró la cultura del chip cuyo máximo exponente podemos encontrar físicamente en un valle californiano popularmente conocido como el "valle del silicio" o Silicon Valley.
 
Fin Capítulo 1.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 2

### Vannebar Bush y el conocimiento acumulado por la humanidad

Una información, para que sea útil a la ciencia, debe ser continuamente ampliada, debe ser guardada y, por encima de todo, debe ser consultada. (...). Presumiblemente, el espíritu del hombre debería elevarse si puede revisar su turbio pasado y analizar de forma más completa y objetiva sus problemas presentes. Ha construido una civilización tan compleja que necesita mecanizar su información por completo si quiere llevar sus experimentos a su conclusión lógica superando los límites de su memoria y no quedar simplemente empantanado a medio camino.

### Vannebar Bush (As We May Think)

Vannebar Bush es el primer personaje de mención obligada en esta historia. Vannevar Bush es conocido por ser el creador del integrador diferencial, desarrollado en el MIT (Massachusetts Institute of Technology) en los años 30, y que supone un paso intermedio, electromecánico, hacia el primer ordenador electrónico a gran escala que fue el ENIAC (Electronic Numerical Integrator And Computer), desarrollado en 1943 por John William Mauchly para automatizar los cálculos de artillería.

Pero Bush también es especialmente recordado por su visión del futuro. En agosto de 1945, este expresidente del MIT y director en esos momentos de la Office of Scientific Research and Development de los EE.UU., escribió un artículo en el Atlantic Monthly titulado "As We May Think" (que podríamos traducir por "Cómo es posible que pensemos en un futuro") y que cambiaría la forma en cómo abordamos la consulta, el almacenamiento y la recuperación de grandes cantidades de información. En él se preguntaba cual tenía que ser el papel de los científicos a partir de la segunda guerra mundial. La respuesta a ello daría pie a una reacción en cadena que podemos considerar termina con la aparición de los sistemas de interfaz gráficos casi cuarenta años después.

Durante muchos años, la comunidad científica se había dedicado casi exclusivamente a ampliar el poder físico del hombre en lugar de su poder intelectual, de su mente. Bush creía que los científicos debían volver inmediatamente a la tarea de hacer más accesible la increíble cantidad de conocimiento que la humanidad había desarrollado y que seguía acumulando. Para él, el principal esfuerzo de los científicos e ingenieros de la posguerra tenía que ser la transformación de la forma en cómo se procesaba, guardaba y recuperaba la información. Evidentemente estaba hablando de los ordenadores, unas máquinas a las que este científico fue capaz de presuponer muchos otros usos además de los puramente matemáticos. La influencia de Vannebar Bush sería enorme no sólo en aquellos que le conocieron sino también en generaciones posteriores de individuos que tuvieron acceso a sus artículos y escritos.

### Cómo funciona la mente humana

En "As we may think" Bush describía el problema al que se enfrentaba el investigador en aquellos momentos. Según él, la ciencia había permitido el almacenamiento de las ideas y su posterior manipulación para poder asimilarlas y evolucionar gracias a ellas. Sin embargo, la creciente y necesaria especialización del estudioso junto con el ingente caudal de resultados que la ciencia iba acumulando imposibilitaban el acceso, comprensión y mucho menos la memorización de todo este material por parte del hombre. Bush reconocía que los métodos de transmisión y revisión de los resultados de la investigación estaban anticuados y ya no servían a sus propósitos iniciales y que era vital evitar que conocimiento importante quedará en el anonimato (o se retrasara su expansión, como le habían ocurrido a las leyes de la genética de Mendel) por culpa de esto.

Bush era consciente de que la mente humana funciona principalmente por asociación y que, en consecuencia, la forma ideal de trabajar para el ser humano sería mediante pensamiento asociativo. Estaba convencido de que el proceso de selección asociativa podía ser mecanizado y con ello se ganaría una permanencia del mismo que no poseía la mente humana, a pesar de ser ésta más rápida en sus procesos mentales asociativos que cualquier máquina inventada o por inventar.

En su exposición, este científico trazó una visión futura tanto de lo que serían las nuevas máquinas como los nuevos métodos de almacenamiento y consulta de esta información. En cuanto a las primeras, al "hardware", el investigador se quedó muy corto. Su horizonte en este campo estaba forzosamente limitado por la tecnología de la época y por ello fijó el máximo desarrollo en los campos de la fotografía, el facsímil y la televisión, entre otros. Aún así, fue capaz de prever que los sistemas de compresión permitirían aumentar enormemente la capacidad de almacenamiento y que la velocidad de las máquinas se dispararía. Celebre es su afirmación de que la Enciclopedia Británica podría ser reducida al tamaño de una caja de cerillas.

Pero en lo que respecta a sus previsiones relativas a los métodos de almacenamiento y consulta, el software, Bush fue un auténtico visionario y algunas de sus predicciones ni siquiera aún hoy en día han sido alcanzadas (por ejemplo, el reconocimiento de voz o escribir en el ordenador mediante dictado oral son tecnologías aún muy inmaduras). De hecho, "As we may think" implica una forma completamente nueva de acoplar las posibilidades mentales y sensoriales de aprendizaje y de asimilación del hombre a la tarea de desarrollar, integrar y aplicar colectivamente el conocimiento.

Partiendo de la idea de que la mente humana funciona por asociación, se comprende el fracaso de los sistemas de indexado vigentes en aquel momento, basados en clasificar alfabética o numéricamente la información, pues son, según Bush, sistemas artificiales que obligan a funcionar mediante reglas distintas a las de los procesos mentales humanos. La solución estaba pues en intentar, si no reproducir artificialmente estos procesos, si al menos aprender de ellos. En tales procesos, la parte repetitiva es perfectamente mecanizable siguiendo pautas lógicas. La parte creativa, la relativa a la selección de la información y al tratamiento que se le aplica a ésta, era según este científico, la más difícil, sino imposible, de mecanizar.

Así, en un alarde de lucidez, Bush hablaba en su artículo de crear máquinas que interpretaran imágenes, que escribieran al dictado, que entendieran idiomas y que utilizaran enlaces hipertextuales y recuperación asociativa para seleccionar la información de las bibliotecas digitales.

### El Memex o máquina del futuro

Para plasmar su dispositivo del futuro, Bush eligió un nombre al azar, Memex, para explicar la especie de biblioteca o archivo privado mecanizado que utilizaríamos para guardar y recuperar la información. En sus propias palabras, se trataría de una especie de complemento íntimo y ampliado de la memoria humana. Con forma de escritorio, encima de él reposarían una serie de pantallas translúcidas en las que se proyectaría el material para poder ser leído más cómodamente. El Memex almacenaría toneladas de libros, imágenes y todo tipo de información. Bush también fue muy lejos a la hora de explicar cómo el usuario buscaría y recuperaría tal información. Además de un teclado, el Memex tendría hileras de botones y palancas que nos servirían para movernos por la información buscando archivos y documentos. Así, Bush imaginó que podríamos acceder a cualquier información simplemente introduciendo un código y que podríamos ver cualquier documento en una pantalla. Añadir anotaciones o comentarios adicionales no sería ningún problema y podríamos establecer asociaciones libremente entre dos documentos cualquiera y verlos simultáneamente en la pantalla. Lo que estaba haciendo Vannebar Bush en 1945 era describir los procesadores de texto o las bases de datos actuales y, lo que es más importante, el método de indexado asociativo del Memex permitía trazar lo que él denominaba "trails", rutas o caminos personales a través de la información. Estaba inventando el concepto de "navegar" por la información, igual que hacemos actualmente en la World Wide Web de Internet. Aunque el reino por el que navegaría no sería definido hasta dos generaciones después y por una novela de ciencia ficción, Neuromante (1984), de la mano del escritor William Gibson: "ciberespacio".

La visión de Vannevar Bush sería ampliada y llevada mucho más lejos por Douglas Engelbart, un ingeniero naval de radares que tendría ocasión de leer "As We May Think" en una biblioteca. Pero tal vez el que más la representaría sería Ted Nelson, el creador del término "Hipertexto". Xanadu, el sueño de Nelson para implementar el hipertexto habría sin duda sido aprobado por Bush. La idea básica de Xanadu era tratar el conocimiento acumulado por la humanidad como un cuerpo único y proporcionar acceso a él a través de una serie de "links", vínculos o enlaces dinámicos. Bush, Engelbart y Nelson gestarían el concepto de "hipermedia", la noción de acceder y guardar información de forma no lineal.

### QUIÉN ES...

### Vannebar Bush<br>(11 de marzo de 1890, Everett, Massachussets, 28 de junio de 1974, Belmont, Massachussets)

Ingeniero eléctrico que, además de desarrollar el analizador diferencial, supervisó la carrera científica promovida por el gobierno americano durante la 2ª guerra mundial. Después de dirigir una investigación sobre detección de submarinos para la Armada americana durante la 1ª guerra mundial, Bush fue a la facultad del MIT en Cambridge en 1919. A finales de los años veinte él y sus estudiantes construyeron diversos ordenadores analógicos para solucionar ecuaciones diferenciales en investigación de ingeniería. Estos analizadores diferenciales fueron muy utilizados y copiados por universidades y laboratorios industriales y utilizados en América y Europa en ingeniería eléctrica y física atómica hasta que fueron sustituidos por ordenadores digitales después de la 2ª guerra mundial. En 1939 abandonó el MIT para convertirse en presidente del Carnegie Institution, posición que ocupó hasta 1955.

Como director del National Defense Research Committee (1940-41) y de la Office of Scientific Research and Development (1941-45), Bush presidió un programa de desarrollo masivo de armas e investigación científica entre cuyos productos se incluían el radar, la goma sintética y la bomba atómica. Fue autor de muchos libros entre ellos Science, the Endless Frontier (1945) y Modern Arms and Free Men (1949) y de una autobiografía Pieces of the Action (1970).

En octubre de 1995 se celebraron los cincuenta años de la publicación de "As we may think" con un Simposium especial en el que participaron especialistas tan conocidos como Douglas Engelbart, Theodor Nelson, Robert Kahn, Tim Berners-Lee, Michael Lesk, Nicholas Negroponte, Raj Reddy, Lee Sproull y Alan Kay.

### Y TAMBIÉN...

### La anticipación del ciberespacio

Julio Verne preconizó en el siglo pasado que iríamos a la luna. Vannevar Bush fue capaz de imaginar en 1945 que navegaríamos por un ciberespacio y que aquellos mamotretos que valían miles de millones de dólares y sólo servían para hacer cálculos acabarían estando, con un tamaño mucho más razonable, en todos los hogares y nos ayudarían a superar el reto de cómo manipular la gran cantidad de información existente acumulada por la humanidad. De hecho, Bush se atrevería a ir mucho más lejos para preguntarse si algún día no sería posible que el vínculo entre la mente y la información exterior se creara directamente, sin necesidad de utilizar los sentidos como hasta ahora (el táctil para los teclados, el oral para el dictado, el visual para la lectura), si no sería posible transmitir la información directamente al cerebro mediante vibraciones eléctricas.

### Un precursor de mucha terminología actual

Muchos de los conceptos que manejaba Vannebar Bush en los años cuarenta tienen ahora su máximo apogeo con la cultura Internet que se está desarrollando. Nociones como los recorridos a través de la información (los "trails"), las marcas o las entradas secuenciales se corresponderían a los actuales "paths", visitas, links o webs.

## Un inventor avezado

El Memex de Bush estaba basado en la tecnología más punta del momento, las máquinas lectoras de microfilms. Aunque este artefacto no llegaría a fabricarse nunca sí lo fueron otros conceptos de Vannevar Bush como el Bush Rapid Selector que fue desarrollado y comercializado por Kodak y otros fabricantes y que lo encontramos ahora en forma de lectores de microfilms con tiras de películas numeradas en los márgenes.

Fin Capítulo 2.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 3

### Douglas Engelbart y el marco de trabajo de los obreros de la información

_La batalla final ha sido la entablada contra el conjunto prevaleciente de paradigmas. Lo que he intentado implementar pedía un cambio de paradigma y, hasta cierto grado, transforma nuestras vidas. Algunos aspectos de esto serán desagradables para algunos, pero aprovecharse de las oportunidades que ofrece beneficiará en última instancia a la humanidad._<br>
Douglas Engelbart

Douglas C. Engelbart es uno de esos personajes prácticamente desconocidos para muchos pero de cuyos logros nos aprovechamos todos a diario. Engelbart llevaría las ideas de Vannebar Bush mucho más lejos materializándolas en inventos concretos y fantásticos. Engelbart se dio cuenta de inmediato que, si bien la hipermedia anticipada por Bush iba a revolucionar la forma en cómo accedíamos a la información, también iba a ser necesario, sin embargo, un determinado marco de trabajo para estructurar todas las posibilidades con las que nos íbamos a topar. De esta idea se derivaría su concepto de la necesidad de "incrementar el intelecto humano", concepto que no serviría sólo como escenario para lo que su propio nombre indica sino también como estructuración de la futura industria de la informática personal.

Todo comenzó cuando Engelbart se enfrascó en una especie de "cruzada" para aumentar la capacidad humana mediante nuevas tecnologías y nuevas formas de interactuar con ellas. Al igual que Bush, Engelbart se percibió de que el conocimiento acumulado por la humanidad superaba sus posibilidades de manejarlo. La única solución era incrementar la capacidad de pensar de las personas para permitirles aumentar su comprensión y facilitarles la solución de problemas. Así, Engelbart predijo sistemas en los cuales los individuos dispondrían de estaciones de trabajo informatizadas y personales, estarían conectados en red y trabajarían con aplicaciones que les ayudarían a solucionar los problemas derivados de la acumulación del conocimiento. Y todo ello lo imaginó a principios de los años 60, cuando los ordenadores valían cientos de miles de dólares y se trabajaba con tarjetas perforadas.

### Inventando los elementos básicos del ordenador personal...

Doug Engelbart tuvo oportunidad de leer "As we may think" de Vannebar Bush cuando fue enviado a Filipinas como ingeniero de radares a finales de la 2ª Guerra Mundial. Posteriormente, en Berkeley, Engelbart crearía, en 1963, el Augmentation Research Center en el marco del Stanford Research Institute (SRI) y fue aquí donde nacieron los principales conceptos con los que trabajan todos los sistemas operativos gráficos actuales.

"Aumentar, no automatizar" sería el eslogan que guiaría su trabajo a lo largo de los siguientes treinta años. Las herramientas clave que surgirían de ello serían los editores para desarrollar ideas, los enlaces hipertextuales, la teleconferencia, el procesamiento de texto, el correo electrónico y la configurabilidad y programación a la que accedería el usuario. Para ello, Engelbart inventaría el ratón, un entorno de software basado en ventanas, los sistemas de ayuda en línea y el concepto de la consistencia en las interfaces del usuario.

Con el entorno de ventanas Engelbart creó un sistema de funcionamiento por el cual no era necesario disponer de un monitor para cada documento, bastaba con uno solo. Con esta nueva técnica se podrían mostrar múltiples archivos a la vez en la pantalla (algo impensable en los ordenadores de la época). Se trataba de un invento que redefinía por completo la forma de trabajar con la información porque suponía una transformación muy profunda de la manera de relacionarnos con ella.

El invento del ratón, algo que difícilmente nadie habría podido imaginar antes del sistema de ventanas, surgiría fruto de la necesidad de un dispositivo de señalamiento adecuado para seleccionar cosas en la pantalla. La verdad es que antes de llegar al ratón, Engelbart rechazó un sinfín de dispositivos de direccionamiento como joysticks, trackballs, lápices láser, etc. para acabar finalmente eligiendo el ratón como el mejor método para relacionarse con ese nuevo sistema de ventanas recién ideado. Más tarde justificó su elección por su envergadura ­del tamaño de la palma de la mano­, su flexibilidad en los movimientos ­el hecho de quedar en posición a pesar de que levantemos la mano para un ligero descanso­ y la posibilidad de utilizarlo prácticamente sobre cualquier superficie dura. Los primeros prototipos de ratón fueron de madera y disponían de tres botones. El que fuera denominado "ratón" ("mouse" en inglés) tal vez se deba al parecido con estos animales que este artilugio tenía (especialmente con sus tres botones iniciales en el lugar de los ojos y la nariz y el cable trasero a modo de larga cola).

### ... o cómo incrementar el intelecto humano

Pero el Augmentation Research Center de Douglas Engelbart no sólo ofreció estos sorprendentes inventos a la sociedad, también diseñó un sistema pensado para lo que el denominaba "obreros de la información". Un concepto que en los años sesenta sonaba casi a ciencia ficción pero que este científico auguraba se convertiría en algo dominante a finales de los años 70. La noción del hipertexto, fragmentos de documentos enlazados con otros fragmentos de información fácilmente accesibles por cualquier persona no experta, sólo era la punta del iceberg. Engelbart ya había propuesto en 1962 un marco de trabajo conceptual para incrementar el intelecto humano. Para poder poner en práctica sus teorías, este científico consiguió, después de muchos esfuerzos, la financiación necesaria del Departamento de defensa norteamericano a través de la Advanced Research Projects Agency (ARPA) y gracias a ello nacieron importantes ideas además del ratón y las ventanas. En 1968 Engelbart ya tenía a punto su sistema denominado NLS (oN Line System) que incluía, además de un sistema de ventanas operado con un ratón, avanzadas prestaciones como correo electrónico, conferencias mediante ordenador y procesamiento de texto. El NLS era un sistema de trabajo diseñado para permitir que cualquier persona pudiera leer material escrito por otras personas y hacer comentarios y vincularle otros documentos desde cualquier terminal conectada al sistema.

Cuando en 1968 Engelbart hizo la primera demostración pública del NLS, un ordenador con ventanas por entre las cuales se navegaba mediante un teclado y un ratón, el concepto de la informática sufrió un avance irreversible (aunque en aquel momento pocos se dieron cuenta de ello y, como el propio Engelbart ha afirmado, la gente quedaba muy sorprendida ante el NLS pero en ningún modo lo asociaban con su propio futuro).

### El taller del conocimiento o el trabajo en red

Los conceptos sobre la necesidad de ampliar el intelecto humano y el NLS conformaban lo que se denominó en general "Knowledge Workshop" o Taller del conocimiento de Doug Engelbart. La idea era que cualquier persona podía conectarse al sistema a través de cualquier terminal enchufado a él. Una vez dentro del Taller, se podría acceder a todos los archivos propios o compartidos con otros usuarios del grupo. Se podrían consultar los archivos, crear de nuevos, hacer anotaciones sobre archivos compartidos y se podrían enviar mensajes de un usuario a otro. Los documentos se transferirían así fácilmente de unos a otros sin necesidad de papel y la transacción sería prácticamente inmediata. Todos podrían pasar documentos a los demás y compartir con ellos los documentos específicamente designados para ser compartidos.

A cualquier usuario de ordenador actual que trabaje conectado a una red le será más que familiar todo lo descrito anteriormente. Y es que el término "trabajo en grupo" o "groupware" es prácticamente el mismo actualmente que como lo imaginaba Engelbart. Sólo en un aspecto difiere notablemente y es en lo relativo al papel. El papel queda completamente erradicado en el sistema de Engelbart. De hecho, lo que se pretendía era eliminar toda la ingente cantidad de notas, recordatorios y anotaciones diversas que pululan por los escritorios para acabar extraviándose más pronto o más tarde. Todo se realizaría electrónicamente en su sistema de modo que sería inmediatamente accesible e imposible de perder. Hoy en día el papel aún dista mucho de haber sido eliminado de nuestras mesas de trabajo, a pesar de todas las nuevas tecnologías.

El trabajo en grupo de forma colaborativa sería otra de las nociones de este precursor de la informática actual. Engelbart imaginó lo que ha visto la luz hace sólo unos años y se ha popularizado especialmente ahora con Internet. El trabajo en grupo colaborativo suponía la posibilidad de dos personas de poder trabajar simultáneamente sobre un mismo documento o grupo de documentos electrónicamente. Dos personas, separadas físicamente (bien situadas en distintas partes de un mismo edificio como en distintas partes de una misma ciudad o de cualquier punto del mundo) pero conectadas electrónicamente, podrían así trabajar conjuntamente en un mismo documento y realizar cambios que se harían inmediatamente perceptibles para el otro en su pantalla del ordenador. Todas las comprobaciones, revisiones o discusiones sobre un documento se podrían hacer en tiempo real por disperso, geográficamente hablando, que estuviera el grupo de trabajo. Sorprende descubrir como un concepto que actualmente ha sido presentado por algunos como una gran novedad ya había sido anticipado por alguien hace más de treinta años.

### El concepto de "Augmentation" de Engelbart

Cuando este investigador hablaba de aumentar o incrementar el intelecto humano utilizaba el término "Augmentation". Con ello pretendía definir una situación que debería producirse obligadamente si la humanidad quería salir adelante frente a los nuevos retos que se le planteaban. El mismo Engelbart definía la noción de "Augmentation" como "aumentar las posibilidades del ser humano para enfrentarse a problemas complejos, conseguir la comprensión para satisfacer necesidades particulares e imaginar soluciones a los problemas" Pero Engelbart no sólo pretendía crear nuevas herramientas para ello sino también crear nuevas formas de trabajar con esas nuevas herramientas.

Tal vez para entender lo anterior lo mejor es atender a la idea de "concepto" para Engelbart. Según él, la noción de "concepto" es algo que evoluciona igual que nuestro pensamiento. De este modo, conceptos pasados de moda pueden ser sustituidos por otros nuevos pero, además, él creía que los procesos del pensamiento humano y lo que el denominaba "estructuras de conceptos" no sólo podían ser observados para anotar sus cambios sino que también podían ser aumentados. Exactamente, Engelbart afirmaba que consideramos a los conceptos como instrumentos a utilizar en nuestros procesos mentales pero estos procesos muchas veces pueden ser mejor acometidos si los conceptos se representan como símbolos. Y aquí es donde Engelbart acertó de pleno. La apreciación de que el proceso mental humano capta mejor los conceptos si estos están representados en forma de símbolos nos conduce directamente a la noción de utilizar símbolos en la pantalla del ordenador para interactuar con él y manipular estos mediante un mecanismo con el que podamos "señalar" y "seleccionar": estamos hablando de las interfaces gráficas basadas en iconos y operadas mediante un dispositivo señalador.

Y todo ello con un único fin: conseguir modificar las estructuras mentales para aumentar nuestras posibilidades de comprensión y de solución de los problemas complejos.

Lo que Engelbart estaba pidiendo a gritos era que se diseñasen sistemas hipermedia que funcionasen de acuerdo a cómo funcionan los procesos mentales humanos, es decir, sistemas que funcionasen como funcionamos las personas. Algo que ahora puede parecer tan lógico pero que no había estado en absoluto presente al diseñar los primeros ordenadores (en parte por la dificultad que ello representaba tanto en la programación como en cuanto al hardware). En esto, como en muchas otras cosas, las ideas de Engelbart eran una prolongación de las ideas de Vannebar Bush.

Engelbart era también plenamente consciente de que la mente humana sólo es capaz de trabajar paso a paso y sólo en breves dosis, no a grandes zancadas para entendernos, y que los medios para aumentar el intelecto humano deberían servir para fragmentar los problemas complejos y dividirlos en fracciones más manejables y asimilables para la mente humana.

Por todo ello, incluido su trabajo actual férreamente dedicado a intentar que fabricantes de informática, desarrolladores y usuarios aúnen esfuerzos para desarrollar la tecnología del siglo XXI, Douglas Engelbart ha merecido y merece el calificativo de ser uno de los padres de la informática personal y, muy especialmente, de la interfaz gráfica del usuario. La incesante lucha por crear un entorno de trabajo que aumentara nuestro potencial intelectual de este pionero de la investigación de las relaciones entre ordenador e individuo, le ha valido numerosas menciones y distintivos a lo largo de su carrera aunque, como él mismo afirmó una vez, la batalla definitiva tuvo que librarla contra lo más difícil de cambiar, los paradigmas prevalecientes e incuestionados que tiene toda sociedad.

### QUIÉN ES...

### Douglas Engelbart<br>(Portland, Oregon 1925)

Graduado en ingeniería eléctrica por la universidad del Estado de Oregon en 1948, obtuvo su doctorado en 1955 en la Universidad de California. De 1948 a 1951 trabajó para los laboratorios Ames Aeronautical, de 1955 a 1956 fue profesor asistente en la universidad de California, al año siguiente trabajó como director técnico y presidente de Digital Tech Inc., de 1957 a 1959 fue investigador del Stanford Research Institute donde en 1963 consiguió fondos para fundar su propio centro, el Augmentation Research Center que dirigiría hasta 1977. También fue ingeniero de investigación del Man-Machine Information Systems del SRI Int de 1957 a 1965 y después jefe de programas hasta 1978. De 1977 a 1984 trabajó como científico para la compañía Tymshare Inc y de 1984 a 1989 para McDonnell Douglas Corp. De 1989 a 1990 fue director del Bootstrap Project en la universidad de Stanford con el que aún colabora ahora y cuya función principal es conseguir que los fabricantes informáticos, los desarrolladores y los usuarios finales trabajen conjuntamente en la tecnología que la cambiante sociedad actual necesita. Douglas Engelbart ha dedicado prácticamente toda su vida a investigar la comunicación entre ordenadores y seres humanos y es considerado el padre de la interfaz gráfica. No ha sido hasta estos últimos años que ha obtenido el reconocimiento que se merece por su trabajo por lo que ha recibido numerosas menciones y premios entre los que destacan el Software Systems Award de la Association for Computing Machinery en 1990, el Computer Pioneer Award del Institute of Electrical and Electronics Engineers en 1993 y el Certificado al mérito del Franklin Institute en 1995. Douglas Engelbart es autor de numerosas publicaciones y posee más de veinte patentes entre las cuales cuenta la del ratón que utilizamos todos a diario.

### Y TAMBIÉN...

### La interfaz gráfica del usuario o Graphical User Interface (GUI)

La interfaz es la forma en como el usuario se relaciona con el ordenador. Se trata de un programa que hace de intermediario entre el sistema operativo y el usuario. La interfaz gráfica del usuario se popularizó a partir de la mitad de los años ochenta pero algunos de sus elementos fueron inventados a principios de los años 60 en el laboratorio de Doug Engelbart y con la participación de otros investigadores. El GUI supone una mayor humanización del ordenador por cuanto que éste se controla señalando, trasladando y manipulando pequeños símbolos gráficos o iconos que representan los diversos elementos del mismo: programas, documentos, periféricos, etc.

### Cuestionarse paradigmas incuestionables

En el Augmentation Research Centre (QRC) de Douglas Engelbart nacieron algunos de los elementos más característicos del ordenador personal actual, aunque en aquel momento se consideraban posibilidades más bien excéntricas o demasiado avanzadas: el ratón, las ventanas, el correo electrónico o el concepto de red informática, por ejemplo. Douglas Engelbart tuvo que luchar muy duro para poder conseguir fondos con los que montar su propio departamento de investigación en el marco del SRI y demostrar así la viabilidad de todas sus ideas. Sin embargo, muy pocos compartían y, muchos menos comprendían, sus ideas. En su primer informe presentado al SRI para conseguir apoyo para montar el ARC se le recriminó su falta de claridad al redactarlo: había necesitado veinte páginas y a pesar de ello nadie lo había conseguido entender. A lo cual Engelbart respondió que era muy difícil describir aquello para lo que aún no existían términos. Engelbart siempre ha postulado que en la sociedad se dan por sentado demasiadas cosas y que esta falta de cuestionamiento es nuestra máxima barrera para avanzar.

### El SRI: fuente de recursos humanos para Xerox

Cuando en 1970 el SRI se descompuso, algunos de sus integrantes clave fueron a una compañía llamada Xerox Corporation, empresa que crearía uno de los centros más punteros del desarrollo tecnológico moderno. Engelbart influenciaría muy directamente toda la investigación realizada posteriormente en el Centro de investigación que la empresa Xerox tenía en Palo Alto, aunque nunca sería invitado a participar en ella.

 
Fin Capítulo 3.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 4

### Ivan Sutherland y el Sketchpad

_El software fantástico trasciende el ordenador e infecta el cerebro de las personas._<br>Guy Kawasaki

Cuando más tarde hablemos del creador de la metáfora del escritorio y de la creencia defendida por algunos de que era posible inventar una interfaz informática que pudiesen utilizar las personas normales y corrientes (algo nada asumido hace quince o veinte años), estaremos hablando de unas personas que se vieron enormemente influenciadas por la imaginación y las posibilidades que Ivan Sutherland demostró ideando una sorprendente aplicación: el Sketchpad.

El Sketchpad fue el primer programa de dibujo por ordenador, un sistema gráfico creado mucho antes de que el término "interfaz gráfica" fuera acuñado, y muchas de las ideas que lanzó ayudaron a cambiar la forma de entender la informática y forman parte de los entornos de trabajo que utilizamos mayoritariamente hoy en día. De hecho, Ivan E. Sutherland fue un pionero de la investigación de los gráficos por ordenador y su trabajo ayudó a sentar las bases del desarrollo de la interfaz gráfica del usuario tal y como la conocemos en la actualidad.

### "Sketchpad: A Man-machine Graphical Communications System"

La imaginación y capacidad de visualización de Sutherland permitieron que desarrollara un programa increíble en un momento en que los ordenadores, de cualquier tipo, eran un bien más bien escaso. De hecho, la primera experiencia de Sutherland con un ordenador fue con el SIMON, un ordenador basado en un mecanismo mecánico con apenas capacidad para sumar y para el que Sutherland, en sus tiempos de instituto, diseñaría el programa más largo jamás escrito para él: ocho páginas de código de algoritmos para que SIMON también pudiera dividir. Pero su gran experiencia la tendría posteriormente en la universidad donde tendría acceso al TX-2, uno de los primeros ordenadores en disponer de algo parecido a una interfaz visual.

El TX-2 era un ordenador que el Laboratorio Lincoln del MIT (Massachusetts Institute of Technology) había construido por encargo de las Fuerzas Aéreas, después del anterior prototipo TX-0, para demostrar que los transistores, en aquellos momentos una tecnología naciente, podían constituir la base de los futuros sistemas informáticos. El TX-2 era, incluso para los estándares de la época, una máquina mastodóntica pero ya disponía de un sistema de almacenamiento por cinta magnética, de una especie de teclado para introducir texto, de la primera impresora de Xerox, de cintas de papel para programar y, sobre todo y lo más importante, de una pantalla de CRT (de tubo de rayos catódicos). El TX-2 usaba una especie de lápiz de luz que permitía introducir formas en el ordenador y que junto con la pantalla y una serie de interruptores conformaban la interfaz sobre el que Ivan Sutherland basó el primer sistema de gráficos interactivos por ordenador. Sutherland desarrolló "Sketchpad: A Man-machine Graphical Communications System" en 1963 para su tesis doctoral en el MIT. En un momento en que los monitores con tubos de rayos catódicos eran toda una novedad, Sutherland diseñó un sistema que permitía a los usuarios dibujar puntos, segmentos de líneas y arcos circulares directamente sobre la pantalla mediante el lápiz de luz. Además, los usuarios podía asignar proporciones a todo lo que quisieran y especificar las relaciones entre los segmentos y los arcos. Podía determinarse el diámetro de los arcos, era posible dibujar líneas horizontal o verticalmente y se podían construir figuras a partir de combinaciones de elementos y formas. Las figuras se podían copiar, mover, rotar o modificar de tamaño sin que sus proporciones cambiaran. De este modo era posible crear, manipular, duplicar y almacenar dibujos de gran precisión. Sketchpad también incluía el primer sistema de dibujo mediante ventanas, la posibilidad de aplicar el zoom sobre una área de la pantalla y la capacidad de trazar líneas y curvas perfectas. Se trataba no sólo del primer programa de dibujo desarrollado en la historia de la informática sino, además, de un programa absolutamente avanzado para su tiempo.

Algunas de las ideas por primera vez demostradas en el Sketchpad forman ahora parte de los entornos informáticos que utilizamos mayoritariamente a diario. Ideas como el concepto de estructura jerárquica interna de una imagen representada por ordenador y la definición de esta imagen en términos de subimágenes; el concepto de imagen maestra, el original, y de copias de ella, ejemplos, que no son más que versiones transformadas del original; el concepto de "proporciones" como método para especificar los detalles de la geometría de una imagen; la posibilidad de mostrar y manipular representaciones icónicas de estas proporciones; la posibilidad de copiar tanto las imágenes en sí como sus proporciones; algunas técnicas de construcción de imágenes mediante lápices de luz; la separación del sistema coordinado en el que se define una imagen de aquel en que se muestra; o, por ejemplo, algunas operaciones recurrentes como "mover" o "borrar" aplicadas a imágenes definidas jerárquicamente.

Posteriormente, Sutherland fue pionero, junto con Dave Evans en muchos casos, de la investigación del modelado informático en 3D y de simulaciones visuales, la base para los sistemas gráficos por ordenador, para el diseño asistido por ordenador (CAD) y para los simuladores interactivos de entrenamiento para pilotos ampliamente utilizados en la aviación comercial y militar. De hecho, puede considerarse que Sutherland, junto con su estudiante en aquel momento, Bob Sproull, fue el primero en implementar lo que hoy denominaríamos un sistema de "realidad virtual" (en 1966, siendo profesor asociado en Harvard, diseñó un sistema por el cual los usuarios, ataviados con unos cascos, podían penetrar en una habitación y mirar en todas direcciones una vez dentro. Una sencilla y ruda recreación ­las imágenes eran muy simples­ de la realidad pero que demuestra el carácter avanzado de este investigador.)

Sketchpad no fue un programa pensado para ser distribuido comercialmente. Ni siquiera llegó nunca a ser portado a otro sistema, sólo funcionó para el TX-2, pero fue el primer programa en alejarse de la concepción que en aquel momento se tenía de lo que debía ser un programa informático. Con aquellas primitivas máquinas todo se veía sólo como combinaciones de unos y ceros y muy pocos eran capaces de imaginar las mil y una posibilidades que estos unos y ceros podían significar en forma de imágenes, sonidos o dibujos. Sketchpad sería un de las primeras aplicaciones informáticas en demostrar las posibilidades del ordenador como extensión de la mente humana y su influencia en los que creían en que tales herramientas debían estar al alcance de todos fue muy grande, como veremos más adelante.

### QUIÉN ES...

### Ivan E. Sutherland<br>(Hastings, Nebraska 1938)

Gran apasionado del álgebra y las matemáticas desde muy joven, Sutherland se licenció en ciencias por la universidad Carnegie-Mellon en 1959, en 1960 obtuvo un máster en ciencias en el California Institute of Technology y en 1963 se doctoró en ingeniería eléctrica en el Massachusetts Institute of Technology. De 1964 a 1966 trabajó como director de técnicas de procesamiento de información para la Advanced Research Projects Agency (ARPA) del departamento de defensa norteamericano. Después de ello fue nombrado profesor asociado en Harvard. En 1968 se trasladó a Utah donde cofundó junto con Dave Evans la empresa Evans and Sutherland Computer Corporation mientras impartía clases a tiempo parcial en la universidad de Utah. De 1974 a 1980 trabajó para RAND Corporation y ostentó el cargo de presidente de Ciencias informáticas en el California Institute of Technology. De 1908 a 1991 fue vicepresidente y director técnico de Sun Microsystems.

Es miembro de la National Academy of Sciences, de la National Academy of Engineering, del Institute of Electrical & Electronics Engineers, de Sigma XI y de la Association for Computing Machinery. Ha recibido numerosos premios y menciones como el máster honorario que recibió en 1966 de la universidad de Harvard, el primer Zworykin Award entregado a un destacado y joven ingeniero por la National Academy of Engineering en 1972, el Systems, Man, Cybernetics Society Award en 1975, el primer Steven Anson Coons Award por su destacada contribución creativa a los gráficos por ordenador del ACM SIGGRAPH en 1983, el AM Turing Award de la Association for Computing Machinery en 1988, el Smithsonian Computer World Award y el certificado al mérito del Franklin Institute en 1996.

Actualmente sigue centrado en los gráficos por ordenador, la arquitectura de ordenadores de alto rendimiento, los algoritmos para la ejecución rápida de funciones especiales, el diseño de circuitos integrados a gran escala y los robots. Es autor y coautor de 45 publicaciones y posee una docena de patentes en su haber.

### Y TAMBIÉN...

### Sketchpad: unos y ceros transformados en dibujos

Ted Nelson describió al Sketchpad en su obra The Home Computer Revolution como uno de los programas informáticos más importantes que se habían escrito. La posibilidad de dibujar formas y figuras con un puntero o lápiz en un ordenador demostró la versatilidad que los ordenadores podían llegar a tener con las nuevas tecnologías y, sobre todo, con la imaginación.

### Mucho más que un mero programa de dibujo

Con el Sketchpad se hizo patente la capacidad del ordenador como extensión de la mente humana, como herramienta para fomentar esa imaginación, más allá de la simple máquina calculadora y ordenadora que muchos sólo eran capaces de ver. Ivan Sutherland ideó el primer programa de dibujo, de hecho demasiado avanzado para su tiempo, e influenció profundamente a todos aquellos que lo conocieron y pudieron utilizarlo.

### Ivan E. Sutherland y Sun Microsystems

En 1980, Ivan y Bob Sproull crearon una empresa de asesoría, Sutherland, Sproull & Associates a la que dos años más tarde se uniría su hermano Bert. En 1990, Sun compró la compañía por los talentos y patentes que acumulaba. Esta adquisición se convirtió en el núcleo de los laboratorios de investigación de Sun Microsystems. Actualmente, Sutherland es Sun Fellow y divide su tiempo entre las tareas corporativas de su cargo y la investigación. En estos momentos trabaja en sistemas asincrónicos, un proyecto que pretende romper el molde del pensamiento "tradicional" relativo al diseño informático.

Fin Capítulo 4.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 5

### Ted Nelson y el concepto del hipertexto

_Este es el principio, sólido como una roca, sobre el cual está fundado el éxito de envergadura galáctica de la corporación (IBM)... sus taras fundamentales de diseño quedan completamente ocultas bajo sus taras superficiales de diseño._<br>Ted H. Nelson.

Ted Nelson fue, con mucho, el más acérrimo defensor de las ideas de Bush y durante mucho tiempo uno de los pocos en hablar de la futura industria de la edición electrónica en los años 70. Nelson daría una respuesta al problema del tratamiento del conocimiento acumulado por la humanidad al que se había referido Bush. Más concretamente, puede considerarse que Ted Nelson fue el creador de la palabra "hipertexto", puesto que fue el primero en utilizarla para describir una forma de escritura no secuencial a mediados de los años 60. Sus libros, especialmente Computer Lib/Dream Machines y Literary Machines, sirvieron para influenciar a la siguiente generación de pioneros hipermedia más que ningún otro escrito.

Mucho antes de que este término se pusiera de moda, Ted Nelson ya había afirmado que llegaría un momento en que seríamos capaces de leer y escribir en la pantalla de un ordenador teniendo fácil e instantáneamente a nuestra disposición vastas bibliotecas de información. La clave para acceder a esta información sería el uso de "una escritura no secuencial", que el definió como "hipertexto". Más concretamente, Nelson definía el hipertexto como el "texto que se entrelaza y permite diversas elecciones al lector y que se lee mejor sobre un monitor interactivo. Una serie de fragmentos de texto conectados mediante enlaces que permiten diversos recorridos al lector".

Era cuestión de tratar la información como un único cuerpo al que accederíamos mediante enlaces dinámicos que realizaríamos a nuestra voluntad (los famosos "links"). Esta idea hace unos años tal vez habría necesitado de alguna explicación adicional, pero hoy en día, con la multimedia e Internet en todos los hogares, ello ya no es necesario. Si el lector aún no sabe que es el Hipertexto sólo tiene que probar un navegador como Netscape Navigator y adentrarse en la World Wide Web (para los más neófitos, valga la explicación de que los links nos trasladan a través de la información haciendo clic sobre palabras concretas relacionadas entre si. Un ejemplo sería cuando al leer una definición en un diccionario electrónico podemos consultar los términos que no entendemos de la misma simplemente haciendo clic sobre ellos). La propia concepción de la World Wide Web ha estado profundamente influenciada por las ideas de Nelson.

La noción que Nelson desarrolló del hipertexto a principios de los años setenta era muy rica. En Dream Machines llegó a describir hipergramas (imágenes entrelazadas), hipermapas (con capas superpuestas transparentes) y películas entrelazadas. La pretensión básica de este libro era intentar expresar la interconectividad del conocimiento, algo que el hipertexto puede reproducir y ofrecer. En esta misma obra, Nelson también habla de tres categorías de hipertexto. La primera, el hipertexto básico, hace referencia a lo que denominamos "links" o enlaces de referencia. La segunda categoría, el "strechttext" o texto extendido, es la implementación completa de los links de expansión, para dar más información. La tercera categoría, la colateral, emana de su trabajo en 1971 y nos coloca a dos documentos en pantalla con los cuales podemos trabajar paralelamente en lo que él denomina "versioning". El versionado es la posibilidad de crear y gestionar diferentes versiones de un mismo documento, algo de gran utilidad para documentos técnicos o código de programación. Además, Nelson también distingue entre hiperlibros "frescos" u originales sobre un tema, hiperlibros "antológicos" entrelazando obras diversas y "grand systems", aquellos que incluyen cualquier cosa acerca de un tema, por lejana que sea la relación, compilados por editores y en los que podemos leer en todas direcciones de modo que pueden crearse rutas distintas a través de la información según quién los consulte. Esta visión, como toda la base de sus ideas, le debe mucho a Vannevar Bush (de hecho, en Literary Machines Nelson incluso incluyó el artículo de Bush, "As we may think", en un capítulo).

### Xanadú o el cuerpo único del conocimiento

El sistema en el que Nelson soñaba implementar todas estas ideas lo denominó Xanadú. En Xanadú, el conocimiento acumulado por el mundo sería tratado como un cuerpo único y podríamos acceder a él a través de enlaces controlados por los individuos. Xanadú era visto por Nelson como una red y almacén global de información basada en el concepto de "hipertexto universal" y que consistiría en miles de nodos por todo el mundo que darían acceso simultáneo a miles de usuarios que podrían consultar montañas de información y crear sus propios recorridos a través de ella.

En su momento esta idea daría lugar a lo que sería el primer programa multimedia del mercado: HyperCard. Aún presente en la actualidad y desarrollado por Bill Atkinson a mediados de los años ochenta para el Macintosh, HyperCard sería la primera aplicación en hacer uso de esta idea de interconectar documentos a partir de palabras destacadas que constituirían rutas o caminos invisibles para trazar a través del cuerpo de la información (a él dedicamos un capítulo más adelante).

Pero ¿qué era o, mejor dicho, qué es (pues el proyecto sigue vigente) Xanadú? Actualmente Xanadú podría ser considerado como el mayor vaporware de la historia de la informática (por vaporware se entiende todo software cuyo lanzamiento inminente se ha anunciado repetidas veces sin llegar a aparecer nunca en el mercado) pues Ted Nelson lleva prácticamente treinta años anunciando la aparición de un entorno de software que ponga en práctica su visión. Sin embargo, Xanadú fue y sigue siendo un paradigma que, sólo ahora, con la expansión de la World Wide Web, hemos empezado a percibir como real. El paradigma de Xanadú es el de una estructura unificada de la información a nivel mundial y de acceso abierto y fácil. El paradigma de Nelson prometía, por ejemplo, que cualquier persona podría tener a su alcance la consulta de antiguos papiros de Alejandría, las tabla babilónicas, todas las obras de Einstein y de quién queramos imaginar de modo fácilmente accesible mediante una simple conexión telefónica. Lo que postulaba Nelson era la visión de todo un nuevo tipo de literatura a partir de un sistema de edición y publicación mundial abierto y funcionando bajo hipertexto. Este nuevo concepto de literatura, que el autor ha ido desarrollando y perfeccionando con los años, sería un sistema de ideas interconectadas, la interconexión de todo el conocimiento acumulado por la humanidad desde la existencia del hombre.

### Hipertexto e Hipermedia

Si por hipertexto Nelson entendía escritura no secuencial y, por extensión, un método no secuencial de recuperar la información ­más popularmente, una serie de fragmentos de texto conectados entre sí mediante enlaces y que permiten al lector definir su propio viaje a través de la información­, la hipermedia era hipertexto con animación, sonido y vídeo. Es decir, lo que hoy en día hemos rebautizado como "multimedia", término que, a su vez, no se popularizó hasta después de la consolidación de las interfaces gráficas como sistemas estándar en la informática personal y que hicieron posible que esa multimedia estuviera presente en cualquier ordenador doméstico.

Ted Nelson consideraba que los ordenadores eran las herramientas hipermedia por excelencia pero era muy crítico con determinados conceptos imperantes en su época, como la estructura tradicional de los archivos en el ordenador. Nelson abogaba por terminar con "la tiranía del archivo", aunque uno de los principales objetos de su ira fueron las primeras formas adoptadas por el CAI (computer-aided instruction), enseñanza asistida por ordenador desarrollada a principios de los años sesenta en Estados Unidos. Nelson veía a esos primeros modelos de aprendizaje mediante ordenador como una forma de paternalismo e incluso como una especie de fascismo. Para él, estos primeros modelos pretendían controlar y restringir a los usuarios, que en ningún momento tenían posibilidad de contemplar el todo global, y que imposibilitaba, pues, que siguieran sus propios intereses y crearan sus propios recorridos de información, algo prioritario en el discurso de Nelson.

### Reunificar el conocimiento con herramientas fáciles de usar

Pero al mismo tiempo, Ted Nelson veía al hipertexto y por extensión a la hipermedia, como un marco para reunificar toda la dispersión y confusión generada por los crecientes componentes mediáticos: vídeo, animación y sonido básicamente. De este modo, sería más fácil unificar y organizar el ingente conocimiento del hombre y facilitar su comprensión.

Ted Nelson, al igual que Bush y Engelbart anteriormente, era plenamente consciente del creciente volumen de información al que se enfrentaba la humanidad y del grave problema que de ello se derivaba: cómo manejar, aprovechar y comprender esa complejidad. Para explicar sus soluciones Nelson desarrolló dos conceptos. El primero estaba basado en la idea de un sistema informático destinado a ayudarnos a manejar toda esta complejidad, un mecanismo destinado a ayudarnos a pensar, de ahí el término que utilizaba para describirlo: Thinkertoy. La propia palabra describe su doble propósito: ayudarnos a pensar ("thinker" significa "pensador" en inglés) y ser fácil de manejar ("toy" significa "juguete").

Para Ted Nelson, la característica principal de cualquier sistema informático debía ser, fuera cual fuera la función para la que estuviera destinado, su facilidad de uso. En Computerlib/Dream Machines, publicada por primera vez en 1974, afirmaba que, a la hora de realizar cualquier tarea, utilizar un ordenador siempre debería ser más fácil que no utilizarlo, y que el problema era que faltaba claridad conceptual en el software. El problema no era, según él, técnico sino conceptual y artístico, era de diseño y si los ordenadores eran impersonales, fríos y poco sensibles era porque representaban las decisiones administrativas de burócratas y programadores que a su vez eran fríos y poco sensibles. Además, para Nelson, y este sería uno de los aspectos más destacables de su tesis, la simplicidad de uso no estaba reñida en absoluto con la potencia. Su defensa de la simplicidad llegaba hasta el punto de declarar como inútiles todos aquellos sistemas que fueran más complicados de lo que el denominaba "sistemas de diez minutos", el segundo concepto sobre el que giraba, y gira, toda su filosofía.

Por "sistemas de diez minutos" Nelson entendía aquellos sistemas que podían ser aprendidos por cualquier novato y llevados a una correcta aplicación en menos de diez minutos. Nelson llegó a decir que "la interacción con los ordenadores podía ser diez veces más fácil, diez veces más potente, diez veces más emocionante" de lo que era y que ello "no era una cuestión de hardware sino de diseño virtual". La línea de pensamiento seguida por estos pioneros de la informática humanizada rompería con la informática del "gran hermano" implantada por los estándares del momento.

### QUIÉN ES...

### Ted Nelson<br>(Virginia 1937)

Caracterizado por su dedicación intensiva a los medios de comunicación, Ted Nelson se licenció en filosofía en el Swarthmore College y a los veintidós años ya había trabajado en televisión, en el New York Times, publicado su primer periódico en la universidad, su primer libro e incluso producido un cortometraje de treinta minutos, un disco y un musical de rock. Sin embargo, en 1960, mientras realizaba un máster en sociología en la universidad de Harvard tuvo ocasión de apuntarse a un curso de informática que le abriría los ojos a la posibilidad de crear un nuevo mundo de medios interactivos, una fusión nueva entre literatura y cine completamente nueva. A partir de aquí su vida fue una constante búsqueda de este sueño, al que llamaría Xanadú, y del que aún no ha podido implementarse ningún producto real. De este proyecto Xanadú surgiría la Xanadú Operating Company que llegaría a ser adquirida por la empresa Autodesk (popular por el software AutoCAD) en 1988 quien, después de invertir considerablemente en esta idea, la dejaría languidecer. Nelson ha anunciado repetidas veces (incluso llegó a realizar una gira mundial en 1990) el lanzamiento del sistema que crearía la red abierta y mundial de edición por hipertexto que postulaba Xanadú y que en 1993 adoptó el nombre de Xanadú On-Line Publishing. Ted Nelson ha ido recabando apoyo de diversas fuentes a lo largo de toda su vida para seguir en lo que aún es su empeño principal. Ha escrito diversos libros entre los que destacan Literary Machines y Computer Lib/Dream Machines (en 1974 y reeditados en 1987) e incluso ha publicado vídeos explicando sus ideas. En Internet, el proyecto Xanadú recibe una extensa atención y tiene varias Webs dedicadas a él.

### Y TAMBIÉN...

### ¿Qué es el Hipertexto?

Por hipertexto entendemos la presentación de la información en forma de nodos entrelazados a través de los cuales los lectores pueden moverse de un modo no lineal. Algunos autores vinculan la definición de hipertexto exclusivamente con la tecnología electrónica mientras que otros no la identifican con una tecnología, contenido o medio concreto sino como una forma de organización de la información. Sin embargo, sobre papel no es posible aprovechar todo su significado, algo que sí se logra electrónicamente.

### La popularización del hipertexto

A pesar de la madurez del término "hipertexto", su máxima popularización no le ha llegado hasta más recientemente con la gran expansión de Internet gracias a la World Wide Web, el sistema gráfico de Internet cuya principal característica es la navegación hipertextual (es decir, haciendo clic sobre palabras destacadas que nos conducen a otros documentos distintos interrelacionados). La World Wide Web es heredera de buena parte de los conceptos de Nelson.

### El "gran hermano" frente a sistemas democráticos

Ted Nelson era implacable con la informática que podríamos denominar "del gran hermano", aquellos sistemas que encorsetaban al usuario hasta el punto de convertirse en fines en sí mismos ­conseguir aprender a utilizarlos­ más que en medios ­ser herramientas para ayudarnos a pensar. Tiranía, fascismo o paternalismo eran términos habituales en su vocabulario a la hora de recriminarles su dificultad e irracionalidad. Por contra, Nelson abogaba por sistemas tan sencillos que cualquiera pudiera usarlos pero cuya sencillez no implicaba una reducción de sus capacidades y potencia.

Fin Capítulo 6.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 6

### Alan Kay y la metáfora del escritorio

_Cualquier medio lo suficientemente potente para extender el alcance del hombre es suficientemente potente para derribar su mundo. Conseguir que la magia del medio trabaje en pro de nuestros objetivos y no en su contra es alcanzar la alfabetización._<br>Alan Kay

Alan Kay es uno de esos entrañables y polifacéticos personajes (un apasionado de la electrónica, músico semiprofesional y hábil programador) cuya biografía sorprende no sólo por su talento inventivo sino también por su anticipación y capacidad de visión. A Alan se le conoce especialmente por ser un visionario de la idea del ordenador personal, del concepto de "ordenador de bolsillo" (los actuales PDA o Asistentes personales digitales), y por encima de todo, por haber sido el creador de la programación moderna orientada a objeto y de las interfaces con ventanas que se solapan, base y fundamento de los actuales sistemas gráficos.

Bajo la influencia de muchas de las ideas del Stanford Research Institute de Douglas Engelbart, como el sistema NLS, y del propio Sketchpad de Sutherland, Kay, que empezó intentando diseñar a principios de los años sesenta un concepto propio de ordenador, acabaría ayudando a fundar uno de los más prestigiosos centros de investigación de tecnología punta que jamás haya existido, el Palo Alto Research Center (PARC).

Se dio cuenta nada más llegar a la Universidad de Utah en 1966, donde pudo entrar en contacto con el programa de Ivan Sutherland, que el Sketchpad convertía al ordenador en una extensión más de la mente humana, y en su intento por diseñar un hardware que permitiese ahondar en ello, descubrió que el problema estaba en las interfaces de los sistemas informáticos del momento. Durante los dos años siguientes a su llegada a esta universidad, Alan Kay intentó diseñar un nuevo tipo de hardware que no se llevaba nada en esos momentos: el ordenador personal. A su prototipo le llamó FLEX y hubiera supuesto, de fabricarse, la materialización de las ideas de Engelbart y Sutherland (su desarrollo constituyó su tesis doctoral presentada en 1968). Disponía de una tableta gráfica para interactuar con la interfaz, un monitor de alta resolución para texto, gráficos animados y múltiples ventanas. Pero la interfaz en concreto era lo que se podría llamar todo un "ahuyenta-usuarios". El fracaso en este aspecto le hizo reflexionar profundamente acerca de lo que significaba un "interfaz del usuario". De hecho, según sus propias palabras, la interfaz es un tema que había estado presente desde que el ser humano empezó a inventar las primeras herramientas, pero incluso en máquinas como los ordenadores, cuya interrelación con las personas es muy estrecha, la interfaz era algo que siempre se había tenido en poca consideración. Nadie se había parado a reflexionar cómo construir sistemas de comunicación con las máquinas que fueran amigables e intuitivos, es decir, que no fueran extraños ni complejos para los no profesionales de la informática, que cualquiera pudiera utilizarlos ­reflexión ésta que era toda una novedad, si tenemos en cuenta que en aquel momento muy pocos veían razón alguna para que las personas "normales y corrientes" pudieran nunca necesitar utilizar máquinas como los ordenadores.

### Una interfaz no hostil

La solución para que la interfaz de las máquinas no fuera hostil a las personas era para Kay convertir la pantalla del ordenador en algo familiar, en algo con lo que los usuarios se sintieran cómodos como en su propia mesa de trabajo. De hecho, una de sus visitas a los laboratorios de Inteligencia Artificial del MIT, donde pudo entrar en contacto con los experimentos de Seymour Papert con ordenadores y niños, le hizo llegar a la conclusión de que los únicos ordenadores que valía la pena diseñar eran los que fueran suficientemente sencillos como para que hasta un niño los manejara. Según sus propias palabras, la mayoría de interfaces intentaban simplificar las cosas complejas y lo único que conseguían era volver complejas las cosas más sencillas.

Hasta ese momento, tanto los sistemas de IBM como el complejo código del UNIX, incluso los lenguajes supuestamente más sencillos como el BASIC, eran auténticas torturas (de programar y de utilizar). El problema radicaba en que estaban más enfocados a las exigencias de las máquinas que a los hábitos de los seres humanos que debían utilizarlos. Kay era además muy consciente de la crucial importancia que los lenguajes tenían en los procesos mentales y llegó a la conclusión de que, para conseguir buenas interfaces, lo que hacía falta eran lenguajes informáticos más parecidos a los lenguajes naturales, es decir, más flexibles, que dieran alas a la creatividad y que fueran más agradables para el usuario ­más "simpáticos" o "cordiales" en palabras suyas. La cordialidad y simplicidad de tales lenguajes le hizo afirmar que llegaría un día en que todos los usuarios serían a la vez programadores.

Con este objetivo en mente, este visionario se dispuso a crear su propio lenguaje informático, el SmallTalk, partiendo de algunos de los elementos aportados por investigadores anteriores como Doug Engelbart pero también de una visión propia conferida a lo largo de los años por la experiencia y por su bagaje académico. De su formación científica, Kay había heredado la formulación de una analogía biológica con la que postulaba que el ordenador ideal era aquel que funcionaría como un organismo vivo: cada "célula" se comportaría en consonancia con el resto para conseguir un objetivo final, pero también sería capaz de funcionar autónomamente. Las "células" también podrían ser capaces de agruparse para poder atajar problemas concretos o llevar a cabo funciones adicionales. SmallTalk nacería con este diseño en la mente de su autor, como la cristalización de su modelo biológico con "células" o entidades individuales que se comunicarían entre sí a través de mensajes.

Pero también con respecto a las tesis de Engelbart realizó Kay notables avances en el campo de la interfaz. Más concretamente, el sistema de ventanas que había ideado Engelbart (una pantalla dividida en diferentes ventanas fijas sobre las que se trabajaba alternativamente) conllevaba algunas limitaciones que Kay solventó considerando la pantalla como virtualmente nuestro escritorio, nuestra mesa de trabajo.

### Un escritorio en la pantalla del ordenador

Puede afirmarse que Alan Kay fue el creador original de la "metáfora del escritorio" en el ordenador. La idea era que los usuarios trabajasen con hojas de papel que pudiesen apilar y guardar como en la vida real, que pudiesen escribir, dibujar o leer en la pantalla y que se trabajase con ventanas entre las cuales nos desplazásemos con el cursor del ratón. Al tocar una ventana, ésta quedaría inmediatamente encima de la otra (esta superposición de las ventanas solventaba todas las limitaciones de las ventanas inamovibles en las que se dividía el monitor de Engelbart). No era el único ni el primero en utilizar gráficos para representar las funciones de un ordenador, pero sí fue el que con mayor convicción y éxito lograría llevarlo a cabo.

Kay trabajaría con su equipo durante casi toda la década de los setenta para conseguir el sistema operativo, derivado del lenguaje SmallTalk, que funcionase de esta manera en su nuevo gran proyecto de ordenador, el Dynabook. Y aunque la tecnología de 1971 no estaba aún a punto para poder implementar las ideas de Alan Kay, el SmallTalk y el Dynabook se convertirían en una referencia para todos, especialmente éste último, a pesar de que nunca llegaría a construirse.

No fue este el caso del SmallTalk, que sí llegaría a funcionar en el que es considerado por muchos el primer ordenador personal, el Alto, desarrollado en el PARC. En 1972 Kay empezó a trabajar para al PARC y consiguió convencer a Xerox para fundar un laboratorio dentro de este gran parque de investigación. Su propio laboratorio estaría destinado a crear el mejor prototipo de hardware de Dynabook que la tecnología del momento pudiera lograr. De este proyecto surgiría el Alto y el Learning Research Group , el equipo con el que Alan Kay se dedicaría a investigar la reacción de los usuarios, especialmente niños, ante sus productos. Durante estos experimentos fue cuando descubrió que los niños necesitaban un hardware y software superior que el que necesitaban los adultos, mucho más proclives a aguantar cualquier tipo de sistema e interfaz por horrible que fuera su diseño, y averiguó también que, en general, los más jóvenes aprendían mejor a través de imágenes y sonidos que a través de mero y simple texto. En aquellos momentos, Xerox no supo ver lo que todo esto significaba para el desarrollo de la informática personal, y a pesar del éxito que las características de su lenguaje e interfaz tenía entre la población infantil de sus pruebas, Kay sólo logró despertar verdadero interés en Steve Jobs y el grupo de ejecutivos de Apple que poco más tarde aparecerían por el PARC.

Alan Kay ha afirmado en ocasiones que, si bien la metáfora de la manipulación y la gestión se empezó a aplicar muy pronto a la comunicación entre hombre y ordenador ­a partir de los años cincuenta inmediatamente después que los científicos se dieran cuenta de que el ordenador era algo más que una máquina calculadora­, sin embargo se ha avanzado muy poco en la investigación de cómo funciona realmente la mente humana. Por más que es cierto que poseemos el mismo nivel de inteligencia que hace cientos de años, el contexto que nos envuelve es lo suficientemente potente como para ampliar muchísimo más nuestra capacidad de pensar. Para Kay, lo primordial era y es aprender más sobre nuestra mente para poder diseñar interfaces que se adapten a la psicología humana y de este modo obtener resultados que consigan simplificar y facilitar la creciente complejidad de la tecnología y de los seres humanos.

Su metáfora del escritorio ­que en su concepción inicial se limitaba a la utilización de hojas y ventanas y que en el PARC se ampliaría después a una metáfora de la oficina (con iconos representando carpetas, impresoras, archivadores y por supuesto papel)­ se extendería más allá para crear el concepto moderno de desarrollo orientado a objeto y se revelaría como la clave para hacer accesible la informática al resto del mundo.

### QUIÉN ES...

### Alan Kay<br>(Springfield, Massachusetts 194?)

Alan Kay es un personaje con talento demostrado en los más diversos ámbitos. Hijo de músicos y artistas, especialmente por parte de su madre, Kay llegó a plantearse el vivir de la música profesionalmente hasta que descubrió su habilidad con la programación informática a principios de los años sesenta, habilidad que le permitió pagarse sus estudios en la Universidad de Colorado. Allí se licenció en matemáticas y biología molecular en 1966, año en el que, al terminar sus estudios, fue aceptado en la Universidad de Utah donde se doctoró en ciencias informáticas en 1969 gracias a su tesis sobre el desarrollo del primer ordenador personal gráfico orientado a objeto. Estando en la universidad de Utah formaría parte del equipo de investigación del ARPA que desarrollaría los gráficos en 3D, grupo que más tarde desarrollaría la ARPANET (que se convertiría en la actual Internet) con la que también colaboraría Kay. Después se uniría al proyecto de inteligencia artificial de la Universidad de Stanford y en 1970 lideraría unos de los diversos grupos de investigación que fundarían el Palo Alto Research Center de Xerox donde permanecería más de diez años. A continuación Kay trabajó como científico jefe en Atari durante tres años hasta que en 1984 se convertiría en Apple Fellow, un consultor y colaborador de la compañía Apple Computer para la que exploraría nuevas ideas de cara al futuro. Hace unos años, pasó a colaborar con Walt Disney de quién se convirtió en Disney Fellow.

Kay ha dedicado estos últimos años a la investigación y el desarrollo del uso de la tecnología en la educación para conseguir crear entornos de aprendizaje de mayor calidad tanto para niños como adultos y especialmente para comprender nuevas y mejores formas de extender, capturar, transmitir y pensar las ideas a través de medios informáticos. Es miembro de la American Academy of Arts and Sciences, de la Royal Society of Arts y del World Economic Forum y ha recibido diversas menciones como el ACM Software Systems Award y el J-D Warnier Prize. A sus múltiples facetas artísticas últimamente ha añadido su afición al órgano clásico.

### Y TAMBIÉN...

### La metáfora del Escritorio

Alan Kay ideó "la metáfora del escritorio" buscando una manera de eliminar la hostilidad de la interfaz de los ordenadores de principios de los años setenta. Toda su filosofía giraba en torno a su experiencia en la educación con los niños, experiencia que le había revelado la posibilidad de interactuar con los ordenadores con algo más que mero texto para conseguir mejores resultados. La aplicación de gráficos y sonidos a la pantalla del ordenador sería una consecuencia lógica derivada del mayor nivel de aprendizaje que los niños consiguen con los objetos gráficos frente a los objetos textuales. El PARC llevaría aún más allá su concepción creando lo que algunos llaman "la metáfora de la oficina" que tendría su máxima representación en un ordenador llamado Star.

### El libro dinámico

Dynabook fue el nombre que Alan Kay dio a su ordenador ideal y que definió como "un medio dinámico para el pensamiento creativo". Se trata de una concepción muy emparentada con el concepto actual de "ordenador de bolsillo" portátil y manejable, los PDA o Asistentes personales digitales que no han visto la luz hasta hace unos pocos años y cuyas características básicas ya trazó Alan Kay en 1968. El diseño del Dynabook, del tamaño de un libro, es una idea que su autor parece que aún no ha abandonado. En una entrevista en 1991, Alan Kay afirmaba su intención de poder llegar a fabricarlo y comercializarlo masivamente y en el que aplicaría la noción de "sistemas basados en agentes", ordenadores que no sólo se adaptarían a las necesidades de sus usuarios sino que aprenderían de ellos.

### El Learning Research Group del PARC

A principios de los años setenta, Alan Kay fundó el Learning Research Group en el Palo Alto Research Center (PARC) de Xerox. En el LRG, Kay investigó cómo hacer a los ordenadores más potentes y fáciles de usar, teniendo siempre en mente el Dynabook, su visión de lo que debía ser un ordenador personal. Fue en este centro donde Kay pudo realizar numerosas experiencias con sus usuarios predilectos, los niños, y donde pudo poner en práctica sus ideas y analizar las reacciones que estas producían en los usuarios. Prácticamente ha sido siempre su profundo interés por la educación y los niños lo que, a lo largo de toda su vida, ha guiado la dirección de su investigación funcionando como catalizador de buena parte de sus inventos. Y es en la educación donde Kay más potencial ve en las nuevas tecnologías informáticas. Según él, los nuevos sistemas de información del futuro no sólo permitirán recuperar hechos sino también puntos de vista y crear así individuos más escépticos.

Fin Capítulo 1.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 7

### El PARC o cómo Xerox quiso inventar el futuro

_La mejor manera de predecir el futuro es inventándolo._<br>Alan Kay

El PARC o Palo Alto Research Center (Centro de Investigación de Palo Alto) fue un centro nacido en 1970 como consecuencia directa del increíble éxito de la empresa Xerox Corporation. Este complejo situado en el número 3333 de Coyote Hill Road en Palo Alto, California, ha pasado a la historia de la informática personal por ser el creador de muchos de los estándares actuales y también por su incapacidad de plasmarlos en productos que fueran comercializables, es decir, económicamente rentables.

Xerox había conseguido crear un imperio con sus máquinas fotocopiadoras pero en la compañía tenían muy claro que ello no les aseguraba el porvenir, especialmente frente a la emergencia de las nuevas formas de manipular la información y de las nuevas tecnologías, mucho más avanzadas que las de las fotocopiadoras. Fue por esas mismas fechas cuando Xerox decidió dar sus primeros pasos para entrar en el mercado de la informática comprando una empresa californiana llamada Scientific Data Systems. Este interés por este nuevo sector, unido al enorme capital que había podido acumular con los beneficios de sus fotocopiadoras, le llevaron a invertir en un centro donde se investigara las posibilidades de la informática del futuro. De hecho, la principal misión de este centro era "crear el futuro", sin preocuparse de lo practicables o comerciales que pudieran ser, a priori, sus inventos.

Xerox llegó a invertir más de 100 millones de dólares en ese proyecto, y aunque no consiguió rentabilizar sus esfuerzos directamente a través de sus productos, sí fue la mayor fuente de inspiración de nuevas tecnologías de los años setenta. Por sus instalaciones desfilaron técnicos e ingenieros de grandes empresas a los que los científicos del PARC demostraban sus inventos, y si bien a finales de su primera década de vida sus puertas ya no estaban tan abiertas como al principio, diversas empresas tuvieron ocasión de visitar, con el permiso expreso de Xerox, los experimentos y resultados de esa gran concentración de cerebros. Delegaciones como la de Microsoft, con Bill Gates al frente, o la de Apple de Steve Jobs, tuvieron ocasión de penetrar en un recinto que era considerado como la Meca de las nuevas tecnologías informáticas. A pesar de que muchos no supieron reconocer el valor de lo que tenían delante, otros tomaron muy buena nota de todo lo que vieron.

## Los "Iluminados" de la informática

El director del centro fue George Pake pero la figura clave fue Bob Taylor que se las arregló para contratar a los investigadores más brillantes del momento, especialmente a los que se sentían atraídos por los temas considerados casi de ciencia ficción. De este modo, el centro consiguió aglutinar al cabo de muy poco tiempo al mayor número de talentos y científicos del país (y probablemente del mundo en el ámbito de las tecnologías de la información) y empezó a forjar su futura áurea legendaria, áurea alimentada vivamente por el prestigio de los que por allí pasarían, definidos muy acertadamente por alguien como los Illuminati (podríamos decir que fueron los "iluminados" de este siglo). Entre sus paredes trabajaron investigadores procedentes del SRI de Douglas Engelbart como Bill English y otros que posteriormente se harían populares por diversas innovaciones: Butler Lampson, Larry Tesler, Owen Desmon, Steve Caps, Bob Belleville, Barbara Koalkin, etc., además del anteriormente mencionado Alan Kay. Muchos de ellos, llegados al PARC con la ilusión de inventar el futuro, abandonaron posteriormente la nave decepcionados por la falta de visión de Xerox (hasta 15 de ellos emigrarían a Apple Computer, por ejemplo, otros lo harían a Microsoft o Digital Equipment Corporation, y otros fundarían sus propias empresas como el inventor de la red Ethernet, Bob Metcalfe, que crearía 3Com Corporation, o Chuck Geschke y John Warnock, creadores del WYSIWYG en los procesadores de texto, que fundarían Adobe). Aunque hay quien les culpa a ellos de no haber sabido producir un ordenador innovador a un precio accesible para el mercado.

Sea como fuere, en esa leyenda se redefinieron los conceptos de la informática y de la relación entre hombre y máquina y se desarrollaron sistemas y dispositivos que significaron un gran paso en la humanización de la tecnología. En nuestra historia del proceso de humanización de la informática y, más concretamente, del desarrollo de los entornos gráficos con los que trabajan el 80% de los usuarios de ordenador actuales, el PARC tiene un papel destacado y un mérito indiscutible, al margen de que Xerox no quisiera o no supiera exactamente qué hacer con sus inventos.

## Los inventos que inspiraron al mundo

Las innovaciones que se "fabricarían" en el PARC darían lugar a lo que hoy se conoce como Interfaz Gráfica del Usuario (GUI en inglés) o, al menos, a un primer estadio de ello (un estadio poco evolucionado que sería mejorado más tarde por empresas como Apple Computer y superado posteriormente por interfaces ya no solamente gráficas, sino también intuitivas), además de a otros estándares de la informática personal actual.

En el centro de investigación de Palo Alto nacerían, por ejemplo:

- los iconos, las representaciones gráficas de las órdenes que podemos dar a un ordenador;

- las ventanas, las áreas de trabajo que permitían delimitar zonas en la pantalla;

- los gráficos gracias a los mapas de bits, una tecnología que permitiría visualizar gráficos en la pantalla de nuestros ordenadores con gran precisión.


También en el PARC:

- se utilizaría el ratón o joystick como forma principal de controlar al ordenador ya no sólo exclusivamente mediante el teclado (un invento heredado de Douglas Engelbart);

- nacería la red Ethernet, la posibilidad de conectar entre sí los ordenadores personales;

- surgirían las primeras impresoras láser que permitirían posteriormente el nacimiento de la autoedición;

- y también vería la luz la programación orientada a objeto mediante el lenguaje SmallTalk (desarrollado por Alan Kay como les contábamos en el capítulo anterior).

Dos serían los productos en los que concretarían todos estos inventos: el Alto primero y el Star después.

### El primer ordenador personal nacería en el PARC

En el PARC nacería en 1973 el primer ordenador personal, es decir, el primer ordenador desarrollado pensando que iba a ser utilizado por una sola persona (algo fuera de lugar en los mainframes o los miniordenadores que habían dominado el sector hasta el momento). Este primer ordenador personal, el Alto, no llegaría a ser comercializado nunca pero en él se plasmarían buena parte de todos los inventos que se produjeron en este centro. Posteriormente, los investigadores del PARC mejorarían este primer prototipo y construirían el Star, una versión optimizada del Alto y con posibilidades de salir al mercado (su precio no era tan astronómico aunque seguía siendo elevado). Ambos ordenadores fueron las estrellas polares de los pioneros que en ese temprano estadio de la informática personal fueron capaces de darse cuenta de la importancia de esos iconos, gráficos y símbolos y de las posibilidades que ello entrañaba. A todos ellos se debe tanto o más de lo que pudo ofrecer el PARC y no sería justo, como algunos pretenden hacernos creer para quitar presuntos méritos a otros, ofrecer la visión de este laboratorio mayúsculo de Xerox como exclusivo autor del concepto del ordenador personal actual.

Sin duda alguna, lo mejor del PARC fueron sus ideas, la posibilidad de trabajar sin pensar en la obligada comercialización de sus productos dio alas a la creatividad de sus miembros y ello acabó por redundar, paradójicamente, en benefició de otras empresas, y a la larga, en beneficio de todos los usuarios. Pero el PARC era heredero de todo un bagaje, y a su vez, fue punto de partida de la labor de muchos otros investigadores a partir de los cuales nacerían buena parte de los estándares actuales. El entorno gráfico del Alto o del Star puede considerarse como el nacimiento de la Interfaz Gráfica del Usuario pero ésta ha evolucionado hoy en día hacia sistemas que más que gráficos, son, o pretenden ser, humanos.

 

### QUIÉN ES...

### Xerox Corporation<br>(Nueva York 1906 ­ ...)

Fundada en 1906 en Rochester, Nueva York, con el nombre de The Haloid Company, pasó a llamarse Xerox Corporation en 1961. The Haloid Company, empresa dedicada a la fotografía, obtuvo en 1947 una licencia para desarrollar y comercializar una máquina fotocopiadora siguiendo una tecnología inventada en 1938 por Chester Carlson, y que no había conseguido convencer a nadie hasta el momento. Carlson y Haloid acordaron, sugeridos por la Universidad de Ohio, denominar con el término de "xerografía" a la nueva tecnología por el significado en griego de esta palabra (derivada de "escritura" y "seco"). Haloid acuñó a partir de entonces el término Xerox como nombre de marca para las nuevas copiadoras. El primer modelo que salió al mercado fue el 914 en 1959. El relativo éxito del mismo llevó a Haloid a cambiarse el nombre por el de Haloid Xerox Inc, primero, y finalmente por el de Xerox Corporation en 1961. El crecimiento de la empresa fue enorme hasta los años setenta (momento en que fundó el Palo Alto Research Center para convertirlo en el "arquitecto de la era de la información" y en su principal laboratorio de investigación). A partir de 1980 la empresa entró en una fase de declive durante la cual perdió el 50% de su cuota de mercado, pero posteriormente se recuperó. En estos momentos, Xerox vende sistemas de autoedición, fotocopiadoras, impresoras, escáners, aparatos de fax y software de gestión de documentos a más de 130 países de todo el mundo, tiene casi 86.000 empleados (46.000 de ellos sólo en Estados Unidos) y sus ingresos brutos en 1995 alcanzaron los 18.900 millones de dólares. Actualmente la empresa tiene su sede en Connecticut y está dirigida por Paul A. Allaire, presidente y jefe ejecutivo.

### Y TAMBIÉN...

### Una leyenda viva

En la actualidad sigue existiendo un Palo Alto Research Center e incluso permanece en el mismo edificio, pero cuando la gente se refiere al célebre PARC normalmente están hablando del momento legendario en que ese centro agrupó a la mayor colección de magos de la informática de los años setenta. Fue durante esa década que surgirían tecnologías clave para la informática personal actual: las estaciones de trabajo modernas y los primeros ordenadores personales con una interfaz gráfica, el SmallTalk, la red EtherNet, las impresoras láser y las redes cliente-servidor. En 1973 incluso se desarrollaría un primer procesador de texto denominado Bravo (posteriormente rebautizado como Microsoft Word).

Fin Capítulo 7.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 8

### Butler W. Lampson y el Alto

_La principal razón de que las interfaces sean difíciles de diseñar es que cada interfaz es un pequeño lenguaje de programación: define un conjunto de objetos y operaciones que pueden utilizarse para manipular los objetos. La sintaxis aquí no afecta pero está presente cualquier otro aspecto del diseño de un lenguaje de programación._<br>Butler W. Lampson

En 1973, el Palo Alto Research Center dio nacimiento a su primer producto, el Alto, el primer ordenador que merece el calificativo de "personal" por ser el primero pensado para que lo utilizara una sola persona. Se trató de un invento revolucionario pero en absoluto de un producto factible. Los ingenieros del PARC, siguiendo lo que se les pedía, no atendieron en ningún momento a lógicas de mercado, y si se hubiera fabricado para ser comercializado, el Alto habría alcanzado un precio astronómico (alrededor de los 40.000 dólares con los márgenes del momento, unos cinco millones de pesetas de la época). Sin embargo, aunque el Alto nunca llegó a venderse fue un producto bien conocido en todo Silicon Valley, el PARC se encargó de mostrarlo públicamente en presentaciones, abiertas primero y privadas después, para inspiración y ejemplo de otros investigadores y empresas.

Uno de los personajes más destacados en el diseño de este ordenador ahora antológico fue Butler W. Lampson, un físico convertido a informático, y uno de los científicos del PARC que compartía la visión de Alan Kay de transformar los mastodónticos ordenadores de finales de los años sesenta en algo útil y práctico para todas las personas. Siguiendo las ideas de Kay, pero con más avances tecnológicos a su favor, Lampson y su equipo crearon el Alto.

La idea que les movía era hacer un ordenador tan potente como los miniordenadores que dominaban el mercado en esos momentos pero más barato y dirigido al usuario general y no al ámbito científico, académico o militar. Por lo tanto, tenía que ser una máquina mucho más intuitiva, más aún, tenía que ser una máquina que perdonara los errores o deslices de los usuarios. No tiene ningún sentido comparar al Alto con los ordenadores en uso en aquellos momentos, con procesadores que ejecutaban millones de instrucciones por segundo. El Alto, por su lado, concentraba toda su potencia y memoria en otros aspectos: se trataba de una máquina ya no dirigida a ejecutar complejos cálculos de instrucciones por segundo para estadísticas científicas o militares sino dirigida a mostrar en pantalla gráficos complejos y a responder con rapidez a las órdenes del usuario a través del teclado o del ratón. Todo un cambio que anticipaba la nueva era informática que se avecinaba.

Prueba de ello es la siguiente anécdota. Desde finales de 1972 a principios de 1973, Lampson, con la ayuda de Chuck Thacker y otros ingenieros, desarrollaron diversos prototipos del Alto, y con ánimo de diferenciarlos claramente de todos los productos existentes hasta ese momento, decidieron que la primera imagen que harían aparecer en la pantalla del ordenador sería el popular monstruo de las galletas de Barrio Sésamo (una imagen que Alan Kay ya había utilizado como diseño de prueba). Monstruos de las galletas en potentes e innovadoras máquinas. El proceso de desmitificación de la ciencia informática había empezado...

### Bitmap versus sistemas basados en caracteres

Pero ¿por qué tanto bombo y platillo alrededor de esta máquina? Básicamente por un par de notables razones, especialmente para la época: los gráficos y la potencia que combinaba el Alto. Algo resumible en una especie de "más por menos": más posibilidades a cambio de menos esfuerzo por parte del usuario.

Por un lado, el Alto utilizaba por primera vez el bitmap, es decir, cada píxel de la pantalla se correspondía con un bit de la memoria del ordenador, según si el bit estuviera encendido o apagado el píxel estaría iluminado o no (bitmap significa "mapa de bits"). Ello permitía tratar cualquier elemento de la pantalla como un gráfico, y por primera vez, tener la certeza de que lo que se vería en pantalla era lo que exactamente se imprimiría: la ahora tan popular característica del WYSIWYG (What You See Is What You Get, algo así como "lo que vemos en pantalla es lo que obtendremos en papel"). Hay que tener en cuenta que las pantallas de los ordenadores de esa época (y aún muchos años después) estaban basadas en caracteres. Así, cuando pulsábamos una tecla de una letra en el teclado, se enviaba un código a la memoria del ordenador que desencadenaba una fosforescencia en la pantalla para generar la letra que se quería reproducir. Había un código para cada letra y al ordenador le bastaba con recibir el código para encontrar el símbolo que se le pedía. Y los dibujos o gráficos se generaban de igual manera. Era un método eficiente pues el ordenador tardaba muy poco en mostrar un gráfico en pantalla pero la calidad de los gráficos dejaba mucho que desear y era completamente insuficiente para ser impresa y utilizada sobre papel. El Alto acababa con todos estos inconvenientes gracias al bitmap. El bitmap dibujaba con precisión en la pantalla cualquier letra, símbolo o imagen porque todo era considerado como un gráfico en pantalla, el único problema era que requería una gran cantidad de memoria, de RAM, –algo que, en aquellos momentos estaba por las nubes y, en consecuencia, encarecía muchísimo la máquina.

La otra destacada característica del Alto era la total integración del ratón con el ordenador. Con el bitmap, el ratón para señalar y las ventanas, el Alto se erigió como el primer ordenador realmente distinto a las hostiles líneas de comandos que conformaban la realidad de la informática de la tercera generación (los ordenadores personales que estaban naciendo serían considerados la cuarta generación).

Aunque la "metáfora del escritorio" de Alan Kay se limitaba a trabajar con papeles y ventanas superpuestas, su filosofía y su implementación en el Alto por el equipo de Butler Lampson sería la piedra angular que conduciría pocos años después del invento de SmallTalk a la creación de lo que ya se denominaría "interfaz gráfica del usuario". Gracias al entorno orientado a objeto del SmallTalk, el Alto fue el primer ordenador con el que se podía interaccionar fácilmente a través de iconos, ventanas y menús sin tener que escribir ni una sola línea de código. Se trató, en definitiva, de toda una revolución dentro de la propia revolución de la informática personal.

El Alto incorporaba además una nueva posibilidad: la conexión en red con otros ordenadores. Mediante la recién inventada Ethernet, cualquier Alto podía estar conectado a otros Altos o a impresoras láser convirtiendo cada ordenador en toda una estación de trabajo, un concepto que le dotaba de grandes posibilidades y potencia.

También es cierto que el ordenador ideado por el PARC era terriblemente lento y pecaba de numerosos defectos, pero la verdad es que, en aquellos momentos, ante una concepción tan arrolladora y distinta, esto importaba bien poco. La versión mejorada del Alto, el Star, aparecería al cabo de poco pero el momento propicio para la propagación de todos estos nuevos conceptos todavía no habría llegado.

### QUIÉN ES...

### Butler W. Lampson<br>(Washington DC., 1943 ­...)

Colaborador actual de Microsoft Corporation y profesor adjunto en la Computer Science and Electrical Engineering del MIT (Massachusetts Institute of Technology), Lampson se doctoró en la Universidad de California en Berkeley y obtuvo el título de Doctor en ciencias por la Eidgeneoessische Techniche Hochschule de Zurich. Además de formar parte del Computer Science Laboratory del PARC de Xerox también participó en el Systems Research Center de Digital. Ha diseñado o colaborado en el diseño de arquitecturas de hardware, sistemas de redes locales, impresoras, lenguajes de descripción de página, sistemas operativos, sistemas de procesamiento remoto, lenguajes de programación, procesos de transacciones, seguridad informática y en editores WYSIWYG, entre otras cosas. Fue uno de los creadores del sistema de tiempo compartido SDS 940, delos protocolos de comunicaciones en dos fases, de la red de área local Autonet y de diversos lenguajes de programación.

Es miembro de la American Academy of Arts and Sciences, de la National Academy of Engineering y Fellow of the Association for Computing Machinery. Ha recibido diversos reconocimientos a su trabajo como el ACM Software Systems Award de 1984 por su trabajo en el Alto, el ACM Turing Award, el mayor reconocimiento posible para un científico informático, en 1992 y el Doctorado honorario en ciencias informáticas de la Universidad de Bolonia en 1996. En su haber cuenta con más de una docena de patentes sobre seguridad, impresión, procesamiento de transacciones y redes informáticas, entre otras. Es autor de numerosas publicaciones sobre diseño de sistemas informáticos, algunas de las cuales están disponibles en Internet.

### Y TAMBIÉN...

### El Star

A pesar de lo revolucionario que era el Alto, los de Xerox nunca tuvieron muy claro que hacer con él. En 1976 se suministraron algunas unidades del Alto IIs junto con impresoras láser a clientes de Xerox y a mediados de 1978 se utilizaban Altos a modo de prueba en cuatro puntos del país: en la Casablanca, en el Congreso de diputados, en la empresa Atlantic Richfield y en las oficinas de California, en Santa Clara, de la propia fuerza de ventas de Xerox. Después de colocar cientos de Alto IIs, los investigadores del PARC propusieron desarrollar el Alto III para el mercado de consumo. Pero la oportunidad no fue aprovechada por la empresa. Finalmente, decidieron intentar vender las ideas del PARC en un producto que no aparecería hasta casi diez años después de que el primer Alto fuera concebido, el Star, que adoptó la filosofía del SmallTalk y del Alto. Pero el Star, lanzado en 1981, fracasaría por completo (básicamente porque se pretendía vender por 18.000 dólares, más de dos millones de Ptas.).

### La invención de la tecnología Ethernet

El Alto incorporaba por primera vez la tecnología de red local más extendida de la actualidad: Ethernet. Su inventor sería Robert M. Metcalfe. Metcalfe formaba parte del Palo Alto Research Center de Xerox y, allí, junto con D.R. Boggs, ideó el sistema de envío de datos mediante paquetes que convertiría al Alto en la primera estación de trabajo personal. La tecnología Ethernet define cómo se transmite la información a través de una red y permite que ordenadores de fabricantes distintos puedan comunicarse entre sí utilizando estándares previamente acordados para la transmisión de paquetes de datos. En 1980, un consorcio formado por diversos fabricantes de informática crearon la primera especificación formal para Ethernet y la tecnología Ethernet se adoptaría a partir de ese momento como el estándar para las redes locales. Sin embargo, sería Metcalfe quien más claramente se encargaría de promocionarla y comercializarla a través de su propia compañía, 3COM, que fundó cuando en 1979 dejó el PARC.

Fin Capítulo 8.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 9

### El Altair y el Homebrew Computer Club

_No es que todos fuéramos tan inteligentes como para ver que se avecinaba una revolución. En aquel entonces, para mi, abrir la puerta del garaje, cuadrar el talonario, conservar mis recetas y cosas así podían ser una revolución. Hay millones de personas que estudian los mercados y analizan las tendencias económicas, personas más brillantes que yo, personas que trabajan para empresas como Digital Equipment e IBM y Hewlett-Pakard. Ninguna de ellas tampoco previó lo que iba a suceder._<br>Steve Wozniak

Nuestra historia de la humanización de la informática no estaría completa si no recordáramos a dos de los principales impulsores del fenómeno de la informática personal: el Altair, el primer ordenador personal en ser un éxito comercial, y el colectivo de ultraentusiastas de la tecnología que, completamente al margen de las grandes corporaciones con recursos, ayudarían a evolucionar la informática. Colectivo que se refleja en los miembros de las primeras asociaciones o clubes de informática y especialmente en los primeros hackers, mal traducidos como piratas informáticos.

### Un ordenador con nombre de estrella

El Altair y toda la pandilla de locos por la informática que agrupó a su alrededor, no forman parte de nuestra historia como destacados precursores de la informática fácil. Ni tampoco de la metáfora del escritorio o de la noción de hipermedia para racionalizar nuestro acceso al ingente caudal de información que estamos generando. De hecho, el Altair tampoco era exactamente el primer ordenador personal en ser comercializado (probablemente ese honor lo merezca el Scelbi8H, un kit comercializado en 1973 por la Scelbi Computer Consulting). Pero MIcro Intrumentation Telemetry System (MITS), la empresa que lo desarrolló, consiguió vender 2000 unidades del Altair en 1975, un número muy superior al de cualquier otro kit del momento.

El Altair tampoco era ninguna maravilla tecnológica. Basado en el procesador 8080 de 8 bits de Intel, tenía 256 bytes de memoria (¡una cuarta parte de un kilobyte!). Los accesorios como teclado, monitor y dispositivos de almacenamiento había que comprarlos aparte, y no venían con ningún software, sistema operativo ni programa alguno, por lo que, para introducir información, la única posibilidad era, de entrada, manipular los interruptores de su panel frontal uno a uno para cada dígito binario. Además, al venderse en forma de kit (los kits eran equipos que se vendían desmontados en componentes sueltos), como la mayoría de los primeros ordenadores personales, el Altair limitaba su atractivo a los aficionados cuya pasión por la informática y la electrónica superaba el mero interés para alcanzar el estadio de verdadero hobby, requisito indispensable para conseguir montar esos primeros aparatos que, por otro lado, hacían bien poco una vez ensamblados. Eso suponiendo que funcionaran, porque la calidad de la mecánica de la máquina dependía en gran medida de la habilidad del montador y, en muchos casos, el usuario no conseguía salir airoso de la empresa. Sin embargo, esto no sería ningún freno para los primeros compradores. El Altair les estaba dando a todos ellos la posibilidad siempre soñada: dejar de hacer cola para poder usar sólo durante unas pocas horas alguno de los grandes sistemas que había en universidades o empresas y poder disponer de todo un ordenador en casa. No importaba que el Altair fuera una simple carcasa, un transformador, una placa lógica con un chip y 256 bytes de memoria y unos meros interruptores como única forma de interactuar con la máquina. No importaba que se tardara horas en montar tal aparato y que, de lograrse, uno tuviera que entenderse con el ordenador con lenguaje máquina (es decir, mediante códigos numéricos, al menos hasta que no se suministrara con el BASIC desarrollado por los fundadores de Microsoft, que, por otro lado, hacía bien poco más que informar de su presencia). No importaba que no existiera ni un solo programa para el Altair y que hubiera que confeccionárselos uno mismo o que no se dispusiera de ningún tipo de almacenamiento y al apagar la máquina toda la información se esfumara. Nada de esto importaba porque las posibilidades eran evidentes.

El Altair del MITS no sólo abrió de par en par el mercado del ordenador personal sino también el de las ferias, los canales de distribución, las revistas de informática, los grupos de usuarios, los intercambios de software y, en general, el de toda una nueva industria que estallaría en pocos años. Y es que la creación del Altair se subscribía a toda una filosofía.

A principio de los años setenta, un pequeño grupo de lo que se ha dado en denominar "rebeldes tecnológicos" (porque no aceptaban los planteamientos de las grandes empresas líderes como IBM o DEC Computer) se decidirían a crear su propia empresa y a desarrollar el Altair 8800 con un único fin: liberar la tecnología, hacer la informática asequible a millones de personas y hacerlo tan rápido que el gobierno de los Estados Unidos no pudiera reaccionar al respecto. El grupo consideraba al ordenador personal como una arma que los individuos podían utilizar para defenderse de las grandes empresas, del poder y del gobierno. Básicamente, toda la filosofía de esos primeros rebeldes informáticos pasaba por crucificar al gobierno y a las grandes corporaciones que, como la compañía telefónica, controlaban las comunicaciones y las nuevas tecnologías. Lanzado en enero de 1975 con el nombre de una de las estrellas más brillantes del universo, el Altair fue definido por la revista Popular Electronics como el primer equipo de miniordenador que rivalizaba con los modelos comerciales. Costaba 395 dólares (unas 50.000 Ptas.).

El Altair no destacaría por sus características pero abriría el apetito informático a un montón de jóvenes que, al margen de las grandes empresas que habían marcado el devenir de la industria hasta el momento (y de donde habría sido más lógico que proviniera la nueva generación de ordenadores), definirían el ordenador del futuro.

### El club de los usuarios del ordenador doméstico

El lanzamiento del Altair sirvió para que se fundara el Homebrew Computer Club (HCC) o Club de los ordenadores caseros. Aunque el HCC tenía un claro antecedente en la People's Computer Company (PCC), una asociación que se reunía en Menlo Park y que se había creado en 1972 con el propósito de desmitificar los ordenadores. La PCC era una asociación informal donde los veintitantos entusiastas que se congregaban durante sus reuniones, no tenía miembros como tal, aprendían los "misterios" de la informática. Era una especie de club social y de empresa informática a la vez. Pero el advenimiento de los ordenadores personales, máquinas pequeñas y compactas y utilizables por cualquiera, les hizo urdir una nueva sociedad que montaría sus propios ordenadores con sus propios recursos. Y así nació el club de los ordenadores domésticos.

La HCC se fundó el 5 de marzo de 1975 y a ella concurrieron treinta y dos personas. Inicialmente, las reuniones se celebraban en un garaje de Menlo Park aunque después tuvieron que trasladarse a un auditorio del recinto universitario de Stanford dado su fuerte crecimiento. En mayo ya tenía ciento cincuenta miembros y al final de su etapa más de 500.

Los miembros del club compartían las actitudes e inquietudes contraculturales de toda el área de San Francisco. El club censuraba la "comercialización" de los ordenadores y apoyaba la iniciativa de dar poder informático a la gente. Compartían en suma los mismos ideales de los primeros hackers o piratas informáticos (cuando este concepto aún no era sinónimo de terrorismo digital). Hay que tener en cuenta que en aquella época la idea de un ordenador que pudiera ser utilizado por una sola persona era revolucionaria y fue adoptada por estos radicales tecnológicos como su aportación a la contracultura. Trasladar el poder informático del Estado y las grandes corporaciones a la gente de la calle era lo que siempre habían soñado y esas nuevas máquinas parecía que podían dárselo.

### De entusiastas a empresarios

Así, entorno al HCC fueron reuniéndose algunos de los más brillantes y jóvenes cerebros de esa subcultura que nacía con fuerza. Actualmente, por hacking o pirateo informático se entiende el acceso no autorizado a ordenadores o sistemas informáticos ajenos, la creación de virus, bombas informáticas e incluso el robo y espionaje a través del ordenador. Pero a principios de los años setenta, los hackers eran simplemente grandes entusiastas de la tecnología en general. De hecho, este colectivo no había nacido con los ordenadores personales sino que su aparición era mucho anterior. Los hackers tienen sus antecedentes en los "phreakers" telefónicos. Si los primeros manipulaban sin autorización los ordenadores, los segundos hacían lo propio con las redes telefónicas para efectuar llamadas de larga distancia de manera gratuita. El "phreaking", una especie de pasatiempo completamente apolítico al principio, fue adoptado por los rebeldes de la informática que conformarían el movimiento contracultural que se desarrolló en paralelo al nacimiento de los primeros ordenadores de uso personal. Muchos de estos primeros phreakers se convertirían inmediatamente en hackers al nacer los ordenadores personales guiados por la misma curiosidad que las había impulsado a ser phreakers. Los dos grupos se fundirían para formar una subcultura de la alta tecnología.

Los primeros hackers fueron un grupo de genios tecnológicos del Massachusetts Institute of Technology que se dedicaban a trabajar juntos para crear programas para los nuevos sistemas que se desarrollaban. Estos hackers o piratas de los setenta conformaban el grupo de jóvenes programadores más brillantes y rápidos del momento. Por su estilo de vida, se apartaban por completo de todos los convencionalismos. Tenían costumbres peculiares: a menudo trabajaban hasta altas horas de la madrugada o se pasaban noches enteras sin dormir, no prestaban ninguna atención a la forma de vestir ni les importaban los hábitos sociales más corrientes. Eran anárquicos. Tenían una misión: derribar las fronteras de la informática dominante, explorar las nuevas áreas tecnológicas para llegar a donde nadie había llegado antes y poner a prueba todos los límites de la ciencia. Su credo, si es que puede decirse que tuvieran uno, ha sido definido por algunos como "mirar sin tocar". Para ellos, el acceso a los ordenadores de todo el mundo tenía que ser ilimitado y total pero funcionando bajo un código de ética: mirar pero no tocar significaba el derecho a penetrar en los ordenadores pero no a manipularlos. La progresiva desvinculación de los hackers a esta regla los ha convertido en los actuales bárbaros digitales que disfrutan por el mero hecho de destruir e ir contracorriente. Su único credo actual podría resumirse en "saltarse todas las normas".

Pero no todos los primeros phreakers y hackers acabarían siendo piratas de la informática en la acepción actual de este término. De este colectivo de locos entusiastas surgirían los fundadores de algunas de las empresas más importantes de la actualidad como Microsoft o Apple Computer. Bill Gates y Paul Allen desarrollarían una versión de BASIC especialmente para el Altair 8800 en su recién creada empresa y Stephen Wozniak y Stephen Jobs formarían parte de los fundadores del Homebrew Computer club.

Motivados por el éxito del Altair, los miembros del HCC empezaron a realizar sus propios diseños utilizando componentes tomados prestados de otros ordenadores. De entre ellos destacarían enseguida Jobs y Wozniak por su decisión de construir y comercializar las cajas azules que permitían hacer llamadas telefónicas gratuitamente. El primer Apple surgiría casi por accidente. La falta de recursos económicos obligó a Wozniak a construirse su propio ordenador, de haber dispuesto de dinero posiblemente se habría limitado a comprar alguno de los populares kits que estaban en el mercado. Wozniak lo construiría en parte para impresionar a sus colegas del club.

Las ideas de democratización y accesibilidad de todos a las nuevas tecnologías que guiaban a muchos de aquellos jóvenes son el mejor reflejo de lo que representaba el giro que la revolución de la informática personal y, en especial, más tarde, la revolución en las interfaces, daría al sector y la industria en general. Obviamente, este gran salto no se daría con las ideas más anárquicas y antisistema de la contracultura de los setenta y principios de los ochenta, sino con la mentalidad abierta y emprendedora de algunos de esos primeros entusiastas de los ordenadores que comprenderían la necesidad de universalizar el acceso a la tecnología y tendrían claro cómo hacerlo: humanizándola. Sorprendentemente, esta nueva tendencia surgiría de reductos tan apartados de las grandes empresas, con muchos recursos para la investigación y el desarrollo, como son los primeros hackers. Aunque tal vez no sea tan extraño, pues la lógica por la que estas empresas se guiaban seguía una dinámica completamente indiferente a las necesidades de los usuarios. La revolución sólo podía salir de un sitio: de los propios usuarios.

### QUIÉN ES...

### Micro Instrumentation Telemetry Systems (MITS)<br>(Alburquerque, Nuevo México 1968­1980)

Compañía fundada por Ed Roberts en Alburquerque, Nuevo México, en un garaje y con la ayuda de un par de compañeros de las fuerzas aéreas, tan aficionados como él a la electrónica. Empezaron vendiendo por correo transmisores de radio para aeromodelos pero, en 1969, Roberts compró la parte correspondiente a sus socios y trasladó la empresa a unas instalaciones mayores donde fabricarían calculadoras, incluyendo kits para los aficionados domésticos. En 1972, la entrada de Texas Instruments en el mercado de las calculadoras fue dramático para las pequeñas empresas como MITS. Texas Instruments había desarrollado su propio chip semiconductor y empezó a vender calculadoras a mitad de precio prácticamente echando del mercado a los pequeños fabricantes. Un poco más tarde, en 1974, un amigo de Ed Roberts, Less Soloman, editor técnico de la revista Popular Electronics, estaba buscando algún modelo de ordenador personal comercializable por menos de 500 dólares para promocionar en sus páginas y Roberts creyó ver su oportunidad para salir adelante. Así nació el Altair (cuyo nombre procedía del último capítulo de Star Trek que acababa de ver la hija de doce años de Soloman, concretamente Altair era el nombre del destino al que se dirigía la nave estelar Enterprise). El artículo publicado sobre el Altair en el número de enero de 1975 de Popular Electronics desató una fiebre por los kits del Altair completamente inesperada y la empresa de Ed Roberts se vio desbordada de peticiones. Algo más tarde, dos programadores de Boston, Paul Allen y Bill Gates, se pusieron en contacto con Ed Roberts para ofrecerle la posibilidad de intentar que el programa de BASIC que habían escrito para los microordenadores funcionase sobre el Altair. Roberts, muy escéptico al principio, acabó convenciéndose de la viabilidad de la propuesta. Después de esta colaboración, Allen se quedó a trabajar en MITS y éste empezó a comercializar un modelo de Altair que se vendía junto con el BASIC, más tarde incluso llegarían a sacar otro modelo de ordenador, el 680b. Pero las cosas se le empezaron a poner difíciles a Ed Roberts con la competencia creciente (principalmente de Apple y Commodore ) justo cuando Allen y Gates le abandonaron para fundar su propia empresa, Microsoft, en 1976. Así que, en 1977, Roberts vendió MITS a Pertec, una empresa especializada en discos duros y cintas para miniordenadores y mainframes, sin embargo ello no hizo más que envolverle en una batalla legal por los derechos del BASIC que Pertec creía haber adquirido al comprar MITS y que resultaron estar en posesión de Microsoft. Ed Roberts, descontento con la nueva dirección tomada por Pertec, abandonó la empresa e incluso el sector para dedicarse de lleno a la medicina. Pertec siguió fabricando Altairs hasta finales de 1978 pero al cabo de dos años había desaparecido cualquier vestigio de la empresa que fundara Ed Roberts.

### Y TAMBIÉN...

### Phreakers y hackers famosos

Steve Jobs y Steve Wozniak, fundadores de Apple Computer, fueron en sus comienzos dos afanados phreakers (Berkeley Blue y Oak Toebark eran, respectivamente, sus apodos para mantenerse en el anonimato) que se dedicaron a comercializar unos curiosos y sencillos aparatos que permitían realizar llamadas telefónicas fraudulentas, es decir, sin pagar. Estos aparatos eran profusamente utilizados para realizar llamadas internacionales o de larga distancia, en una época en que este tipo de llamadas eran muy poco habituales por su elevado coste.

De la anticultura, el antisistema y el antigobierno a la multinacional

Para los phreakers, defraudar a la compañía telefónica era casi un derecho de todos los ciudadanos, tal era su fobia contra esta gran corporación. Sin embargo, algunas de aquellas jóvenes mentes rebeldes y antisistema se desvincularían paulatinamente de esta ideología para desarrollar un nuevo espíritu empresarial: el de las empresas nacidas en garajes y convertidas en multinacionales.

Fin Capítulo 9.<br>
[Volver a Indice](#indice)

<br><br>
 
## CAPITULO 10

### Dan Bricklin, Bob Frankston y el VisiCalc

_VisiCalc fue indiscutiblemente la primera "aplicación de éxito" tan atractiva que la gente compraba hardware sólo para ejecutarla. Acabó por convertirse en uno de los productos de software más vendidos de la industria de la informática personal._<br>Owen W. Linzmayer

Volviendo a recuperar los conceptos clave en la humanización de los sistemas operativos, nos encontramos con que una de las primeras materializaciones del uso de la "metáfora gráfica" en un producto comercial, sino la primera, fue sobre una hoja de cálculo desarrollada en 1978.

La idea provino de Dan Bricklin, un estudiante de la Harvard Business School, enfrentado a la ardua tarea de preparar una hoja de cálculo para un caso a estudiar en clase. Tarea que derivó en frustración ante las únicas dos alternativas existentes en aquellos momentos: realizar los cálculos a mano, o reservarse algunas horas de los mainframes disponibles para utilizar los programas, por otro lado poco adecuados, que ofrecían. Tan desalentadoras opciones, sumado a sus anteriores experiencias programando calculadoras para ordenadores y trabajando con procesadores de texto en Digital Equipment, le habían llevado a pensar que los ordenadores podían ser muy útiles en la empresa y que tenía que existir una forma mejor de realizar todos esos penosos cálculos. De aquí surgiría la idea de crear un programa que permitiera visualizar los cálculos a medida que se realizasen y que estuviera basado en la metáfora de una pizarra electrónica en la que se pudiese escribir como se hacía con la tiza sobre la pizarra de las aulas. Así que Bricklin empezó a desarrollar el programa que sería para los números lo que los procesadores de texto habían sido en su momento para las palabras, es decir, una herramienta que permitiera insertar y borrar elementos en cualquier momento y mostrara inmediatamente el resultado de tales variaciones.

Al llegar el verano de 1978, Bricklin ya había programado la primera versión operativa de esta idea. El programa permitía a los usuarios introducir una matriz de cinco columnas y veinte filas. Sin embargo, esta primera versión no era muy "amigable" para el usuario así que Bricklin acudió a su amigo del Massachusetts Institute of Technology, Bob Frankston, para mejorar y ampliar el programa y conseguir un producto viable para ser comercializado. De este modo, Bricklin se concentró en el diseño y la documentación y Frankston en la programación. En 1979 fundaron su propia empresa y, paralelamente a la finalización de sus estudios, siguieron dedicados al proyecto (Frankston escribió el programa por las noches, cuando el alquiler de los mainframes era más económico). Finalmente consiguieron terminar una versión de poco más de 20K para el Apple II y la lanzaron al mercado, y de inmediato se pusieron a trabajar en las versiones para el Tandy TRS-80, el Commodore PET, y el Atari 800. El producto recibió el nombre de VisiCalc, una abreviación de "Visible Calculator" (calculadora visible), que reflejaba ni más ni menos que la materialización de la idea que les había llevado a desarrollarlo.

### Una nueva forma de pensar y procesar la información

Lo que desarrollaron Dan Bricklin y Bob Frankston hace ya casi veinte años fue un programa basado en una de las metáforas más comprensibles que uno pudiera imaginar para todas aquellas personas cuya tarea giraba entorno al dinero y la contabilidad ­contables, tesoreros, economistas, auditores, estudiantes, analistas, etc. La metáfora representaba sencillamente una hoja con columnas y filas en las que insertar las cifras para calcular ingresos, gastos, pérdidas y beneficios. Igual que cualquier libro de contabilidad, este programa utilizaba la idea de trabajar sobre una hoja con la misma estructura que los libros convencionales utilizados para llevar las cuentas de cualquier empresa. Era una mezcla de libro de contabilidad y calculadora que permitía resolver cualquier tipo de planificación financiera estableciendo relaciones matemáticas entre las diferentes celdas de columnas y filas. La hoja de cálculo electrónica tenía numerosas ventajas sobre sus homólogas en papel: por ejemplo, no era necesario volver a repetir todos los cálculos cuando se modificaba un mero apunte de una fila, la hoja de cálculo lo hacía automáticamente por nosotros. El error humano, tan habitual en los cálculos manuales, se reducía drásticamente.

Este programa, no sólo supuso un paso de gigante en el salto que los ordenadores personales darían en los primeros años para pasar de ser utilizados por simples fanáticos de la informática a convertirse en herramientas útiles para cualquiera, sino que además le dio un vuelco a la forma en como la gente estaba acostumbrada a trabajar en este ámbito. La libertad de poder equivocarse o experimentar que este programa permitía hizo que la contabilidad dejara de ser algo engorroso y aburrido para convertirse en una importante herramienta de la empresa. VisiCalc permitía hacer un seguimiento exhaustivo y preciso de los resultados empresariales y servía además como herramienta para simular con facilidad posibles escenarios y predecir pérdidas y ganancias. La contabilidad se convirtió en algo divertido e imaginativo. Tampoco hay que olvidar que la introducción especialmente de este tipo de aplicaciones informáticas supuso toda una revolución para una determinada categoría profesional. Los antaño indispensables contables, que además eran los únicos de la empresa en dominar al detalle la situación económica interna, pasaron a ser prescindibles y substituibles. La profesión desapareció, o más bien cambió, no porque hubiera máquinas que hicieran el trabajo de estos profesionales (aunque sí es cierto que hacían parte de él), sino porque había unas herramientas que lo simplificaban, lo optimizaban y ampliaban sus posibilidades, herramientas que además podían ser manejadas por cualquiera. Ya no era necesario perder el tiempo entre números y cifras calculando y comprobando una y otra vez columnas y filas, ahora la contabilidad era un proceso creativo que permitía mucha más sofisticación con menos esfuerzo.

Si ahora echáramos un vistazo a VisiCalc muchos de nosotros no conseguiríamos entender dónde estaba la "metáfora gráfica" en esa aplicación, pero para los usuarios del momento este programa con sus filas y columnas era lo más parecido que habían visto jamás en la pantalla de un ordenador a sus hojas de cálculo tradicionales. De hecho, para muchos, VisiCalc fue la primera aplicación que por su gran atractivo y utilidad conseguiría vender un determinado hardware sólo para poder usar este programa (el Apple II, para el cual fue diseñado inicialmente, debió gran parte de su éxito a la demanda que suscitó el software VisiCalc).

No fue hasta la aparición de VisiCalc en 1979, que el sector de la pequeña y mediana empresa empezó a valorar en su justa medida al ordenador personal. Hasta ese momento, se le consideraba como un mero juguete para aficionados a la tecnología (incluso había empresas que se permitían el lujo de ponerse nombres tan poco serios como Apple, "manzana" en inglés, u ordenadores con nombres tan poco ortodoxos como el PET, la "mascota" de Commodore) y no parecía que existiera motivo alguno para que las empresas tuvieran que confiar en él como herramienta de trabajo. Hasta que apareció VisiCalc. No es ninguna exageración afirmar que esta aplicación ayudaría enormemente a introducir el ordenador personal en la empresa, y que la visión que desde la empresa se tenía de la informática personal cambiaría radicalmente a partir de ella.

### QUIÉN ES...

### Dan Bricklin<br>(Filadelfia, 1951 ­ ...)

Nacido en la cuna del ENIAC, Dan Bricklin se matriculó en 1969 en el Massachusetts Institute of Technology para estudiar matemáticas pero cambió esta materia al cabo de poco por la de ciencias informáticas. Estando en el MIT trabajó en el Laboratorio de ciencias informáticas donde ayudó a diseñar una calculadora electrónica y participó en la programación para implementar el lenguaje APL. Fue durante esta época que entró en contacto con Bob Frankston con quien entablaría una gran amistad. Después de graduarse en 1973, Bricklin trabajó para Digital Equipment Corporation hasta 1976 en que pasó a FasFax, una empresa fabricante de cajas registradoras. En 1977 acudió a la Harvard Business School quién le otorgó un máster en administración de empresas en 1979. Fue en esta misma época que consiguió un préstamo para su primera empresa, Software Arts, con la que lanzaría VisiCalc. Pero a partir de 1982, la competencia procedente de otras hojas de cálculo debilitó enormemente a la empresa que finalmente fue adquirida por Lotus Software en 1985. Bricklin actuó de asesor para Lotus durante muy poco tiempo y a finales de 1985 creó otra empresa, Software Garden Inc, cuyo primer producto fue Demo Program, una aplicación que permitía a los desarrolladores crear demostraciones de sus productos ante de tener una versión final de los mismos. En 1992, Bricklin no sólo dirigía Software Garden sino que también pasó a ser vicepresidente de la Slate Corporation, una compañía dedicada a desarrollar un programa basado en lápices electrónicos. Dan Bricklin ha destacado además por la creación de innovadores programas como OverAll, AtHand o TimeLOCK.

### Y TAMBIÉN...

### Una aplicación incomprendida

VisiCalc, primero bautizada como Calculedger, fue desarrollada por Dan Bricklin y Bob Frankston para Personal Software Inc que la mostró a diferentes empresas, como Microsoft y Apple, para que compraran los derechos de la misma. Sin embargo, inicialmente nadie se interesó lo más mínimo por ella. Personal Software decidió finalmente lanzarla ella misma al mercado en 1979 para el Apple II y fue un éxito inmediato. Consiguió convertirse en el estándar para todas las empresas, además de vender muchos Apple II.

### La caída y sucesión de la primera metáfora contable

VisiCalc fue la primera hoja de cálculo electrónica y uno de los primeros programas en aplicar la noción de la metáfora gráfica a la informática (el primero en aplicarla a un programa de utilidad empresarial). Pero cuando el mercado para las hojas de cálculo electrónicas se disparó a principios de 1980, VisiCalc reaccionó con lentitud a la introducción del PC de IBM que utilizaba chips de Intel. Mitch Kapor desarrolló Lotus para el nuevo estándar de hardware y su hoja de cálculo se convirtió rápidamente en la aplicación dominante. Cuando en 1985 Lotus adquirió Software Arts eliminó inmediatamente a VisiCalc de su lista de productos. Lotus 1-2-3 fue el software dominante en este campo hasta finales de los 80. Pero, paradójicamente, la misma lentitud de reacción que VisiCalc tuviera ante el PC de IBM pareció vivirla Lotus ante la aparición de Windows (1-2-3 tardó muchísimo en tener una versión para este entorno y de 1989 a 1992 Microsoft Excel fue la única hoja de cálculo disponible para Windows 3.0). Lotus fue finalmente comprada por IBM a mediados de 1990 sin que esta adquisición haya supuesto la remontada de este producto hasta ahora. En la actualidad, Microsoft Excel es la hoja de cálculo líder en la mayoría de entornos.

Fin Capítulo 10.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 11

### El segundo intento de Xerox: El Star

_Estos paradigmas cambian la forma en como realmente pensamos. Conducen a nuevos hábitos y modelos de comportamiento más potentes y productivos. Pueden conducir a un sinergismo de maquinación humana._<br>D.C. Smith, R. Kimball, C. Irby, B. Verplant, B. Haslem (refiriéndose al Star)

Ya hemos hablado del primer fruto tangible de ese gran parque tecnológico que sería el PARC y que daría el disparo de salida para muchas de las tecnologías que adoptaría la industria a posteriori. Pero si bien el PARC había creado uno de los productos más revolucionarios de la era informática con el Alto, la verdad es que Xerox no aprovechó esta ventaja inicial. Las razones de su fracaso fueron diversas. Algunos las ubican en la falta de mentalidad comercial de los desarrolladores del producto mientras que, para otros, lo determinante fue que Xerox nunca llegó a comprender del todo la magnitud del invento que tenía entre sus manos, y mucho menos lo que llegaría a dar de si esa tecnología. La verdad es que, fuera o no intención de Xerox, en el PARC se respiraban otros aires. Muchos de los que allí trabajaron han llegado a afirmar que la presión por rentabilizar comercialmente los esfuerzos de investigación era prácticamente nula. La investigación en sí era lo importante. Las ideas eran sus productos. Materializar esas ideas en artículos o tesis que posteriormente se publicaban en el entorno académico y tecnológico era el fruto de esas ideas y parece que eso les podría bastar a la mayoría de científicos que trabajaban en el PARC, al menos al principio, antes de que se iniciara el lento discurrir de investigadores hacia otras empresas.

Sea como fuere, Xerox llevó a cabo una segunda tentativa para comercializar todos los conceptos desarrollados en el PARC y que habían tenido una primera aplicación experimental en el Alto. En noviembre de 1979, parece ser que fueron los propios investigadores del PARC los que intentaron conseguir más fondos de Xerox para desarrollar un nuevo sistema informático. Inicialmente hubo reticencias procedentes de algunos importantes directivos que no querían apostar por el mercado de la informática personal, pero después de meses de duras luchas internas el proyecto del Star fue finalmente aprobado.

### Siguiendo la estela dejada por el Alto

El hardware del Star siguió el modelo marcado por el anterior y experimental Alto. Al igual que en el Alto, el Star consistía en un procesador MSI (Medium-Scale Integration) desarrollado por Xerox, un disco duro de almacenamiento, una pantalla bitmap con una resolución de 72 puntos por pulgada, un dispositivo señalador (el ratón) y una conexión a red Ethernet. Pero el Star era una máquina de un rendimiento muy superior al del Alto: era casi el triple de rápida, tenía 512 Kb de memoria principal (frente a los 256 Kb de la mayoría de Altos), de 10 a 29 Mb de memoria en disco (mucho más que los 2,5 Mb del Alto), una pantalla de 10,5 pulgadas por 13,5 pulgadas (la pantalla del Alto era de 10,5 por 8 pulgadas) y Ethernet a 10 megabits por segundo (en lugar de a 3 megabits). El Star, como el Alto, se conectaba a otros Star vía Ethernet y compartía así archivos, correo y servidores de impresión. Los servidores se conectaban con Ethernet entre ellos directamente, o bien a través de la línea telefónica permitiendo trabajar en red a ordenadores alejados físicamente.

Evidentemente la principal y más destacada característica del Star, al igual que del Alto, sería lo que el usuario vería en su pantalla: las imágenes (iconos), las listas con múltiples opciones (menús) y las diversas secciones en las que se fragmentaba la pantalla (las ventanas). Además, el Star sería el primer ordenador personal en incorporar gráficos bitmap, un ratón, una impresora láser (lo único que se había conseguido comercializar con éxito del sistema del Alto), un procesador de texto WYSIWYG, conexión en red con otros Star vía Ethernet y software que combinaba texto y gráficos en el mismo documento. Y lo que era más importante, el sistema era muy sencillo de utilizar, más de lo que hubiera utilizado nadie hasta ese momento.

Un análisis más pormenorizado de los principios básicos de la interfaz del usuario del Star descubre un montón de características en las que aún se basan los sistemas gráficos actuales. Estos principios básicos eran a su vez los que habían guiado el diseño y creación de la interfaz del usuario del Star.

En primer lugar existía un modelo conceptual familiar para el usuario. Se trataba del concepto o grupo de conceptos que explican el comportamiento de un sistema, el modelo recreado en la mente del usuario y que permite que una persona comprenda e interactúe con el sistema. Los diseñadores del Star habían decidido, después de años de trabajo, implementar el modelo basado en la metáfora de la oficina. Lo primero que veía un usuario del Star era el "Escritorio" cuyo aspecto era lo más parecido que uno pudiera imaginar al escritorio de una oficina, incluyendo la mesa y demás mobiliario de trabajo. Representaba el entorno de trabajo, donde se encontraban todos los proyectos en curso. En el Escritorio se daba la máxima implementación de la metáfora de la oficina.

Después estaba la primacía del "ver y señalar" frente al "recordar y escribir". Los diseñadores habían dado prioridad a la visualización de las cosas en lugar de obligarnos a recordar convenciones y combinaciones de teclas y comandos. Según ellos, un sistema bien diseñado era aquel que lo convertía todo en una tarea visible en la pantalla. Para representarlo todo en pantalla se utilizaban los iconos, ventanas, propiedades y hojas de opciones.

El tercer principio básico del Star era el WYSIWYG (What You See Is What You Get) o "lo que vemos es lo que obtenemos " y que era posible gracias a los gráficos bitmap del Star.

Los comandos universales eran otra de sus guías básicas. Se trataba de un puñado de comandos que se podían utilizar en todo el sistema. Básicamente eran los relativos a desplazar, copiar, borrar, mostrar y copiar las propiedades, repetir y deshacer una acción, y obtener ayuda. Estos comandos recibían el calificativo de universales o "genéricos" porque ejecutaban siempre la misma acción fuera cual fuera el objeto seleccionado, y eran mucho más sencillos que los complejos comandos a los que estaban habituados los usuario de otros sistemas informáticos del momento.

Después estaba la consistencia en el funcionamiento general del Star. Esta era otra de las claves de su sencillez de uso y otra de las grandes herencias de los mejores sistemas actuales. La consistencia suponía que los distintos mecanismos de funcionamiento debían ser siempre los mismos manteniendo una coherencia global. La consistencia no sólo implicaba al sistema del Star sino también, y al igual que los comandos universales, al diseño de las aplicaciones desarrolladas para su entorno. Así, el principio de la consistencia hacía que todos los programas funcionasen de modo similar y coherente. Bastaría con aprender a utilizar uno sólo de ellos para dominar cualquier programa.

Otro de sus principios era la simplicidad. Los investigadores del PARC partieron de la idea de que un sistema sencillo era mejor que un sistema complicado, siempre y cuando ambos tuvieran las mismas capacidades. Y, siguiendo esta norma, intentaron que el Star fuera tan sencillo de usar como fuera posible.

Y, finalmente, estaba la interacción entre modos y la personalización del sistema. La máxima del Star era que el usuario siempre debían tener claro en todo momento en qué modo de trabajo se encontraba, este era el ideal para la interacción entre modos. En cuanto a la personalización del sistema, se trataba de un principio destinado a satisfacer a todos los usuarios. Los diseñadores del Star tenían claro que un sistema, por potente o genérico que fuese, jamás contentaría a todos los usuarios. Las personas somos diferentes y la única solución es diseñar un sistema que permita a cada uno configurarse las cosas a su gusto, es decir, que sea personalizable.

Así, en abril de 1981, ocho años después de la invención del Alto, Xerox presentó el sistema de oficina Star. La realidad era que los creadores del Star consideraban que esa máquina tenía todas las cartas para cambiar el mundo. La evidencia les provenía de sus experiencias con numerosos usuarios que habían probado los innumerables prototipos fabricados de ese ordenador. De las experiencias de estos usuarios y la adopción de los paradigmas del SmallTalk y del Alto nació el Star. Los ingenieros que diseñaron el Star llegaron a afirmar que estos paradigmas "modificaban la forma en cómo pensamos y que nos conducían a adoptar nuevos hábitos y modelos de comportamiento más eficientes y productivos que desembocaban en una máquina humanizada".

### Demasiados obstáculos insalvables

Sin embargo, Xerox demostró no tener la menor idea de cómo vender una maravilla como esa. Baste decir que el modelo más económico que se comercializó se vendía por el nada módico precio de 18.000 dólares, una suma demasiado elevada para un ordenador demasiado avanzado a su tiempo. Además, en 1981 ya existían otros sistemas alternativos, ninguno como el Star pero tampoco valían su precio. Por otro lado, el concepto de informática personal de la Xerox con estaciones de trabajo conectadas en red, impresoras láser y archivadores electrónicos no estaba precisamente en consonancia con el concepto más extendido de ordenador personal. No sólo se hacía difícil de comprender para la mayoría sino que además el Star no funcionaba a la perfección. En primer lugar porque, debido al ambicioso software que incorporaba, el sistema era muy lento, mucho más que el de otros ordenadores personales, y en segundo lugar, porque a pesar de ir dirigido a gerentes y ejecutivos de empresa principalmente, el software del Star no incluía una hoja de cálculo, y lo que era peor, ninguna de las hojas de cálculo disponibles en el mercado funcionaban en él. Y este era su tercer problema. El Star era un sistema cerrado. Tan cerrado que no sólo era incompatible con el resto de ordenadores existentes sino que además, Xerox no tenía la menor intención de hacer público su lenguaje de programación y preveía que sólo, y exclusivamente, los empleados de Xerox iban a poder escribir aplicaciones para él. Si a ello le sumamos la falta de preparación de la fuerza de ventas de la empresa cuyos conocimientos en máquinas fotocopiadoras eran enormes pero absolutamente nulos en sistemas informáticos, y aún menos en el innovador Star, se comprenderá la posterior debacle en su comercialización. Y queda una última explicación. A pesar de todos esos inconvenientes, Xerox aún habría tenido alguna oportunidad de triunfar con el Star si no hubiera sido por la coincidencia en el tiempo y el momento con la aparición de IBM en el mercado del ordenador personal, aparición que barrería del mercado prácticamente a todos, con muy pocas excepciones.

Sin embargo, si bien el nuevo compatible PC de IBM no adoptaría las principales innovaciones aportadas por el Star (más bien se alejaría diametralmente de ellas) estas no se perderían. Diversas empresas tendrían oportunidad de tomar contacto con ellas, y lo que es más importante, se inspiraríanen ellas. Una en especial tomaría muy buena nota de todo ello y desarrollaría el primer ordenador personal realmente intuitivo, ya no comercializado en forma de kit, que sería un éxito comercial y que marcaría todo un punto de inflexión en la industria de la informática. Estamos hablando, naturalmente, de Apple Computer.

### QUIÉNES SON...

### D.C. Smith, R. Kimball, C. Irby, B. Verplant, B. Haslem

David Canfield Smith, Ralph Kimball, Charles Irby, Bill Verplant y Bill Haslem fueron los cinco integrantes del equipo del PARC que desarrollarían el Star siguiendo el diseño original del sistema experimental del Alto. Después del fracaso comercial del Star, Xerox se planteó seriamente donde iba a concentrar su investigación. A principios de los años noventa tomó la decisión de centrarse en el documento en sí y apartarse del sector del procesamiento de datos y de los ordenadores personales. El foco de la empresa parece estar ahora ubicado en asistir a los ejecutivos de las empresas a crear, utilizar y transmitir informes, memorándums y bases de datos. La intención parece ser que es transformar las fotocopiadoras de Xerox en equipos versátiles capaces de enviar documentos y gráficos a los ordenadores y a aparatos de fax y capaces de conectarse electrónicamente a estaciones de trabajo.

### Y TAMBIÉN...

### Un producto explosivo

Xerox presentó el Star en numerosos actos públicos en los cuales la respuesta siempre era parecida: tenía un éxito total. La gente quedaba enormemente sorprendida con el ordenador y la empresa incluso llegó a hacer experimentos prácticos con el mismo como, por ejemplo, probarlo con un grupo de secretarias después de haber instalado un procesador de texto que explotaba sus características más "amigables". Los resultados siempre eran impresionantes y las secretarias conseguían, a las pocas horas, trabajar con ellos muy productivamente.

### El tiempo perdido e irrecuperable

Si Xerox no hubiera fracasado en la introducción del ordenador personal con una interfaz gráfica del usuario desde el primer momento en que la tuvo en sus manos en los años setenta, ahora estaríamos contando otra historia. Para algunos, esta incapacidad llevó a retrasar hasta en una década el progreso de toda la industria de la informática personal y obligó a la mayoría de usuarios a convivir con sistemas mucho menos avanzados durante la década de los setenta y de los ochenta. Para mucha gente, Xerox pervivirá en su recuerdo como la empresa que inventó el ordenador personal moderno pero fue incapaz de difundirlo.

Fin Capítulo 11.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 12

### Apple y el Lisa

_Vamos a dejar huella en el universo._<br>Steve Jobs (refiriéndose al Lisa)

Por las mismas fechas en que Xerox dedicaba todos sus esfuerzos al Alto y el Star, una joven empresa con nombre de manzana, Apple Computer, empezaría a comercializar de forma abierta lo que sería el primer ordenador personal con posibilidades gráficas como las de los sistemas del PARC. Se trataba del Lisa.

Steve Jobs, fundador de Apple junto con Steve Wozniak, tenía claro que no había entrado en la industria de la informática para perpetuar los obsoletos sistemas informáticos dominantes en aquellos años. Jobs quería hacer algo grande, distinto, algo que sacara a la informática del reducto en el que se encontraba para colocarla en las mesas de trabajo de todos los ciudadanos del mundo. Lisa no sería el primer ordenador creado por Apple pero sí el primero en plasmar esos ambiciosos objetivos.

Para diseñar el Lisa, Jobs contrató en 1979 a dos ingenieros procedentes de Hewlett-Packard, John Couch y Ken Rothmueller, pues el modelo inicial que tenía en mente era el del Hewlett-Packard 3000, un miniordenador que, al final, no tendría nadar que ver con el producto realizado por la compañía de la manzana. La principal característica del Lisa no se debería tanto a estos dos ingenieros como al trabajo de Bill Atkinson y las rápidas rutinas de software que sería capaz de desarrollar para conseguir gráficos bitmap. Y especialmente, a una productiva visita que Apple tendría oportunidad de realizar al PARC de Xerox.

### La fuente de inspiración

En diciembre de 1979, los investigadores del PARC recibían su visita más especial, aunque en aquellos momentos ellos aún no lo sabían. Se trataba de una delegación de Apple que venía a comprobar cuales eran aquellas tecnologías que tanto fascinaban a los visitantes de Xerox en Palo Alto. Durante esa visita, Steve Jobs, Bill Atkinson, Mike Scott (por entonces Presidente de Apple) y diversos ejecutivos más de la empresa, tuvieron ante sus ojos la representación de todos los nuevos conceptos destinados a "aumentar el intelecto humano": el ratón, las ventanas, el bitmap, los iconos, los menús.

Xerox hacía tiempo que había dejado de realizar sesiones públicas de presentación de sus inventos pero había permitido la visita de Apple con la intención, según diversas fuentes, de convencer a la compañía para que fabricara para ella los ordenadores potentes y económicos, basados en esas nuevas tecnologías, que la empresa de las fotocopiadoras había demostrado ser incapaz de producir (cabe decir que Xerox acababa de realizar una importante inversión en acciones de Apple).

Pero el equipo de Apple iba a ir mucho más lejos. De hecho se ha hablado mucho de lo que Apple se llevó durante aquella visita del PARC, de las tecnologías que copió o dejó de copiar del centro de Xerox. La verdad es que lo que vieron ese día los ejecutivos y técnicos de Apple fue exactamente lo que habían visto decenas de personas antes que ellos. Sin embargo, el grupo de Apple, y muy particularmente Steve Jobs, salió de las instalaciones del PARC con un preciado, y al parecer único, tesoro: la idea e inspiración de que era posible crear un sistema informático intuitivo y familiar, agradable y divertido de utilizar.

Y Apple decidió ir más allá con todo lo que había visto ese día en el PARC y empezar una revolución por su cuenta para toda la informática personal. Una revolución que cambiaría la forma en cómo trabajamos y en cómo pensamos. La gente de Apple querían cambiar el mundo y lo consiguieron, aunque un poco más tarde.

### Lisa: el preludio de una revolución

Después de esa visita al PARC, el modelo del Hewlett-Packard 3000 se arrinconó indefectiblemente. Trip Hawkins reescribió el proyecto del Lisa intentado integrar todas las innovaciones que habían visto en el PARC, la mayoría procedentes de Engelbart, y especialmente las ventanas, los iconos y el uso del ratón. Todas y cada una de esas nuevas concepciones fueron reinventadas y rediseñadas en los laboratorios de Apple. La idea seguía siendo comercializar el ordenador como una herramienta de trabajo para todas las oficinas pero al integrar las nuevas nociones de Xerox, Lisa dejaría a años luz al resto de ordenadores que se comercializaban en ese momento.

El trabajo de Bill Atkinson sería fundamental: generar todas las rutinas que permitirían controlar al ordenador mediante una interfaz gráfica, es decir, mediante iconos y ventanas. Pero al proyecto del Lisa también se incorporarían con el tiempo otros ingenieros clave procedentes de Xerox, como Larry Tesler, Owen Densmore, Bob Beleville, Barbara Koalkin, Alan Kay y Steve Capps.

A mediados de 1980, Apple ya había superado algunas de las ideas del PARC, después de percatarse que el mercado exigía una mayor sofisticación de muchas ellas. Así, el Lisa de Apple, inspirado a imagen y semejanza primero de un modelo Hewlett-Packard y después del Alto y el Star, acabaría por superar ampliamente a todos ellos. El equipo de Apple no se dedicó a hacer una mera adaptación del Alto sino que amplio y mejoró todas y cada una de las ideas que de él recibió. No sólo tomaron prestadas las ventanas superpuestas y los menús que aparecían en la pantalla del SmallTalk sino que inventaron conceptos como la barra de menús siempre fija en la parte superior de la pantalla, los menús desplegables, las barras de desplazamiento por las ventanas, el ratón de un solo botón, la idea de cortar y pegar cosas en un portapapeles o la papelera en el escritorio. Una de las principales diferencias del Lisa con respecto al Alto fue las posibilidades de manipulación directa que tenía el usuario. Con el Lisa era posible manipular cualquier cosa de la pantalla sin tener que acudir en absoluto a menú alguno, mientras que el Alto requería en la mayoría de casos utilizar las opciones de los menús y el Star no permitía arrastrar las ventanas por la pantalla para cambiarles el tamaño o moverlas de sitio. Lisa también era mucho más efectivo en cuanto a los clics necesarios para seleccionar, abrir o cerrar objetos. Pero el mayor paso se daría con el Desktop Manager. El Desktop Manager era el programa con el que se topaban todos los usuarios nada más encender el ordenador. Desde él se realizaban todas las tareas comunes de mover, copiar o renombrar archivos. Como su nombre indica, era el administrador del Escritorio y suponía la máxima integración de la metáfora del Escritorio jamás realizada.

### Nada que ver con los "compatibles PC"

Cuando Bill Atkinson, Dan Smith y Frank Ludolph concibieron en 1982 este interface, el 99% de la industria informática personal estaba dominada por el CP/M y el DOS. El CP/M fue el primer sistema operativo en funcionar sobre máquinas de diferentes fabricantes. A finales de los años setenta se había convertido en el sistema operativo dominante aunque nunca llegó a ser un estándar tan extendido como su heredero, el DOS, que además de coger el testigo del liderato se inspiró notablemente en él.

El DOS apareció en 1981, cuando IBM decidió entrar de lleno en el mercado del ordenador personal con el IBM PC, un ordenador sólo algo más rápido que la mayoría pero apoyado por el gran peso psicológico que ante los consumidores tenía la marca "IBM". Para su procesador Intel 8088, IBM encargó a Microsoft un sistema operativo, el futuro MS-DOS, y permitió que Microsoft lo licenciara abiertamente. Así, cualquiera podía fabricar ordenadores capaces de ejecutar este sistema, ordenadores que se denominarían "compatibles IBM". Estos clónicos no competían en calidad ni prestaciones con IBM sino en lo que más daño hace: en precio. La gran profusión de fabricantes de clónicos y la credibilidad de una empresa como IBM llevaron a convertir a estos sistemas en el estándar de la industria (sistemas a los que también se identifica como "PCs" a pesar de que en inglés estas siglas significan "Personal Computer" en general). Sin embargo, el MS-DOS era un sistema arcaico que no reflejaba en modo alguno los avances realizados en el interface de los ordenadores por investigadores como, por ejemplo, los del PARC.

Cuando en 1983 Apple lanzó el Lisa, el mercado ya se encontraba ampliamente dominado por el PC de IBM. Así pues, por aquellas fechas, mientras el Lisa disponía de un escritorio virtual como interfaz para interactuar con el ordenador y en el que los disquetes, las carpetas y documentos estaban representados por iconos y los archivos se abrían mediante un simple gesto con el ratón, el resto del mundo se peleaba a diario con unos sistemas tiranizantes e irracionales que obligaban al usuario a recordar largas líneas de comandos simplemente para abrir un documento, y con una limitación tal del número de caracteres en los nombres de los archivos que uno acababa con el disco duro infestado de referencias crípticas e imposibles de identificar con el paso del tiempo.

Ciertamente el Lisa fue toda una revolución avanzada a su tiempo e incomprendida por la mayoría. Pero la novedad no fue la única causa de su escaso éxito. El principal problema era, como ocurriera con los productos de Xerox, su precio. El bitmap, el ratón y el sistema de interfaz gráfica con iconos y ventanas y la multitarea (el Lisa probablemente fue el primer ordenador personal en ofrecer multitarea, es decir, la posibilidad de realizar varias acciones simultáneamente) tenían grandes exigencias de hardware y de memoria (esta última era especialmente cara en aquellos años). Si bien primero se vendió sólo con unidad lectora de disquetes, acabó siendo necesario incorporar un disco duro, algo muy raro y caro en aquellos momentos. El precio final superaba el millón y medio de pesetas de la época y además era una auténtica tortuga, no tanto como el Star de Xerox pero aún demasiado. En definitiva, el Lisa era demasiado lento y caro para lo que el mercado estaba preparado para comprar. Sin embargo marcaría el futuro de Apple y del resto de la industria informática.

### QUIÉN ES...

### Apple Computer<br>(Cupertino, CA, 1976 ­ ...)

Fundada por Steve Jobs y Steve Wozniak en el garaje del padre del primero, Apple se convertiría al cabo de unos años en la única empresa capaz de sobrevivir a la entrada de IBM en el mercado de la informática doméstica. Jobs y Wozniak empezaron en 1976 fabricando el Apple I, un kit que les daría suficientes beneficios como para diseñar el Apple II, el primer producto de éxito de la compañía. El Apple II fue el primer ordenador personal que no se vendiera en forma de kit y la aparición en el mercado de la hoja de cálculo electrónica VisiCalc exclusivamente diseñada para él lo convirtió en un éxito de ventas. El Lisa, lanzado en 1983, plantaría las semillas que fructificarían en 1984 con el Macintosh, el producto que ayudaría a consolidar la empresa. Los Macs se convertirían así en una especie de segundo estándar frente al dominante compatible PC. En 1992 Apple se unió a IBM y Motorola en una alianza estratégica para la tecnología del futuro que daría sus primeros frutos en marzo de 1994 con el lanzamiento de los primeros Power Macintosh. Los Power Macintosh son una nueva generación de ordenadores basados en los nuevos chips RISC PowerPC desarrollados conjuntamente por Motorola, IBM y Apple. A partir de ese momento, Apple realizó una progresiva transición de toda su gama de productos hacia la tecnología PowerPC y decidió licenciar el sistema operativo del Macintosh, lo cual permitió que apareciera un mercado de "compatibles Mac" inexistente hasta el momento. Aunque esta iniciativa duraría bien poco pues Apple daría finalmente marcha atrás con las licencias del Mac OS en el verano de 1997.

Apple ha destacado siempre por ser una de las empresas más innovadoras de la industria de la informática personal, fue la primera en lanzar un asistente personal digital en 1992, el Newton, y una de las primeras en apostar de lleno por Internet. Sin embargo, la empresa ha pasado por numerosos períodos de crisis que la han obligado a reestructurarse en diversas ocasiones. Actualmente Apple cuenta con más de 13.000 empleados en todo el mundo y ha vendido casi 30 millones de ordenadores entre sistemas Macintosh y Power Macintosh. En diciembre de 1996 Apple compró Next Inc. y recuperó a su fundador, Steve Jobs, que desde setiembre de 1997 ha ejercido el cargo de presidente y consejero delegado de Apple de forma interina.

### Y TAMBIÉN...

### El primer ordenador conceptualmente moderno

El Lisa pesaba 21,8 Kg e incorporaba un microprocesador Motorola 68000 a 5 MHz, 1 Mb de RAM, dos disqueteras de 5,25 pulgadas para discos de 860 Kb, un disco duro de 5 Mb, un teclado separado, ratón de un solo botón y un monitor monocromo integrado a la carcasa del ordenador con una resolución de 720 x 364 píxels y 12 pulgadas de tamaño. Características todas ellas actuales (no en cifras sino en presencia) que el Lisa presentaba de forma estándar por primera vez en la industria de la informática personal.

### Literalmente enterrados

En setiembre de 1989 Apple decidió eliminar definitivamente al Lisa del mercado (se había dejado de fabricar en 1985 pero había un abundante stock en circulación y por vender) de una forma más bien curiosa. Bajo la atenta mirada de guardias de seguridad armados, 2.700 Lisas fueron sepultados en el vertedero de Logan, Utah. Lisa desapareció del todo y Apple se embolsó a cambio una pequeña subvención de la administración norteamericana por haberse desprendido de este modo del sobrante stock del Lisa.

Fin Capítulo 12.<br>
[Volver a Indice](#indice)

<br><br>

### CAPITULO 13

### De la interfaz gráfica a la interfaz humana: el Macintosh

_Es mejor ser un pirata que unirse a la Armada._<br>Steve Jobs (explicando el atractivo que tenía el formar parte del equipo original del Mac)

La materialización de todos los conceptos anteriores en una máquina sería popularizada finalmente por un famoso y entrañable ordenador sonriente: el Macintosh. El popular Mac supondría una apuesta por la informática intuitiva y amigable que, puesta en duda durante sus primeros años de vida, acabaría arrastrando al resto de la industria.

Sin embargo, la historia de este ordenador siempre ha estado ligada a un idolatrado personaje, Steve Jobs, y no hace justicia a todo el equipo creador del Mac. De hecho, podríamos decir que este revolucionario producto se debió a la inspiración de un olvidado personaje, a la ambición y energía de otro y a la dedicación absoluta de un equipo entregado a un ideal. A todos ellos, en su justa medida, hay que recordarlos como padres de un producto que demostraría que todos esos conceptos e ideas, lanzados por algunos pioneros años antes y anticipados en algunos ordenadores de "laboratorio", podrían sintetizarse en un producto rentable y comercializable y serían mucho más que un mero sueño de un puñado de jóvenes.

### El hombre en la sombra

El Macintosh fue primero y ante todo producto de la inspiración de Jeff Raskin. Raskin fue quien inició el proyecto del Macintosh en Apple y lo defendió a viento y marea cuando la empresa aún no creía en él. Raskin era un acérrimo defensor de que lo importante son las personas y no las máquinas y que estas últimas debían aliviar las flaquezas humanas en lugar de obligar a los individuos a doblegarse ante sus exigencias para poder aprovechar sus posibilidades. Desde el principio, Raskin estaba convencido de que las interfaces de la época eran la antítesis de lo que el hombre necesitaba para desarrollar sus capacidades productivas y mentales. Es decir, que en lugar de suponer una plataforma para extender nuestros conocimientos, eran un freno.

Jeff Raskin fue contratado por Apple inicialmente para redactar el manual del Apple II y pasó después a integrarse al equipo del Lisa. Fue el principal promotor de la visita al PARC que tanto influenciaría a la compañía de la manzana (se pasó semanas enteras insistiendo a los ejecutivos de Apple de la necesidad de realizar esa visita. Jeff Raskin había trabajado en estrecho contacto con el laboratorio del futuro de Xerox y sabía que en él se estaba trabajando en tecnologías muy interesantes). Después de esta visita, Jobs se volcaría de lleno en la implementación de los conceptos del PARC en el Lisa y Raskin sería apartado del proyecto. En este punto se marcaría un nuevo objetivo: el de un ordenador sencillo de usar, como el Lisa, pero sin tantas pretensiones y a un precio mucho más accesible. La idea básica de Raskin se simplificaba en una frase: construir un ordenador tan fácil de usar como cualquier electrodoméstico o, lo que es lo mismo, construir un ordenador con mentalidad de electrodoméstico para que cualquier persona pudiera utilizarlo. Cuando Jobs fuera a su vez apartado del Lisa quedaría encandilado con esta idea del ordenador-electrodoméstico.

### La llegada del motor impulsor del proyecto

Steve Jobs ha sido definido con muchos calificativos: empresario emprendedor, visionario, pionero informático, revolucionario y quien sabe cuantos adjetivos más. Pero lo que muchos libros y artículos dedicados a su persona olvidan mencionar es su enorme egocentrismo y ambición. Jobs quería cambiar el mundo tanto para que la sociedad pudiera mejorar su calidad de vida como para que él pudiera inscribir su nombre en los anales de la historia. Jobs quería ­y sigue queriendo­ hacer historia. Y el Macintosh triunfaría no sólo por esos primeros calificativos que describíamos de su personalidad sino también, y en gran medida, por estos últimos. El empuje, perseverancia, tenacidad y, también, tozudez, de este hombre le darían buena parte de su personalidad al Mac.

De Jobs se han dicho muchas cosas y algunas son importantes para entender el producto que creó después de retomar la idea de Raskin y echar a éste del proyecto. Jobs no sería ­ni es­ un técnico experto ni tan siquiera un creador de dotada imaginación, pero sí era alguien capaz de reconocer una buena idea y de absorber, eficazmente, las ideas de los demás. De todo ello se beneficiaría el Macintosh, aunque también de su impetuosidad, prepotencia y ambición. Jobs conseguiría con su capacidad de fascinación y encanto personal transmitir cualidades similares al Mac pero, a cambio, éste también sufriría de sus excesos y pretensiones.

### Los verdaderos constructores

Pero tal vez lo más importante que hizo Steve Jobs fue conseguir hipnotizar como lo hizo a todo un equipo de personas que dieron lo mejor de sí porque no estaban inventando una máquina, estaban inventando el futuro y creían apasionadamente en lo que hacían. A todos ellos, la historia también les debe un apunte.

En primer lugar, la personalidad del Mac se debió en gran parte al programador de la ROM: Andy Hertzfeld. El Mac incorporaría algo completamente inusual hasta el momento, una ROM fija o código residente en un chip del ordenador: el ADN de la máquina. En la ROM habría la Toolbox del Mac, una de las principales razones de éxito de este ordenador. Con la Toolbox se abrían las puertas para que cualquier empresa pudiera desarrollar aplicaciones para el Mac que siguieran unos mismos parámetros.

El Mac no sólo adaptaría toda la iconografía y metáfora gráfica del Lisa, mejorándola y simplificándola, sino que conseguiría algo inaudito hasta el momento ­inaudito aún hoy en día fuera de él­, la coherencia en el funcionamiento de todas las aplicaciones y operaciones. Joanna Hoffmann, otro componente de ese primer equipo, desarrollaría lo que llamarían el "Human Interface Guidelines", una especie de guía para implementar la interfaz humana y que permitirían que la pantalla del Mac tuviera siempre un aspecto similar, fuera cual fuera la aplicación en la que estuviéramos, que las principales opciones y menús estuvieran siempre en el mismo lugar, y que las principales acciones y operaciones se realizasen con mecanismos parecidos (arrastrar texto, seleccionar objetos, salir de los programas, etc.). Ello hacía que una vez se conociese el funcionamiento de una aplicación, se pudiese intuitivamente utilizar cualquier otra aplicación nueva con una curva de aprendizaje mínima ("cuando conoces una aplicación las conoces todas" afirmaban pletóricos los primeros usuarios de Mac). El equipo de desarrollo de la interfaz humana se dio cuenta de que la facilidad de uso de un ordenador no sólo dependía de implementar al máximo posible la metáfora gráfica del Escritorio de Alan Kay, sino también de conseguir que el usuario fuera capaz de moverse por él intuitivamente. La consistencia en la interfaz ayudaba enormemente a ello y la Toolbox de la ROM fue la pieza crucial para hacerlo posible. Esta consistencia, adelantada por los pioneros del PARC con su declaración de principios para el Star, aún no ha sido igualada por ningún otro entorno informático hasta el momento.

El encargado de desarrollar la interfaz gráfica del Lisa, Bill Atkinson, también colaboró con el equipo del Mac creando uno de los programas que mejor demostraban las posibilidades gráficas de este ordenador: el popular MacPaint. De hecho, la interfaz del Mac partió del Lisa por completo y Bill Atkinson dejó también su impronta en el Mac. Otro programa crucial para mostrar las posibilidades de esta máquina y que se vendió con ella fue MacWrite, desarrollado por Randy Wigginton.

Quien definió el aspecto visual del Mac fue Susan Kare, la encargada del diseño gráfico del Mac, de las imágenes e iconos de su interfaz. Susan también fue la creadora de los tipos de caracteres con que se vendería la máquina y que identificaría de inmediato a los documentos realizados con Mac.

Pero al igual que el Lisa, el programa más importante del Mac fue el que el usuario se encontraba en primer término al encender la máquina: el programa que permitía interactuar con el ordenador, con los programas, con absolutamente todo. En el Mac, este programa se denominó Finder (el "Buscador" pues es el encargado de encontrar en el ordenador todo lo que el usuario solicita) y sus creadores fueron Bruce Horns y Steve Capps. En el Finder es dónde mejor se plasmaba toda la filosofía de la metáfora gráfica del Escritorio.

### La dificultad de ser pioneros

El primer modelo de Macintosh que salió al mercado en 1984 adolecía de algunas carencias que deberían ser solventadas de inmediato (algunas debidas a la obcecación de Jobs). Por ejemplo, los 128 K de RAM del primer Mac eran claramente insuficientes para un ordenador con sus exigencias gráficas, tampoco incorporaba disco duro, algo que se convertiría en esencial a partir de ese momento, o por ejemplo, el teclado no tenía teclas de movimiento del cursor (las típicas teclas con flechas de las que Jobs no quería ni oír hablar porque afirmaba que sólo eran necesarias para sistemas operativos rudimentarios como el DOS).

El Mac tuvo también que enfrentarse a una oleada de críticas discriminatorias que lo tildaban de ordenador-juguete (posiblemente porque la mayor complejidad de los sistemas no intuitivos se había llegado a asociar con una especie de expresión de su potencia o incluso de su masculinidad/virilidad ­o de la de aquellos que eran capaces de utilizarlos­, algo que un ordenador con dibujitos en la pantalla no estaba en absoluto en condiciones de igualar). Pero las posibilidades de un entorno como el desarrollado por Apple en el Macintosh acallarían con el tiempo todos los prejuicios y el futuro confirmaría la dirección tomada por esta empresa.

El coraje de que hizo gala Apple lanzando un producto como el Mac debe ser bien comprendido. En 1984, cuando apareció finalmente el Macintosh, el mercado estaba completamente dominado por los compatibles IBM con el sistema operativo que Microsoft había vendido a esta gran compañía: el DOS. Un sistema críptico y complicado que sin embargo había invadido el mundo gracias a su superioridad sobre el anterior CP/M y a la estratégica visión de Microsoft e IBM al licenciarlo abiertamente y prácticamente regalarlo con cualquier máquina compatible. Lanzar en esos momentos un ordenador no compatible con el estándar era para muchos una osadía que iba a costarles muy cara (algunos, incluso creían que "debían" pagarla muy cara). Que además ese ordenador tuviera un aspecto poco "serio" con iconos y gráficos, es decir, tuviera una concepción completamente distinta de cómo debía ser la informática personal, era casi una hazaña y toda una provocación al stablishment de la informática. Paradójicamente, cinco o seis años después la mayor parte de la industria se decidía a apostar por esa concepción. Hoy en día el Mac es una referencia obligada a la hora de buscarle antecedentes a la primera implementación comercial con éxito de los ordenadores humanizados.

### QUIÉN ES...

### Steve Jobs<br>(Los Altos, California 1955)

Huérfano adoptado por Paul y Clara Jobs en Mountain View, Steve Jobs creció en el mismo centro neurálgico del desarrollo tecnológico mundial. Después de terminar el instituto en 1972 se matriculó en el Reed College en Portland al que sólo acudió durante un semestre abandonándolo para ir a trabajar en Atari. Allí, renovaría su amistad con Steve Wozniak a quién había conocido trabajando en verano en la Hewlett-Packard y a quien ayudaría a fabricar las ilegales "cajas azules" telefónicas, tan populares en aquellos años, mientras participaba activamente en el Homebrew Computer Club, el club de los aficionados a la electrónica y la informática. En 1976 ambos dejaron Atari para fundar Apple Computer en el garaje de los padres de Jobs. En 1985, después de haber sido el más ferviente defensor y promotor de los grandes proyectos que asumiría la compañía, entre ellos el Lisa y el Macintosh, abandonaría Apple, presionado por el presidente y CEO John Sculley, para fundar NeXT Inc., una empresa en la que pretendería repetir la revolución que había conseguido con el Macintosh con una nueva generación de ordenadores personales. Sin embargo, las cosas no fueron como había previsto y NeXT cerró su división de hardware en 1993 para pasar a comercializar sólo software y lanzó una versión del sistema operativo NextStep para los PCs equipados con procesadores Intel. A pesar de lo avanzada de su arquitectura y su potencial y posibilidades, NextStep no consiguió obtener una cuota de mercado mínima para su subsistencia. En diciembre de 1996 Steve Jobs vendió la compañía a Apple Computer por 400 millones de dólares y volvió a la empresa que fundara en 1976.

Visionario, de fuerte carácter (incluso despótico para algunos) y con mucho carisma, Jobs se convirtió en multimillonario antes de los treinta años cuando Apple Computer empezó a cotizar en bolsa en 1980, fortuna que creció considerablemente en 1995 cuando el éxito de la película Toy Story disparó las acciones de la empresa que la hiciera posible, la Pixar Animation Studios, una compañía que Jobs compró a George Lucas en 1991 para penetrar en el mercado de Hollywood.

### Y TAMBIÉN...

### El mítico garaje y el socio olvidado

Es harto conocida la historia de que Apple empezó en un garaje con dos Steves como protagonistas: Steven Paul Jobs y Stephen Gary Wozniak. El primero, vehemente, locuaz y enérgico emprendedor capaz de atraer hacia su causa a cualquiera, el segundo, estudioso y enamorado de las nuevas tecnologías, aportaría la experiencia técnica necesaria. Sin embargo, en ese garaje no nació el Mac propiamente sino el Apple I, el primer modelo comercializado por Apple. El Mac se desarrollaría gracias a los beneficios generados por los primeros modelos de Apple, especialmente el Apple II, y por la experiencia técnica desarrollada y aplicada en el Lisa.

Por otro lado, no es nada conocida la existencia de un tercer socio en el momento de creación de Apple. Steve Jobs convenció a un compañero de trabajo de Atari, Ron Wayne, para que se uniera a ellos en la constitución de la empresa, algo que Wayne no resistiría más de unos meses. Acabaría abandonando el barco asustado por las numerosas deudas que inicialmente amenazaban a la compañía.

### Manzanas por todas partes

El nombre de Macintosh lo eligió Jeff Raskin de entre muchos otros descartados previamente. Macintosh era la variedad de manzana preferida de Raskin, y además, iba en consonancia con el nombre de la empresa.

### El gato de la Canon

Después de ser apartado del proyecto del Macintosh, Jeff Raskin tuvo oportunidad de plasmar su propia visión de cómo debían ser estos electrodomésticos de la información. Construyó un ordenador muy sencillo de utilizar y que se comercializó como el Canon Cat, sin embargo, su efecto en la industria informática fue mínimo.

### El hermano pequeño del Lisa

El Mac nació como hermano pequeño del Lisa, un ordenador con menos pretensiones pero igual de sencillo de utilizar que pudiera venderse a un precio más razonable. Nadie esperaba que el Mac pudiera llegar a eclipsar al Lisa y a relanzar a Apple como llegó a hacerlo.

### Por qué humano más que gráfico

De la interfaz del Macintosh se dice que es mucho más que una simple interfaz gráfica, que es una interfaz humana. "Human interface" es una expresión nacida con y para el Mac y que significa que estamos ante un entorno que es mucho más que una simple metáfora gráfica del entorno de trabajo real. Si los experimentos de "laboratorio" realizados hasta el momento habían dado lugar a ordenadores gráficos como el Alto o el Star o incluso el Lisa, el Mac suponía un paso más. Surgía así por primera vez un sistema intuitivo y pensado para las personas. La humanización de la informática empezaba a dejar de ser una visión para convertirse cada vez más en realidad.

Fin Capítulo 13.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 14

### Los grandes olvidados: Atari ST y Commodore Amiga

_Ordenadores para las masas, no para las clases._<br>Jack Tramiel

En el proceso de acercamiento de la informática al hombre, en este arduo camino que venimos intentado trazar a lo largo de estas páginas recordando a aquellos que lucharon por la idea de crear máquinas que estuvieran al servicio de las personas, jugó también un papel importante la aparición de una nueva forma de presentar la información. Hablábamos capítulos atrás del nacimiento del término hipertexto y la importancia que este concepto tendría para la futura definición de los sistemas o interfaces gráficas, llamémosles ya humanas. De hecho, el término hipertexto está directamente asociado con el término hipermedia que no es más que lo que actualmente ha revolucionado el mercado informático mundial y, especialmente, el mercado editorial: la multimedia. Multimedia es la combinación de texto, imágenes, sonido y animación, algo que existía desde hace tiempo por separado pero que sólo unos pocos ordenadores eran capaces de ofrecer conjuntamente. La multimedia es en la actualidad todo un fenómeno de masas cuya única mención parece asegurar la calidad o posibilidades de un producto y que se utiliza prácticamente para definir cualquier tecnología de la información que se precie de "moderna". Las armas por excelencia de la multimedia son, sin embargo, los CD-ROMs, el primer medio en combinar todos los elementos pero no el único. Internet se augura como el gran difusor potencial de contenidos multimedia y se convierte así en el otro gran medio multimedia por excelencia. El soporte del futuro será sin duda el resultante de las numerosas propuestas existentes (la fibra óptica, el híbrido del cable, los satélites, etc.) cuya viabilidad pasa, principalmente, por la necesidad de ofrecer el suficiente ancho de banda como para transportar imagen en movimiento, sonido, datos informáticos, voz, texto y todo lo que se nos ocurra en un futuro.

### Una novedad con unos cuantos años

Pero la multimedia no es un fenómeno tan reciente como lo pintan algunos, al menos en su concepción básica de combinación de medios diversos para transmitir información. La popularidad actual de la multimedia se debe exclusivamente a un fenómeno concreto: la aparición de hardware con la capacidad de manejar multimedia. El CD-ROM existe desde hace más de una década, concretamente se fecha su año de nacimiento en 1985 (sólo una año después que el Macintosh de Apple). Por su gran capacidad se intuyó de inmediato que se trataba de un medio con grandes posibilidades. Sus más de 600 Mb de espacio lo convierten en el medio idóneo para almacenar datos, imágenes, sonidos y aplicaciones de gran tamaño. Sin embargo no ha sido hasta principios de la década de los noventa que el CD-ROM ha irrumpido en nuestras vidas como el medio de difusión de información por excelencia. ¿Por qué tanto retraso en convertirse en un medio de utilización universal? La respuesta pasa por el hardware. Durante prácticamente los cinco primeros años de vida del CD-ROM existían dos grandes inconvenientes que impedían su avance como principal difusor de multimedia. Uno era la lentitud de los lectores que hacían inviable la visualización correcta de animaciones o vídeo desde el disco. El segundo era la incapacidad del hardware de la época para mostrar esas imágenes y sonidos. La mayoría de ordenadores de finales de los ochenta, es decir, los que utilizaban alguno de los sistemas operativos de Microsoft, no tenían ni memoria, ni capacidad gráfica ni de audio suficiente para ejecutar aplicaciones multimedia, lo cual no era tan extraño pues los sistemas operativos de Microsoft de esa época (DOS y Win 1x y 2x) no estaban pensados para la multimedia. Esto no llegaría hasta principios de los años noventa de la mano de Windows 3.0 y de lectores cada vez más rápidos que empujarían al avance del hardware en este sentido (de la inicial simple velocidad se pasó a la doble velocidad, mínimo indispensable para ver imágenes en movimiento, y en el momento de escribir estas páginas la velocidad normal ya es de x24).

### Multimedia y humanización

Y ¿por qué es tan importante la multimedia en nuestra historia? Básicamente porque podemos considerar a la multimedia como la expresión natural de la informática humanizada, aunque ésta se encuentre tan sólo en una fase aún adolescente. Esta humanización pasa principalmente por tres elementos. En primer lugar, por la simplificación del uso de la máquina haciéndola más intuitiva y sencilla. En segundo lugar, por convertirla en algo divertido y amigable y romper con la barrera que separaba a las personas y su forma de pensar de estas máquinas. Y, finalmente, por permitir de este modo que todo el mundo pueda acceder a todo el potencial de las máquinas con un mínimo esfuerzo. El esfuerzo debe ponerse en el conocimiento no en la forma de acceso a él. En estos tres elementos, la multimedia tiene un peso muy importante para ayudar a hacer más fácil el uso de las máquinas, para dotarlas de una cara humana más amigable y para universalizar el acceso a la información, entendiéndose aquí por "universalizar" el facilitar, gracias a su sencillez de uso, que todo el mundo pueda acceder al conocimiento. Por todo ello, la multimedia ha evolucionado desde los primeros iconos y sonidos separados a la combinación de medios que conocemos en la actualidad y que forman parte inherente de las nuevas tendencias en interfaces.

Pero la multimedia ya existía antes del boom de principios de los noventa. O, al menos, se había anticipado en forma de productos muy avanzados para su tiempo. Este y el próximo capítulo quieren recordar cuales fueron estos productos y ponerlos en su lugar pues, si bien todos ellos serían superados rápidamente por el mercado, tuvieron el honor de presentar la multimedia en sociedad.

### Dos pioneros adelantados a su tiempo: Atari ST y Commodore Amiga

Cuando se habla de los primeros ordenadores multimedia, mucha gente piensa inmediatamente en el Macintosh de Apple que a finales de los años ochenta era un ordenador prácticamente multimedia y que sería uno de los primeros fabricantes, sino el primero, en comercializar ordenadores con lector de CD-ROM incorporado de serie. Sin embargo, dos fabricantes de ordenadores se le habían adelantado a Apple creando antes que nadie los primeros ordenadores multimedia. El hecho de que ambos fabricantes hayan desaparecido y de que no consiguieran una participación de mercado como la de Apple ha empañado su recuerdo y el merecido honor de ser considerados los primeros ordenadores multimedia, olvido muy común especialmente entre los que han descubierto la multimedia recientemente.

Primero hay que recordar a Atari, que se avanzó unos meses a Commodore. Atari Corporation fue absorbida por JTS Corporation (un fabricante y distribuidor de discos duros) el treinta de julio de 1996 día en que desaparecía como tal dejando a unos cuantos millones de usuarios sin referencia alguna, soporte ni mantenimiento. Tal dramática desaparición hubiera sido inimaginable 11 años atrás, cuando la compañía copaba el sector de la música con sus nuevos modelos de máquinas multimedia. El Atari ST, lanzado en enero de 1985, fue un ordenador claramente anticipado a su tiempo. Ya disponía de una interfaz gráfica basada en iconos que, a pesar de no ser tan intuitiva como la del Macintosh, era mucho más manejable que el sistema DOS de los compatibles IBM (de hecho compartía muchas de las características básicas que después adoptaría Windows). Pero, además, era una máquina que, por un precio más que accesible, ofrecía gráficos en color, funcionalidad mediante ratón e interfaz MIDI. El MIDI (Musical Instrument Digital Interface) es una opción obligada para cualquier músico que quiera trabajar con ordenadores y en 1985 Atari lo ofrecía de serie en sus máquinas por menos de 1.000 dólares (unas 140.000 Ptas.). Sobra decir que el Atari ST ayudó en gran medida a difundir el uso de la informática en el sector musical y se convirtió de inmediato en el estándar de esta industria. Aún hoy puede encontrarse en los estudios de más de un profesional (y de más de un nostálgico) que descubrieron las posibilidades de la informática musical gracias a este ordenador.

El caso de Commodore es también sorprendente. Tampoco existe como empresa y también ocupó las primeras filas de la informática mundial de los ochenta con diversos modelos de ordenadores, entre los que se cuentan los varios Amiga, pero sobretodo y especialmente, el VIC-20 ­el primer ordenador en color por debajo de los 300 dólares­ y el popular Commodore 64 ­el modelo de ordenador más vendido de todos los tiempos (más de 10 millones de unidades). Entre los dos, Commodore podría alcanzar la cifra de 17 a 22 millones de máquinas vendidas.

También en este caso, una vez desaparecida la empresa, sigue existiendo un "mercado Commodore" tan o más animado que el "mercado Atari" aún latente. Los usuarios de Commodore 64, que aún los hay en activo, y los numerosos del Amiga siguen consumiendo productos y por ello sigue apareciendo software, emuladores de Amiga para otros ordenadores, componentes, etc. y se mantiene viva una relativamente variada gama de publicaciones para este entorno. La empresa cerró pero muchos usuarios se negaron a abandonar sus máquinas. El final trágico de esta compañía tampoco hace justicia a su papel en la informática moderna.

Commodore realizó significativas contribuciones a la tecnología. El Commodore 64 fue, por ejemplo, el primer ordenador con un chip sintetizador (el Sound Interface Device); el SX-64, lanzado en 1983, fue el primer portátil en color; y el Plus/4 ya integraba software en la ROM. Pero fue en julio de 1985 cuando Commodore presentó en Nueva York una nueva generación de ordenadores. La más revolucionaria y avanzada para su tiempo y, también, paradójicamente, la que le hundiría económicamente. El Amiga estaba tan adelantado a su tiempo que nadie, incluyendo al departamento de márqueting de Commodore, supo realmente llegar a apreciarlo (en aquellos momentos parece ser que muy pocas personas supieron captar la importancia de una máquina tan avanzada en gráficos, sonido y vídeo).

En un momento en que la mayoría de ordenadores apenas podían mostrar 16 colores EGA, el Amiga alcanzaba los 4096 e incorporaba chips aceleradores de vídeo y salidas de vídeo para televisión y cámara, algo que aún no es una opción estándar en los ordenadores actuales (y que encarece bastante los sistemas). Disponía además de sonido estéreo y fue el primer ordenador en incorporar síntesis de voz y ser capaz de leer los textos de pantalla. Y continúa siendo el único sistema capaz de mostrar múltiples ventanas a diferentes resoluciones en un solo monitor. Aunque lo más sorprendente del Amiga era el sistema operativo que había diseñado para él Carl Sassenrath. Desde el primer modelo, el Amiga dispuso de un sistema de ventanas para interactuar con la máquina y de gráficos en color, sonido y vídeo de suficiente calidad como para que más de un profesional lo incorporase a su estudio. Se trataba de una máquina especialmente concebida para ofrecer multimedia y disponía de un diseño de hardware especialmente sofisticado y adecuado a ello, además de ser un sistema multitarea (poseía multitarea preemptiva desde el principio, es decir, el ordenador era capaz de ejecutar diversas tareas simultáneamente) en una época en que esta capacidad no estaba en absoluto al alcance de los usuarios domésticos. Además, el sistema operativo era escriptable e incluía un sistema de mensajes y, lo que es mejor, funcionaba sobre una máquina con sólo 256 Kb de RAM y que no superaba los 1.200 dólares (170.000 Ptas.). Si el Atari ST sería el primer ordenador con interfaz MIDI, el Amiga 1000 sería el primer ordenador multitarea de la informática doméstica (sólo el Lisa de Apple había incluido con anterioridad la multitarea pero su elevado precio lo alejaban de las posibilidades de un usuario doméstico).

Tanto Atari como Commodore se adelantaron a su tiempo con estas máquinas y, como Apple, osaron realizar propuestas distintas e incompatibles a las de los líderes del mercado. Tanto Atari como Commodore tuvieron graves problemas financieros y acabaron por extinguirse sin haber podido encontrar su hueco en el mercado. Sus posibilidades gráficas los convirtieron rápidamente en ordenadores ideales para jugar y vivieron sus últimas etapas considerados, básica e injustamente, como fabricantes de ordenadores para juegos. Pero esta fue una baza que se mostró insuficiente para poder sobrellevar el fuerte tirón de Windows en todos los ámbitos. Sólo Apple consiguió, ampliando sus campos de acción y consiguiendo atrincherarse en importantes nichos de mercado, sobrevivir de entre ese grupo de pioneros.

### QUIÉN ES...

### Commodore y Atari

Commodore Business Machines fue fundada en 1954 por Jack Tramiel en Canadá como servicio de reparación de máquinas de escribir. Al cabo de poco se pasó al negocio en auge de las calculadoras y a finales de los años setenta al de los ordenadores personales. En 1977 lanzó uno de los tres primeros ordenadores personales que aparecerían en el mercado listos para el usuario (ya no en forma de kit): el PET (Personal Electronic Transactor) (los otros dos serían el Apple II y el Tandy TRS-80). El PET utilizaba un procesador 6800 de Motorola y disponía de 8K de memoria. Al éxito del PET le sucedió en 1981 el VIC-20, el primer ordenador en color por debajo de los 300 dólares y del que Commodore llegó a vender 9.000 unidades diarias. En 1982 lanzó el Commodore 64 y en 1985 aparecería el Amiga cuyos principales modelos serían el 500, 1000 y 2000. La empresa desapareció en 1994.

Jack Tramiel, que en 1983 dejó Commodore para ponerse a las riendas de la división doméstica de Atari, legó a la industria mucho más que una serie de innovaciones tecnológicas. Tramiel, un agresivo empresario y pionero, dejó su impronta más preciada: que los ordenadores fueran potentes pero accesibles para cualquiera. Su máxima, con la que encabezamos este capítulo, "computers for the masses, not the classes" (ordenadores para las masas y no para las clases), ayudó notablemente a crear la expectativa de que la informática tiene que ser cada vez mejor y más barata. Con su empuje consiguió convertir a Commodore a finales de los años setenta en una de las empresas que alcanzaba los mil millones de dólares.

Atari Inc. fue fundada por Nolan Bushnell en 1972 y de inmediato consiguió un gran éxito con un popular juego que muchos recordarán, el Pong. En 1976, Bushnell vendería la empresa a Warner Inc. que posteriormente en 1984 la dividiría en dos: Atari Corp, que pasaría a dirigir el ex fundador de Commodore, Jack Tramiel y Atari Games/Tengen. La empresa lanzó en 1978 sus primeros ordenadores de 8 bits, el Atari 400 y el 800 a los que seguirían las series 1200XL y VCS5200 y las 600XL y 800XL en 1983. El declive del mercado de los vídeojuegos en esas mismas fechas junto con una mala gestión le llevarían a acumular sus primeras pérdidas hasta la citada división de la empresa en 1984. En 1985, Atari Corp lanzó el primer Atari ST, el 520ST y empezó la era de los ordenadores de 16 bit. y una buena época para la compañía. En 1988 sacó al mercado el Atari TT, el primer ordenador de 32 bits de la compañía; el Atari Portfolio, un ordenador de bolsillo; y el Atari Lynx, un sistema de vídeojuego portátil y en color. En 1993, Atari Games adquirido por Time-Warner se convirtió en Time-Warner Interactive y Atari Corp lanzó el Jaguar, la primera consola de vídeojuego de 64 bits y la última gran apuesta de la compañía. En 1995 las cosas parecían ir bien cuando se consiguió un acuerdo con uno de los grandes almacenes de Estados Unidos pero la falta de difusión de los productos (Atari estaba convencida de que los buenos productos no necesitan publicitarse) trastocó sus previsiones y en 1996 Time-Warner Interactive (la división de juegos de Atari) fue vendida a WMS y Atari Corp (la división doméstica) fue absorbida por la empresa JTS Corporation, un fabricante y distribuidor de discos duros. De la noche a la mañana la empresa y la marca Atari ya no existían dejando innumerables unidades por vender en stock y miles de usuarios desconcertados.

### Y TAMBIÉN...

### ¿Un Jaguar sin garra?

La última gran apuesta de Atari fue el sistema Jaguar que supuso la introducción, poco antes de las navidades de 1993, del primer sistema de juegos de 64 bits (a pesar de que Nintendo insista en que su sistema fue el primero de 64 bits). El Jaguar tuvo inicialmente un buen recibimiento entre los aficionados a los juegos pero si bien Atari entró el primero en la carrera tecnológica no tuvo ninguna opción en la otra gran batalla: la que se libra con las armas de la publicidad y el márqueting. La empresa dedicó todos sus recursos a tener un suficiente stock de unidades como para poder satisfacer una demanda que se preveía alta, especialmente gracias al acuerdo que se había llegado con una de las principales superficies comerciales del país (que había prometido distribuirlos en más de 400 puntos de venta). Pero en un sector como el de las vídeoconsolas en el que la demanda procede de los más pequeños, es crucial la publicidad y especialmente la aparición del producto en televisión.

### De Commodore a Atari

En 1983 la división de ordenadores domésticos de Atari pasó a ser dirigida por Jack Tramiel, ex-fundador de Commodore Business Machines. Una de las principales críticas a su gestión es que, habiendo vivido una experiencia como la del lanzamiento del Commodore PET en 1977 que fuera centro de atención de toda la prensa del país y de parte del mundo, Tramiel fue incapaz de comprender que, no estando Atari en esa misma situación, debía ser la empresa la que persiguiera a los medios de comunicación. Sin embargo, aún así, Tramiel consiguió introducir una innovadora línea de ordenadores de 16 bit a un precio accesible y lanzó las vídeoconsolas 7800. Pero toda la inventiva y creatividad que se puso en las máquinas a nivel tecnológico no tuvo su parangón a nivel de promoción y márqueting.

### ¿Usuarios de un fantasma?

En el mercado se producen a veces situaciones asombrosas y no es para menos el caso de Atari y Commodore. Ninguna de las dos empresas existe en el mercado y, sin embargo, los seguidores y usuarios de ambos sistemas son aún tan numerosos que se mantiene todo un mercado destinado a ellos tanto en el sector editorial convencional como en el de software (por no mencionar el activo mercado de hardware de segunda mano). Publicaciones para el Amiga, clubes de usuarios de Atari o juegos en CD-ROM para ambos entornos son sólo algunos ejemplos de un mercado que, aunque progresivamente va menguando, se resiste a desaparecer.

Fin Capítulo 14.<br>
[Volver a Indice](#indice)

<br><br>

### CAPITULO 15

## Bill Atkinson y la primera herramienta multimedia: HyperCard

_HiperCard ocupa el mismo nicho en la evolución del software que los seres humanos en la evolución de la vida._<br>Douglas Adams

Si hemos recordado a los primeros ordenadores multimedia, es decir, los primeros ordenadores capaces de mostrar gráficos y sonido, bien debemos recordar al primer programa que permitía la creación de multimedia desde cualquier ordenador doméstico. Aunque, bien pensado, HyperCard se escapa a cualquier definición estereotipada.

Una herramienta multimedia es toda aplicación que permite combinar texto, imágenes, audio y vídeo para presentar la información entrelazada en diversas formas. HyperCard, lanzado en 1987 para la plataforma Macintosh era, de hecho, mucho más que esto. HyperCard no era una aplicación gráfica, ni una base de datos, ni un procesador de texto pero incluía elementos de todos ellos junto con, lo más importante, funciones de hipertexto. Es decir, funciones para combinar todos esos elementos. En el momento de su aparición, sus desarrolladores la presentaron como un nuevo tipo de aplicación, un entorno de información único útil para buscar y guardar información (palabras, esquemas, imágenes, fotografías digitalizadas) de cualquier tipo y conectarla toda entre sí.

### Los conceptos de Bush y Nelson hechos realidad

Cuando Bill Atkinson, trabajando en Apple Computer, terminó de escribir MacPaint (la primera aplicación que se desarrolló para el Macintosh y que se suministraba junto con el ordenador), empezó a investigar si sería posible desarrollar una herramienta gráfica de hipertexto partiendo del código utilizado en MacPaint. Lo que buscaba Atkinson era la posibilidad de crear un entorno en el que el usuario se moviera con el ratón y que, por ejemplo, pudiera leer la leyenda de una imagen al hacer clic sobre ella. A medida que su trabajo avanzaba y el potencial de un medio interactivo como éste se le hacía más y más evidente, Bill Atkinson se dio cuenta de la necesidad de convencer a Apple para llevar a cabo el proyecto. Apple aceptó por lo atractivo de la idea pero también, y sobretodo, porque Dan Winkler, un programador de la casa, sugirió completar el programa con un lenguaje orientado a objeto y convertir así a HyperCard, además, en un entorno de programación. Apple dijo que sí sencillamente porque era justo lo que le faltaba al Mac. Los primeros ordenadores de IBM o incluso el Apple IIe se suministraban con compiladores de BASIC pero el Mac nunca había incluido ningún lenguaje de programación con él (algo coherente con su filosofía de ordenador, como decía la campaña, "for the rest of us", para el resto de nosotros, para los que no sabíamos ni teníamos necesidad de programar). Con el compilador de BASIC, los programadores (los usuarios también pero con mayor esfuerzo por la dificultad del lenguaje) podían escribir código original, rudimentario y de poca utilidad pero original. Con HyperCard los usuarios de Mac podrían ir mucho más lejos. No sólo se incluía un lenguaje de programación, el HyperTalk, sino todo un entorno para desarrollar presentaciones o interfaces interactivas y todo ello siguiendo la filosofía del Mac: haciendo que programar fuera fácil y sencillo.

Apple afirmaba que era una aplicación coherente con la estrategia de la empresa pues ponía al alcance de cualquier persona toda la potencia de la tecnología. Con este paquete los usuarios podían escribir sus programas, organizarse la información y utilizarla de modo completamente distinto porque su novedoso método de navegación permitía hojear y encontrar lo que se buscaba rápidamente en grandes bancos de información.

La insistencia de Atkinson también conseguiría que HyperCard se incluyera gratuitamente en todos los primeros Macs, una táctica que inicialmente contribuiría a vender más de uno de estos ordenadores. Durante el tiempo en que no existían herramientas de este tipo e HyperCard se regalaba con todos los Macs, los usuarios de estos ordenadores no sólo adquirían un ordenador con su compra sino todo un nuevo sistema de organización y edición de la información.

### Una integración de tecnologías de la información

Para entender cómo funcionaba y qué era HyperCard (qué es en presente, pues de hecho aún existe), debemos pensar en las ideas de Nelson y Bush, en el funcionamiento de Internet o de cualquier aplicación o herramienta multimedia actual. HyperCard permitía enlazar rápida y fácilmente todos los fragmentos de información que incluyéramos, fuera cual fuera su forma (texto, imágenes, sonido y ahora también animación) mediante "links", enlaces o conexiones, que nos conducían de un punto a otro de la información. No se necesitan grandes conocimientos de programación para crear aplicaciones con HyperCard, de hecho, cualquiera puede confeccionar un "stack", el nombre que reciben las aplicaciones en HyperCard (traducido a veces por "pila") y agrupar en ella gran disparidad de información. Estos "stacks" o aplicaciones incluyen uno o varios fondos y diversas tarjetas que comparten dicho fondo. Las tarjetas a su vez pueden contener botones, para que el usuario desencadene acciones, y campos, donde se introduce el texto que se va almacenando. Pero todo esto era posible en 1987, hace ya más de diez años, y en un momento en que no existía ni nombre para catalogar a este tipo de entorno-herramienta-aplicación.

Cómo Vannevar Bush había planificado para su máquina Memex, HyperCard era capaz de esquematizar todas las conexiones posibles en un mapa y mostrarnos una página con todos los vínculos creados.

En cierto aspecto era como un anticipo de la nueva era multimedia que se avecinaba y en la que todos tendríamos acceso completo a cantidades inmensas de información y tendríamos, además, la capacidad de manipular eficazmente todo este conocimiento acumulado. Para muchos fue, realmente, la confirmación de que los sueños de Vannebar Bush y Ted Nelson se hacían realidad.

### El fracaso de un éxito

En su momento, HyperCard fue una buena plataforma de creación multimedia, también llamada "de autor", para integrar elementos digitales y crear un entorno eficaz de manipulación. Era bastante potente y también lo suficientemente sencillo de utilizar, al menos al nivel más básico y, gracias a la estrategia inicial de incluirlo con todas las máquinas fue una aplicación bastante extendida. Sin embargo, a Apple le pasó con HyperCard algo similar a lo que le ocurrió a Xerox con los inventos del PARC. No supo o no pudo convertirla en un producto rentable y comercializable. Con una base instalada como la existente y la ventaja de salir con unos cuantos años de adelanto, a Apple le habría bastado con mejorar algo el producto y hacerlo disponible para las otras plataformas del mercado para convertirlo en un estándar. Pero la realidad fue otra bien distinta. A principios de los años noventa empezaron a aparecer herramientas de autor cada vez más potentes y sofisticadas e HyperCard quedó arrinconado. La última renovación del programa (realizada muy recientemente) tenía intención de hacerlo más competitivo y recuperarlo en un mercado en el que la creación de contenido multimedia se muestra como uno de los principales motores de tracción. Ciertamente, algunos desarrolladores siguen utilizándolo pero en muy menor proporción a las otras herramientas de autor más extendidas y populares. Sin embargo, a Bill Atkinson hay que reconocerle el honor de ser el creador de un nuevo tipo de aplicación y a Apple de haberlo sabido apoyar aunque finalmente, como ha ocurrido con la mayoría de visionarios, el mercado les acabaría dando la razón pero también les acabaría desbordando.

### QUIÉN ES...

### Bill Atkinson

Bill Atkinson trabajó en Apple Computer desde sus comienzos donde ayudó a "inventar" el Macintosh. Concretamente fue el autor de QuickDraw (las rutinas básicas que definirían la interfaz del usuario primero del Lisa y después del Mac) y de MacPaint (el primer programa escrito para el Macintosh y que se suministraba gratuitamente con él junto a otros programas). Atkinson llegó a Apple procedente de la Universidad de California, en San Diego, donde había sido alumno de Jef Raskin, el padre del Macintosh. En Apple empezó dedicado al Lisa para el cual desarrollaría QuickDraw, las rutinas gráficas que después se trasladarían al Mac, un trabajo por el que adquirió un merecido prestigio a partir de ese momento (buena parte de la personalidad y diseño de la interfaz del Mac se debe a él). Bill también fue el creador, entre otras cosas, de la fuente Venecia, uno de los tipos de caracteres que se suministraban con el Macintosh.

En 1990 abandonó Apple para fundar General Magic junto con Marc Porat y Andy Hertzfeld, ambos también procedente de Apple. Inicialmente, la empresa fue impulsada por Apple Computer como corporación independiente dedicada a desarrollar oportunidades de negocio más allá de la informática personal tradicional, pero acabó por independizarse del todo de ella. General Magic atrajo en seguida a numerosos expertos con amplia experiencia en software y comunicación y actualmente tiene como a principales socios a Apple, que conservó su inversión inicial, Sony, AT&T, Motorola, Matsushita y Phillips. La compañía, centrada exclusivamente en el campo de las comunicaciones personales del futuro, ha licenciado dos tecnologías de forma abierta a socios estratégicos que a su vez las aplican a productos concretos. Las dos plataformas licenciadas son Magic Cap, un completo software de comunicaciones fácil de usar y Telescript, un lenguaje de programación orientado a objeto que permite crear aplicaciones que funcionen como agentes digitales. Sin embargo, las últimas noticias que se tienen de este diseñador de software es que por fin ha podido dedicarse de pleno a lo que era su gran afición, la fotografía naturalista, que ahora elabora haciendo uso de las nuevas herramientas de fotografía digital.

### Y TAMBIÉN...

### HyperCard hoy

HyperCard sigue comercializándose en la actualidad. La última versión lanzada al mercado es la 2.3 y aún existe una pequeña comunidad de programadores que la utilizan profesionalmente para desarrollar sus programas (el popular juego de aventuras Myst y Lake Iluka, por ejemplo, fueron desarrollados con HyperCard) y un amplio número de usuarios que se inician con ella (su modesto precio está al alcance de cualquiera). Desde su aparición en 1987 han aparecido numerosos libros sobre HyperCard (algunos de los cuales aseguran ir por los 400.000 ejemplares vendidos) y se pueden encontrar diversos CD-ROM repletos de stacks de todo tipo. Sin embargo, y a pesar de la gran versatilidad y potencia de las últimas versiones, Apple nunca ha hecho una verdadera campaña de HyperCard para lanzarlo como lo que podía haber sido, una útil herramienta multimedia, y más bien parece mantenerlo por la fidelidad de sus usuarios que, a pesar de todo, siguen comprándolo y utilizándolo.

Fin Capítulo 15.<br>
[Volver a Indice](#indice)

<br><br> 

## CAPITULO 16

### Y el resto del mundo cambió: Windows e Internet

_Mejorar nuestra forma de vida es más importante que extenderla. Si lo hacemos de forma suficientemente satisfactoria se extenderá automáticamente._<br>Charles A. Lindbergh

Desde que en 1984 Apple creara el concepto de "interfaz humana" con el Macintosh han ocurrido muchas cosas pero tal vez la más importante sea el factor de "arrastre", el tirón, que ese concepto tuvo en el resto de la industria. Por interfaz humana entendemos aquella forma de interactuar con los ordenadores que además de utilizar una metáfora gráfica ­iconos representando elementos de nuestro entorno real, ventanas para trabajar en diversos planos, activación de las acciones pulsando botones, etc.­ está centrada en el comportamiento y hábitos del ser humano. La máquina responde a nuestras acciones según parámetros humanos, como el individuo espera según la lógica humana que las cosas reaccionen. Es algo así como enseñar a que las máquinas piensen como las personas. Para ello, para conseguir "humanizar" al ordenador, es necesario que éste siga unas pautas que en cierta manera los diseñadores del Star, en el PARC de Xerox, ya supieron adelantar como principios básicos de comportamiento de la interfaz: por ejemplo, y muy importante, la coherencia y la consistencia de la misma pero, también, la simplicidad, la claridad, la familiaridad, sencillez de uso, etc. Si la interfaz es la forma en cómo nos comunicamos e interactuamos con el ordenador, una interfaz "humana" es aquella que gira entorno a las necesidades del usuario y, como tal, del ser humano. Por ello, a este tipo de interactuación con las máquinas también se la ha definido como "amigable para el usuario" o "centrada en el usuario". La interfaz humana es pues la máxima expresión de la informática "humanizada" hasta nuestros días y la llave de la popularización de la misma. Desde su aparición consiguió arrastrar al resto de la industria de tal modo que en la actualidad, al menos en el mundo occidental o moderno, la mayor parte de las personas que interactúan con un ordenador lo hacen a través de algún sistema operativo basado en estos principios.

La universalización del uso del ordenador que permitió la popularización de este concepto se refleja principalmente a través de dos vías. La primera, la proliferación de sistemas operativos parecidos al del Macintosh o su precedente, el Star, y, la segunda, a través de ejemplos como la propia expansión de la red Internet.

### Del DOS a Windows

Diversos han sido los sistemas operativos que, en la última mitad de la década de los ochenta, empezaron a hacer uso de la metáfora gráfica y de alguna u otra adaptación del concepto de "interfaz humana", pero sin duda se debe principalmente a uno la gran popularización de la misma: nos referimos a Microsoft Windows. Microsoft también tuvo ocasión, como Apple, de visitar el PARC de Xerox y de presenciar en primera fila todos sus avanzados inventos. De hecho, a finales de 1981 ya empezó a desarrollar una primera versión del sistema operativo más popular en la actualidad. El "Interface Manager", como se denominaría Windows en un primer momento, pretendía aprovechar conceptos vistos en el PARC, como las ventanas inamovibles que enlosaban la pantalla del Star o sus menús abatibles y cuadros de diálogo. Sin embargo, Windows 1.0 no aparecería hasta 1985 y finalmente se inspiraría más en el Macintosh que en el Star de la Xerox (de hecho, Apple y Microsoft firmaron un acuerdo por el cual Apple permitía a Microsoft el uso de algunos de los principales conceptos de la interfaz del Mac, como las ventanas y los iconos, a cambio de que Microsoft desarrollara software para la recién nacida plataforma Macintosh). Microsoft, que en aquellos momentos dominaba el mercado de los sistemas operativos con el MS-DOS, no pudo empezar a desarrollar una interfaz gráfica hasta que el fabricante de los procesadores sobre los cuales funcionaba el DOS, Intel, desarrolló un chip capaz de ejecutar una interfaz gráfica.

Tras diversas actualizaciones de la primera versión, en exceso lenta, (Windows 2.0 y Windows/386), Microsoft lanzaría en 1990 Windows 3.0, un entorno de trabajo gráfico que funcionaba sobre el MS-DOS y que permitía a los numerosos usuarios de ese popular sistema seguir utilizando todos sus anteriores programas para DOS y disponer, por primera vez, de un sistema gráfico mucho más sencillo de usar. Las sucesivas actualizaciones de Windows (la 3.1 en 1992, la 3.11 en 1994 y Windows 95 en 1995, principalmente) no hicieron más que aumentar la expansión del sistema gráfico de Microsoft. Sin ánimo de intentar aquí analizar las cuestiones del gran éxito de Windows a partir de 1990, especialmente por la importantísima base de MS-DOS que sustentaría su avance, sí es de destacar que la proliferación de Windows abrió el acceso a la informática a muchas personas que antes no querían o no sabían como enfrentarse a sistemas no gráficos como el MS-DOS y, lo que es igualmente importante, permitió que muchas otras personas que ya utilizaban el anterior sistema operativo de Microsoft, pudieran aprovechar infinitamente más el potencial de sus ordenadores gracias a un sistema y programas mucho más intuitivos. Podríamos decir que el Star de Xerox esbozó la idea, el Macintosh de Apple definió el concepto y Microsoft Windows extendió su versión del mismo como la pólvora (lo cual tiene doble mérito pues la suya no es precisamente la mejor versión de interfaz gráfica que existe).

Pero la importancia de las interfaces humanizadas no sólo se plasma en los sistemas operativos de las máquinas y el mayor ejemplo de ello es sin lugar a dudas la enorme popularización que la red Internet ha tenido a partir de la adopción de una interfaz gráfica e intuitiva.

### Y la gran red también se humanizó

Internet fue creado inicialmente por la Defence Advanced Research Projects Agency (DARPA) del ministerio de defensa de los EE.UU. en 1969 para intercambiar conocimientos militares entre investigadores sitos en diversos centros del país. Posteriormente ampliada a otros organismos, no se popularizó para el gran público y abrió sus puertas al sector privado hasta muchos años después de su nacimiento y cuando sus objetivos se habían modificado substancialmente. Tradicionalmente montada sobre sistemas Unix, una tecnología potente pero arcaica y, en consecuencia, muy hostil para el usuario, la navegación por esta red formada por diversas redes independientes requería de ciertos conocimientos especiales sobre su manejo y funcionamiento. Ello era una dificultad añadida para poder abrir la red a todos los sectores de la sociedad.

Pero en 1989 Tim Berners-Lee, un ingeniero inglés de software, creó la World Wide Web, una red dentro de Internet que permite acceder a gran parte de su información a través de una interfaz gráfica. La traducción de "world wide web" sería "tela de araña de alcance mundial" porque la idea base que la sustenta es tejer enlaces entre los diferentes puntos de información. Las tres principales claves de la WWW es la navegación vía hipertexto, el soporte multimedia y la integración de los servicios preexistentes. Navegación por hipertexto porque enlaza la información mediante palabras clave destacadas que sólo tenemos que tocar para desplazarnos por los documentos; multimedia porque las páginas Web pueden incluir textos, gráficos, sonidos e incluso vídeo; e integración de los servicios preexistentes porque desde la WWW el usuario puede acceder a otros servicios de Internet que hasta ese momento requerían cada uno de su propia aplicación, servicios como Telnet, FTP, grupos de noticias, Gopher, etc.

El señor Berners-Lee ideó este sistema para poder comunicarse más fácilmente con otros físicos (el proyecto fue financiado por el CERN europeo) dada la complejidad del acceso a la red, pero en 1993 la aparición de una aplicación que permitía acceder a este entorno gráfico dentro de Internet la popularizaría más allá de las fronteras para la que habría sido inicialmente creada, nos referimos al popular Mosaic más tarde reconvertido en Netscape Navigator.

La expansión posterior de la red de redes y el acceso y democratización de la misma se la debemos en gran parte a la creación de esta interfaz gráfica que utilizan ahora millones de personas desde sus casas para acceder a ingentes cantidades de información mediante el sistema que Vannebar Bush, Douglas Engelbart y Ted Nelson ya nos habían descrito bastantes años atrás.

### Una tendencia dominante

Pero la humanización de las interfaces, o mecanismos de interacción con las máquinas, también se plasma en muchos otros dispositivos electrónicos que requieren de una interrelación con los usuarios. Un buen ejemplo son los asistentes personales digitales o PDA (Personal Digital Assistant) también denominados ordenadores de bolsillo. Estos aparatos vienen a ser la evolución lógica de las tradicionales agendas electrónicas, esas especies de calculadoras donde con muchos esfuerzos la gente lleva su agenda de teléfonos, su dietario semanal y algunas anotaciones. Estas agendas se pusieron de moda en los ochenta pero en la mayoría de los casos su programación era engorrosa y no precisamente amigable. En los noventa estas agendas han sido sustituidas por potentes herramientas que incluyen todas sus funciones y muchas más y que se caracterizan por unas interfaces gráficas que simulan escritorios de trabajo o blocs de notas. La interrelación con ellos ya no es mediante códigos y botones a pulsar sino mediante lápices que utilizamos para presionar sobre los iconos y menús de pantallas sensibles al tacto. Sus posibilidades son muchas, además de las tradicionales en las antiguas agendas electrónicas, pero aún más importante es la humanización de su interfaz. No hay ni un sólo modelo de PDA en el mercado cuya interfaz no pretenda (con mayor o menor éxito) plasmar alguna metáfora gráfica.

Pero el uso de interfaces humanas permite aumentar en tal modo el rendimiento y efectividad de los aparatos electrónicos que es de prever que estos se extiendan a muchos otros dispositivos como, por ejemplo, los electrodomésticos o los vehículos.

### QUIÉN ES...

### Microsoft Corporation<br>(Redmond, 1975 ­ ...)

Microsoft Corporation nació en 1975 fundada por William H. Gates y Paul G. Allen. Ambos crearon la empresa después de desarrollar en seis semanas un intérprete de Basic para el Altair, el primer ordenador personal comercializado y creado por Micro Instrumentation Telemetry System, de Nuevo México. Tras el éxito de este intérprete, Gates y Allen crearían la compañía, que inicialmente se dedicaría exclusivamente a los lenguajes. En 1980 ya habría lanzado dos más al mercado: Microsoft Fortran y Microsoft Cobol. En 1981 y a raíz de la petición de IBM de un sistema operativo que funcionara sobre el chip de Intel que el gigante azul había elegido para sus primeros ordenadores personales, Microsoft se lanzó también al mercado de los sistemas operativos con MS-DOS. Posteriormente la empresa también empezaría a desarrollar programas y juegos convirtiéndose a partir de 1990 en el principal desarrollador de software del mundo. Microsoft cuenta en la actualidad con más de 6.000 empleados y lidera por completo el mercado de los sistemas operativos con Microsoft Windows y buena parte del mercado del software con aplicaciones como Microsoft Word o Microsoft Excel. La sede de la empresa se encuentra en Redmond, Washington y su carismático líder, Bill Gates, es uno de los personajes más solicitados del planeta, además de ser el hombre más rico de los Estados Unidos.

### Y TAMBIÉN...

### El proyecto World Wide Web

La iniciativa de Tim Berners-Lee en 1989 daría lugar al proyecto World Wide Web del CERN. El proyecto definía la WWW como una iniciativa de recuperación de la información hipermedia destinada a dar acceso universal a un amplio espectro de documentos. Berners-Lee desarrolló el primer servidor y cliente Web y definió las especificaciones URL, HTTP y HTML mientras trabajaba en el CERN, el laboratorio de física de partículas de Ginebra. Berners-Lee se licenció en la Universidad de Oxford y actualmente es director del W3 Consortium, un foro abierto de empresas y organizaciones que tiene por misión expandir todo el potencial de la WWW. También es uno de los científicos del LCS (Laboratory for Computer Science) del Massachusetts Institute of Technology.

Fin Capítulo 16.<br>
[Volver a Indice](#indice)

<br><br>

## CAPITULO 17

### De la antiinterfaz a la interfaz humana pasando por la interfaz gráfica: Síntesis

_Sabemos lo que somos pero no lo que podemos ser._<br>William Shakespeare

El proceso que hemos intentado describir a lo largo de este libro tiene como principal y destacado protagonista a la interfaz. La interfaz es simplemente el sistema mediante el cual nos comunicamos con las máquinas. Existen interfaces, pues, desde que se inventó el primer artilugio mecánico pero, por ilógico que parezca, la interfaz ha recibido poca o nula atención hasta muy recientemente. En la historia de la informática, la reflexión sobre la forma en cómo interactuamos con el ordenador no ha empezado a adquirir relevancia hasta estos últimos diez años en que paulatinamente ha comenzado a ser objeto de atención preferente.

Los primeros ordenadores eran meras calculadoras gigantes y su principal finalidad era exclusivamente agilizar los cálculos masivos y reducir al mínimo la posibilidad de error. La evolución de la tecnología daría lugar al cabo de los años a chips más pequeños primero, que abrirían la posibilidad de máquinas más manejables, y a procesadores cada vez más potentes y con más capacidades, como las gráficas, después, que permitirían el nacimiento de sistemas más sofisticados. Sin embargo, el ordenador personal nacería heredando la mentalidad de las anteriores máquinas que servían para unas finalidades y un entorno muy concreto y cuyo diseño se encontraba anclado en los parámetros de la revolución industrial, es decir, en la mentalidad que exigía que fueran los usuarios los se adaptaran a las máquinas y no al revés.

Pero el advenimiento del ordenador personal, dirigido a un público mucho más variado, y sobre todo mucho más inexperto, convertiría en un tema crucial la forma en cómo este público se comunicaba con la máquina. La gran expansión que la informática personal vivió a partir del uso y proliferación de interfaces más humanas demostró cuan importante era el mecanismo de interacción y hasta que punto habían sido un freno las primeras interfaces.

### El fracaso de las primeras interfaces

Durante la primera década de vida del ordenador personal, la limitación de la propia tecnología junto con la falta de visión de la mayoría de fabricantes y desarrolladores desembocó en la creación de sistemas que no tenían en cuenta las necesidades de las personas. Durante prácticamente toda la década de los setenta y parte de los ochenta, la mayor parte de la industria se resistió a abandonar unos parámetros heredados de las primeras generaciones de ordenadores y subordinados a un hardware diseñado años atrás. Estos sistemas interactuaban con el ordenador a base de comandos crípticos y poco lógicos, y en consecuencia, difíciles de memorizar y de aplicar sin cometer errores. El resultado de ello era una reducción notable de la productividad potencial de los individuos y una gran cantidad de tiempo perdido dedicado a aprender el funcionamiento de los programas. Así se había funcionado hasta ese momento con los grandes ordenadores denominados mainframes y los miniordenadores aparecidos a posteriori. La propia concepción del ordenador personal no parecía ser motivo suficiente para que inicialmente la industria, al menos sus líderes, se planteara algún cambio al respecto. Estos primeros años crearon toda una generación de individuos reticentes a confiar en unas máquinas que les pedían tanto a cambio de, en aquellos días, aún tan poco. Los ordenadores se convirtieron para muchos en algo asociado a la dificultad, a máquinas para gente preparada, experta, y sobre todo, en una amenaza intrínseca a sus puestos de trabajo. El ordenador se veía, es visto aún por muchos, como una herramienta de progreso a costa de los individuos. Este creciente temor había sido generado por la dificultad de aprender a utilizar estos primeros sistemas, por esa larguísima y tortuosa curva de aprendizaje que exigía su manejo, y por la hostilidad que tales sistemas engendraba en las personas. En el fondo, se trataba exclusivamente de un estrepitoso fracaso de la interfaz.

Poco a poco, entre los avances del hardware y la persistencia de los primeros pioneros, la industria fue volcando su interés en facilitar la interactuación con las máquinas y el esfuerzo de humanizar los ordenadores se convirtió en una necesidad. Sin embargo, tan sinuosos comienzos dejarían su legado. Por un lado, numerosas personas quedarían al margen de la informática por miedo a enfrentarse a ella y a fracasar, o por ser incapaces de terminar felizmente el prolongado periodo de aprendizaje que habían iniciado. Otras quedarían "enganchadas" a la informática obscura y serían muy reticentes a pasarse a otros sistemas más humanizados después de haber dedicado tanto tiempo y esfuerzo a dominar los anteriores. Este primer fracaso de la interfaz crearía muchos enemigos de la informática, y lo que es peor, generaría toda una industria económicamente dependiente de estos sistemas y que se aferraría a ellos para amortizar sus inversiones. Entre las personas mayores aún podemos encontrar en la actualidad numerosos ejemplos de esta reacción "antiinformática".

### El ordenador como extensión de la mente humana

Muchos han sido pues los obstáculos que ha tenido que superar la concepción de una informática pensada para las personas y que, a pesar de ver sus frutos tan recientemente, ya fuera proclamada hace tantos años. Y, no obstante, sus postulados son más bien obvios: máquinas al servicio de las personas y no personas doblegadas a las exigencias de las máquinas, hacer accesible el poder de la informática a todo el mundo o el ordenador como electrodoméstico. Es como si hubiéramos estado inmersos en un callejón sin salida que nos ha hecho perder un tiempo precioso al empeñarnos en utilizar algo que no nos llevaba a ninguna parte y no nos permitía sacar lo mejor de nosotros.

Esta humanización, en fin, de la informática no es más que una democratización de los recursos. Volver a poner a la máquina donde le corresponde en su papel de herramienta, de medio y no de fin, es abrir las puertas del conocimiento al mundo entero. Y dando poder al usuario y no al programa estamos fomentando la imaginación de las personas que es la principal herramienta de la que dispone el ser humano. Fomentando esta imaginación los individuos se pueden enfrentar a los problemas actuales de muy distinta manera. Los discursos sobre la supuesta falta de puestos de trabajo o la usurpación de estos por parte de las máquinas desaparecerían. Bien al contrario, la gente se alegraría de que fueran las máquinas las encargadas no sólo de las tareas más duras físicamente sino de las más aburridas y repetitivas intelectualmente. ¿Qué hay de malo en que un ordenador se pueda encargar algún día de clasificar a diario la correspondencia que llega a Correos, de atender el servicio de información de las compañías telefónicas o de conducir un autobús de rutinario recorrido? Las personas, con su imaginación, encontrarán nuevas ocupaciones cada vez más creativas y acordes con las necesidades actuales de la sociedad. Con imaginación aprenderemos a resolver antes los problemas y podremos dedicar el tiempo sobrante a nuestro ocio particular. Lo cierto es que mucha gente se resiste a usar su imaginación por falta de práctica, comodidad o simplemente miedo al fracaso, algo fomentado por el uso de herramientas poco creativas como son las interfaces mal diseñadas o los medios de comunicación que exigen un comportamiento tan pasivo del usuario como la televisión. La finalidad última de las máquinas debería ser que fueran capaces de impulsar nuestra imaginación, que constituyeran una extensión real de nuestra mente y, para ello, es indispensable un buen diseño en los sistemas de interacción entre hombre y máquina.

### La interfaz actual

De las antiinterfaces que caracterizaron los primeros años de la informática personal (a las que denomino así por su habilidad por poner trabas a lo que era su principal objetivo: la comunicación entre usuario y ordenador) se pasó a mitad de los ochenta a las interfaces gráficas que conocemos en la actualidad. Pero la progresión podríamos sintetizarla en las siguientes fases.

En primer lugar aparecieron los sistemas basados en una simple representación de la metáfora gráfica. Sus elementos centrales eran el uso del ratón y el teclado para interactuar con el ordenador a través de iconos, menús y símbolos en la pantalla. La metáfora gráfica de la oficina en primer término y del escritorio de trabajo después se rebelarían como la mejor manera de familiarizar al usuario con el entorno informático. El Alto de Xerox se encontraría en este estadio. El Star, el Lisa y el Macintosh formarían parte de la siguiente evolución de la interfaz a la que se sumaría posteriormente Microsoft Windows y otros sistemas operativos, y naturalmente, la mayor parte del software diseñado para ellos. En la primera fase nace propiamente el concepto de interfaz gráfica del usuario, concepto que evoluciona y se perfecciona después con sistemas que son más que meras metáforas gráficas de un entorno familiar para el usuario y se optimiza especialmente con el paso de gráfico a humano. La interfaz humana es además intuitiva porque permite que el usuario se mueva por ella simplemente guiándose por sus sentidos. Los sistemas humanos, "human-centered", como dicen los americanos, son sistemas centrados en la persona.

Obviamente, el diseño de la interfaz está en constante evolución. En la actualidad existen numerosos organismos públicos y privados dedicados a estudiar la interacción entre seres humanos y ordenadores, especialmente en Estados Unidos pero también en Europa, organismos que tienen como primer objetivo desarrollar tecnologías que sirvan a las necesidades reales de las personas. Pero las guías básicas del diseño de las interfaces las encontramos ya en dos documentos con más de una década de antigüedad. El primero eran los principios básicos que los diseñadores del PARC de Xerox definieron para el Star en 1981 y el segundo son las directrices básicas diseñadas por Apple Computer pocos años más tarde para la interfaz del Macintosh. Puede decirse que la evolución de los parámetros de interacción hombre-máquina han seguido esas líneas básicas hasta ahora en que las nuevas necesidades de las personas y las nuevas posibilidades de la tecnología están empezando a ampliar los horizontes de los diseñadores de interfaces. Y es que los parámetros diseñados en la década de los ochenta han dejado de ser los más adecuados para la realidad existente a las puertas del siglo XXI.

En la actualidad, la interfaz gráfica del usuario ha invadido prácticamente todo el mercado de la informática personal tanto en lo que se refiere a sistemas operativos como en cuanto al software en general. De los diversos sistemas operativos gráficos que existen en estos momentos en el mercado destacan especialmente tres: Windows 95, en primer lugar, por ser el más extendido del mercado; el Mac OS de Apple, que puede considerarse como el segundo estándar del mercado a pesar de poseer sólo alrededor del 10% del mercado; y el OS/2, el sistema operativo propietario de IBM, en vías de extinción pero todavía, aunque escuálidamente, presente en el mercado. Existen otros sistemas operativos con una interfaz gráfica del usuario pero, a parte de algún caso aislado como el Be OS, todos están pensados para ser utilizados en ordenadores conectados a un servidor y funcionando como estaciones de trabajo. De entre ellos destacan los sistemas operativos de Sun o Silicon Graphics en segundo término y a un nivel mucho más avanzado, el NextStep, un sistema operativo diseñado por el exfundador de Apple, Steve Jobs, con una de las interfaces más elegantes y avanzadas, adquirida por Apple Computer a finales de 1996. Pero en el entorno doméstico, al menos en los países desarrollados, las plataformas dominantes son Windows, Mac y OS/2 siendo Windows, con mucho, la plataforma más extendida. Podemos encontrar alguna versión de Windows en más de las tres cuartas partes de todos los ordenadores personales.

Sin embargo, ni Windows ni el OS/2 nunca han tenido ni la consistencia ni la integración del Mac OS, dos de las principales características de la interfaz humana que Apple diseñara en 1984. La plataforma abierta que ha sido siempre Windows junto al hecho de que se trata de una interfaz diseñada a posteriori para montarse sobre el sistema operativo MS-DOS han impedido que gozara de las peculiaridades de un sistema diseñado desde su nacimiento como un entorno integrado. Los usuarios de Windows gozan de un aspecto global muy parecido al del Macintosh pero en absoluto de su consistencia. Sin embargo, incluso una interfaz como la del Macintosh necesita en estos momentos de una profunda revisión. En estos momentos, importantes empresas como Microsoft Corporation, Apple Computer o Sun Microsystems centran gran parte de sus esfuerzos e inversión en investigar como debe evolucionar la interfaz en colaboración con numerosos institutos y laboratorios de todo el mundo. En esta tarea participan tanto los diseñadores de hardware ofreciendo tecnologías más potentes, como los diseñadores de software creando aplicaciones más inteligentes e incluso los investigadores de la psicología humana, profundizando en sus conocimientos sobre el comportamiento humano. Conseguir que las personas trabajen más cómodamente con las máquinas y sean el máximo de productivas y creativas con ellas es su reto y el de todos nosotros en la medida en que somos consumidores de sus productos y podemos, de hecho debemos, hacerles llegar nuestras exigencias y necesidades.

### Y TAMBIÉN...

### "Human-Computer Interaction Research"

En estos momentos existen en todo el mundo gran cantidad de institutos y departamentos, especialmente de universidades, que dedican gran parte de su investigación a la interacción entre el hombre y la máquina, lo que los anglosajones denominan como "human-computer interaction". Los esfuerzos de todos estos centros se suman así a la propia investigación privada que realizan las empresas muchas veces en colaboración con estos institutos. Estados Unidos es, naturalmente, donde podemos encontrar más institutos dedicados a la investigación de la interfaz y la interacción con los ordenadores (prácticamente en todas las universidades de informática hay alguno). Los principales ejemplos son el Media Lab del Massachusetts Institute of Technology, el Human-Computer Interaction Institute de la School of Computer Science en la Universidad Carnegie Mellon, el Human-Computer Interaction Laboratory de la Universidad de Maryland, The Centre for People and Systems Interaction de la Universidad South Bank, el Interactive Systems Group del departamento de informática y ciencias de la información de la Universidad de Oregon, o el Human Computer Interaction Lab de la Universidad de Binghamton. El Palo Alto Research Center de Xerox edita además una publicación titulada Human-Computer Interaction. Pero también en Europa existe un gran interés al respecto. Una muestra de ello es, por ejemplo, Gran Bretaña donde podemos encontrar el Human Centered Systems Design de la Universidad de Plymouth, el Human Cognition Research Laboratory de la Universidad abierta o The Human-Computer Interaction Group del departamento de ciencias informáticas de la Universidad de York. También están el Groupe Interaction Homme-Machine del Laboratoire de Recherche en Informatique de la Universidad de Paris-sur en Francia o el Institut für Telematik de la facultad de informática de la alemana Universidad Karlsruhe.

Fin Capítulo 17.<br>
[Volver a Indice](#indice)

<br><br> 

## CAPITULO 18

### ¿Y después qué? Claves para entender el futuro

_En el futuro, la gente necesitará trabajar de forma natural en sus casas, en el ferrocarril, en los aviones y en los restaurantes de comida rápida, con acceso completo a su espacio de información y a sus amigos y compañeros de trabajo. Sus ordenadores se convertirán en parte de sus vidas, una extensión natural de sus mentes como un lápiz o un bolígrafo lo son de sus cuerpos. Necesitarán el tipo de entorno armonioso (...)_<br>Bruce Tognazzini.

Y lo único que queda ya por preguntarnos es a lo que hace referencia el título de este capítulo. ¿Y después qué? o más bien ¿y ahora qué? porque en pocas áreas el "mañana" se convierte tan rápidamente en "ahora".

La realidad es que al camino en pos de una tecnología amigable, humanizada, aún le queda un largo trecho por recorrer. A pesar de los innumerables avances que se han ido produciendo, la tecnología aún está a años luz de lo que debería ser (aunque nadie sabe a ciencia cierta cómo definir este "debería ser"). Es cierto que el paso más importante ya ha sido dado: reconocer que hay que trabajar por una tecnología centrada en el individuo. El problema ha sido también identificado y ampliamente definido: el hombre no debe adaptarse a la tecnología, es ésta la que está al servicio del ser humano y la que tiene que evolucionar en este sentido. Pero los ordenadores aun distan mucho de ser máquinas potentes pero simples, sofisticadas en cuanto a resultados posibles pero no en cuanto a uso.

### El futuro está a la vuelta de la esquina

También es cierto que empiezan a hacerse comunes en los ordenadores virtudes que hasta ahora parecían de ciencia ficción: el reconocimiento de voz (la interacción con el ordenador mediante el habla), las tecnologías por infrarrojos (el fin de los engorrosos cables), los sistemas o asistentes inteligentes (los llamados "ordenadores de bolsillo"), la concentración de todos los sistemas de comunicación en uno (el ordenador-teléfono-módem-fax-televisor), etc. Pero, posiblemente, lo que realmente nos depara el futuro está aún lejos de lo que el visionario más ferviente sea capaz, hoy por hoy, de imaginar. Igual que nuestros antepasados jamás imaginaron todo el mundo virtual que las redes de comunicación nos ofrecen cada día (correo electrónico, tiendas virtuales, conferencias y forums con gente de todo el mundo, contacto con diferentes culturas) posiblemente nosotros aún no seamos capaces de poder imaginar todo lo que se nos avecina.

Sin embargo podemos hacernos una idea. Especialmente a corto plazo. Imaginar cómo evolucionaremos en los próximos diez años es bastante factible, posiblemente sólo es cuestión de hacer una lista de todas las nuevas tecnologías de las que ahora se empieza a hablar o a perfilar, especialmente en el marco de las comunicaciones e Internet, y anticipar su aparición real en el mercado. Probablemente el ordenador del futuro más inmediato se encenderá con nuestra voz, nos recordará las tareas pendientes, nos corregirá cuando nos equivocamos, nos filtrará las llamadas telefónicas, estará conectado permanentemente con el resto del mundo y será nuestro colaborador más fiel y eficaz. De aquí a diez años las máquinas serán cada vez más potentes y dispondrán de mayor capacidad de almacenamiento y gestión de los datos con un menor coste y el ordenador personal se consolidará como el electrodoméstico más importante de la casa (en el momento de escribir este libro en el mundo occidental ya se estaban vendiendo más ordenadores personales que aparatos de televisión). Sin embargo, el plazo hasta el que somos capaces de imaginar es realmente muy corto, estos diez años inmediatos no son nada aunque verán nacer muchas nuevas tecnologías. Pero más allá se nos hace realmente difícil de precisar. Podemos deducir que seremos una sociedad absolutamente conectada e informada y con acceso inmediato a toda nuestra memoria colectiva, podemos argüir que el ordenador personal como tal desaparecerá para pasar a ser omnipresente en todos y cada uno de los dispositivos susceptibles de ser programados, manipulados u optimizados (más que "un ordenador" será un conjunto de ellos, una interminable lista de agentes inteligentes, como los denominan algunos, esparcidos por nuestro entorno). Podemos imaginar que en el futuro será inconcebible que todas las "unidades" con un chip procesador no estén permanentemente conectadas al espacio virtual y que estando siempre activadas se desconecten por si solas y se vuelvan a conectar por motivos de ahorro energético y racionalidad. Podemos suponer que las formas de transporte y comunicación actuales quedarán superadas e incluso podrían desaparecer ante la eficacia, efectividad y rapidez de las transmisiones electrónicas. Posiblemente todo, absolutamente todo lo que podamos hacer con un ordenador (con cualquier máquina con un procesador, bien sea la nevera, la cocina, la plancha o la impresora) será tan sencillo como pronunciarlo con nuestros labios: "arranca el procesador de textos", "abre una hoja en blanco", "escribe lo que te voy a dictar...", "pégale el membrete de la empresa e imprímelo en la impresora de la segunda planta". O "abre el reproductor de vídeo", "grábame la película del canal 4 de esta noche", "envíale una copia a Pedro" y "mañana recuérdame que la vea". Y aún podemos ir mucho más lejos e imaginar que la ciencia conseguirá hacer realidad los viejos sueños de las películas de ciencia ficción y podremos viajar en el tiempo (hacia adelante o hacia atrás), trasladarnos a miles de quilómetros en segundos (descomponiéndonos molecularmente), desarrollar personalidades y caracteres a medida (mediante técnicas de clonaje y perfeccionamiento) e invertir el tiempo de trabajo y ocio de la sociedad (trabajaremos dos o tres horas al día durante tres o cuatro días a la semana). Pero difícilmente podemos prever todos los cambios que la tecnología nos aportará por la sencilla razón de que en la actualidad no existen los parámetros ni elementos a partir de los cuales imaginarlas. Aún no se han inventado. Las limitaciones de la técnica actual y de nuestra propia mentalidad esculpida y modelada para una época y una sociedad concreta nos impiden superar ciertas barreras. Si a ello sumamos el vertiginoso crecimiento al que nos somete el progreso actual la dificultad aun es mayor porque el ritmo de los avances tecnológicos se sucede con tal rapidez que cada vez se requiere menos tiempo para conseguir avanzar un paso más y cada vez este paso es mayor, y nuestra capacidad para predecirlo a largo plazo menor.

Sin embargo, a pesar de ello hay algo que considero obligatorio para que el progreso no cese: la humanización creciente de la tecnología y especialmente de las máquinas más inteligentes de las que disponemos y que ahora denominamos ordenadores. Esta humanización vendrá en forma de simplificación, de conseguir que utilizar cualquiera de estas tecnologías sea sencillo y, aún mejor, divertido. Con el tiempo, las máquinas inteligentes, distribuidas por todos los rincones de nuestra vida, nos sonreirán, nos aconsejarán e incluso nos consolarán mejor que muchos humanos y utilizarlas será más fácil de lo que jamás hayamos imaginado. Y esta misma humanización de la tecnología servirá para anular los temores y miedos que muchos aún sienten por estos hijos del progreso. Aunque ello nos cueste, por el camino, algunos sinsabores, especialmente en el mal uso puntual de estos avances, algo de lo que difícilmente nos salvaremos.

### Un derecho universal

En un plano más cercano, el mayor problema y reto con el que nos enfrentaremos será el de democratizar todos estos avances. Las nuevas tecnologías indefectiblemente llegarán y madurarán pero, para que ello redunde en beneficio de todos, deben alcanzar al máximo número posible de personas, deberán popularizarse y extenderse como el teléfono, la televisión o la radio hicieron en su día. O aún más si cabe. En el mundo hay millones de familias cuyas restricciones llegan a privarles de bienes tan básicos como los alimentos y la educación pero, sin embargo, muchas de estas familias dedican gran parte de los escasos ahorros familiares a adquirir un televisor, el rey de la segunda mitad de este siglo. Durante el próximo siglo algo parecido ocurrirá con los ordenadores, o como quiera que pasemos a llamar a nuestras máquinas inteligentes. Las nuevas tecnologías tienen que ser consideradas un derecho de todos los ciudadanos del mundo, un derecho universal, porque en ellas se encontrará buena parte del progreso de toda nuestra sociedad.

Por ello, será injusto que millones de personas queden atrás con interfaces obsoletas y restrictivas, sistemas que no ayudan a pensar, que limitan sus capacidades intelectuales. Todos estos usuarios, la mayoría de países subdesarrollados, tienen derecho a participar del futuro en igualdad de condiciones y a poder utilizar sistemas que, como explicaban los protagonistas de los primeros capítulos de este libro, ayuden a aumentar el intelecto humano.

### Los factores que están cambiando el concepto de interfaz

Dos son los aspectos principales que más están contribuyendo actualmente a modificar el concepto de interfaz. Uno es la propia obsolescencia de los sistemas actuales y el otro es, como no, Internet.

Puede afirmarse que todas las interfaces del usuario que se utilizan prioritariamente en la actualidad fueron diseñadas hace diez años o más, y curiosamente han evolucionado muy poco desde entonces. En consecuencia, la mayoría de sus características fueron pensadas para las necesidades y el hardware de otra época. Todas han podido sufrir, y así ha sido, sucesivas operaciones de modernización, pero su núcleo básico sigue siendo el mismo que hace diez años y en su mayor parte los cambios sólo han sido cosméticos. Esto es aplicable en mayor o menor medida a los principales sistemas operativos actuales con una interfaa gráfica del usuario. Algunas de estas características básicas se han convertido en todo un lastre, en un obstáculo insalvable para avanzar en esta imparable carrera tecnológica en la que estamos inmersos.

Pero el aspecto que más está contribuyendo a redefinir la interfaz del usuario y que mayor peso jugará a partir de ahora es Internet. Su influencia pasa por tres puntos:

Primero por la necesidad de integración de los espacios de trabajo diarios del usuario con Internet.

Segundo por el paso de la metáfora del escritorio, una concepción de la informática centrada en el documento, a la metáfora del canal, una visión centrada en la red.

Y tercero por la necesidad de poder acceder a nuestro espacio de trabajo desde cualquier punto.

El primer punto, la integración del entorno de trabajo diario con la red, proviene de la necesidad de disponer de un único entorno, de unificar las aplicaciones productivas con el acceso a la información. Esto significa que desde cualquier aplicación con la que trabajemos habitualmente tenemos que poder acceder a los contenidos de Internet. Ello supone en suma la integración, en una, de las dos metáforas: la del escritorio, con la que funcionan los ordenadores personales, y la de la red, con la que funciona Internet. Las propuestas actuales al respecto son varias y están lideradas por empresas como Microsoft, Apple o Netscape. Las opciones van desde integrar el escritorio de trabajo y todas las aplicaciones productivas a Internet (Microsoft con Desktop Active) a integrar Internet al escritorio de trabajo (Netscape con Constellation) o pasar a trabajar mediante componentes separados o módulos a los cuales se integraría la red (Apple, por ejemplo, lo intentó con OpenDoc y Cyberdog).

Pero aún más importante que esta integración de la interfaz de Internet con la interfaz del espacio de trabajo habitual es el giro que da el flujo de información. Buena parte de las propuestas actuales entienden que, dado el ingente caudal informativo que recorre la red y el problema creciente del conocimiento acumulado, la metáfora debe tomar otra dirección en lo que se ha denominado la nueva "metáfora del canal". En las nuevas interfaces ya no debe ser el usuario el que acuda a buscar la información tanto dentro como fuera de su ordenador. Debe ser ésta la que le encuentre a él. La información debe fluir hacia el usuario siguiendo unos parámetros marcados por éste en función de sus intereses, necesidades y objetivos. La interfaz del futuro estará preparada para recibir un flujo informativo constante sobre las noticias que cada mañana debamos leer, para actualizar automáticamente nuestros documentos con datos variables sitos en Internet y para recoger puntualmente de la red las nuevas actualizaciones de los programas con los que trabajamos. Deberán inventarse las aplicaciones que nos filtren y recojan la información y que, en definitiva, realicen una selección personalizada de ésta.

El tercer aspecto en que Internet está influenciando al diseño de la interfaz del futuro es en la posibilidad, que la propia red por sus características ofrece, para acceder a la información desde cualquier lugar. La integración del escritorio con Internet o de Internet con el escritorio, como se prefiera, abre a su vez el acceso a nuestro entorno de trabajo estemos donde estemos y desde cualquier máquina. Otra de las propuestas que se están pronunciando dibuja un panorama independiente de la máquina. La posibilidad de acceder a nuestro propio entorno de trabajo desde cualquier punto se basa en la idea de que nuestro ordenador ya no estará conectado a la red sino que estará "en" la red. Ello plantea una interfaz independiente de la plataforma y, por lo tanto, la necesidad de unificar los diferentes sistemas para universalizar este acceso y minimizar las curvas de aprendizaje.

### Personalizar, o una interfaz distinta para cada uno

Otra megatendencia en el diseño de las interfaces es la personalización del entorno de trabajo. Las experiencias sufridas hasta ahora con los sistemas actuales ha demostrado que no existe una interfaz ideal para nadie. Existen, en cambio, distintas necesidades para distintos grupos de personas. Los niños, los estudiantes, los profesionales, la gente mayor y los científicos, por poner algunos ejemplos, requieren de formas diferentes de interactuar con las máquinas y la interfaz del futuro debe ser maleable hasta este punto. La interfaz debe serlo todo: sencilla y sofisticada. Debe permitir simplificar al máximo sus funciones para no confundir a personas inexpertas o de corta edad y al mismo tiempo debe ser lo suficientemente sofisticada como para que los que requieran de toda su potencia puedan aprovecharla. Esto no tiene porque significar restringir a unos y dar más a otros sino ser capaces de crear sistemas flexibles para cada caso y cada momento (una misma persona puede requerir de distintas interfaces según actividades o etapas vitales).

### Romper con lo "normal"

Y, finalmente, debemos ser capaces de romper con las ideas tradicionales de lo que se considera "normal" a la hora de utilizar un ordenador o máquina inteligente de cualquier tipo. Algunos investigadores actuales postulan al respecto por la búsqueda de alternativas para sustituir al ratón y los iconos tal y como los conocemos ahora y por una visión menos centrada en el teclado. El concepto de movimiento por la pantalla del ordenador realmente es casi idéntico hoy en día al del primer prototipo de sistema gráfico con ventanas y ratón que Douglas Engelbart presentara en 1968. Cierto es que se ha sofisticado mucho pero su esencia es la misma y seguimos dependiendo fuertemente del uso del teclado. El replanteamiento de los conceptos básicos de la interfaz gráfica actual vendrá sin duda impulsado por los avances tecnológicos pero cabe pensar que estos se pueden adelantar, o incluso modificar, si nosotros conseguimos imaginar formas más lógicas e intuitivas de interactuar con las máquinas (desde las que ya son casi una realidad como con la voz, hasta las más inverosímiles como gesticulando con el cuerpo, con expresiones faciales, o con el pensamiento).

Las nuevas formas de acceso a la información, la integración de nuestro entorno de trabajo cotidiano a ellas, la necesidad de personalizar los espacios de trabajo y la obligación de replantearnos muchos de los mecanismos de funcionamiento actuales son sólo algunas de las claves que esconden lo que algunos han dado en llamar la "próxima revolución", la que supere el concepto actual basado, en el fondo, en una metáfora imaginada hace más de veinte años.

Fin Capítulo 18.<br>
[Volver a Indice](#indice)

<br><br>

## CRONOLOGIA

### Fechas para recordar

- **Abaco.** Tablero que se utilizaba para realizar operaciones matemáticas mediante bolas que se trasladaban de un extremo a otro de unas cuerdas. Fue utilizado durante más de 2.000 años hasta la aparición de las primeras calculadoras.

- **1642: Primeras calculadoras.** Blaise Pascal construye la primera calculadora aunque sólo era capaz de realizar sumas. En 1791 Gottfried Wilhelm von Liebniz desarrolló un modelo que ya podía multiplicar.

- **1820: Primera calculadora electrónica.** Tomas of Colmar inventa la primera calculadora mecánica que sería un éxito comercial. Este aparato sumaba, restaba, multiplicaba y dividía.

- **1823: Máquinas diferencial y analítica.** Charles Babbage construyó un artefacto completamente automático que definió como motor diferencial y que funcionaba a vapor e imprimía los resultados en tablas siguiendo una serie de instrucciones. En 1833, construiría un modelo más avanzado que denominaría analítico y que ha sido considerado como el primer ordenador digital mecánicamente automatizado. La máquina se controlaba completamente mediante programas e imprimía sus resultados en fichas perforadas.

- **1930: El integrador diferencial.** Vannebar Bush construyó un integrador diferencial, un paso intermedio electromecánico a camino de los ordenadores de tarjetas perforadas dominantes y el ENIAC, el primer ordenador electrónico.

- **1935-40: Consolidación de las tarjetas perforadas.** Hacia finales de 1930, las calculadoras u ordenadores con tarjetas perforadas alcanzaron cierto grado de consolidación y puede decirse que eran el estándar, un estándar liderado por una empresa que erigiría a partir de ello todo un imperio: IBM.

- **1942: El ENIAC.** Presper Eckerts y John W. Mauchly de la escuela Moore de Ingeniería eléctrica de la Universidad de Pensilvania desarrollan su propio integrador diferencial que acabaría resultando la primera máquina electrónica, el ENIAC (Electronic Numerical Integrator and Computer). Primer ordenador electrónico considerado la primera generación de ordenadores. Utilizaba tarjetas perforadas, tubos, también llamados válvulas, al vacío y pesaba treinta toneladas.

- **1945: Vannebar Bush publica "As we may think".** Ensayo crucial que inspiraría a muchos científicos posteriores y en el cual Bush preconizaba la nueva forma de abordar la consulta, almacenamiento y recuperación de la información ante la creciente cantidad de conocimientos acumulados por la humanidad. Bush fue capaz de imaginar conceptos tan actuales como navegar por un ciberespacio de información, disponer de enlaces de conexión entre informaciones relacionadas y utilizar el pensamiento asociativo.

- **1947: Nace el transistor.** William Shockley de los laboratorios telefónicos Bell inventa el transistor, pequeños pedazos de un material semiconductor que sustituirían a los tubos al vacío y posibilitarían la creación de ordenadores más pequeños y baratos.

- **1963: Ivan Sutherland diseña el Sketchpad como tesis doctoral.** A principios de la década de los setenta, Ivan Sutherland diseñó el primer programa de dibujo para ordenador. El Sketchpad demostraría la potencial versatilidad de los ordenadores.

- **1965: Ted Nelson acuña el término hipertexto.** A mediados de la década de los sesenta, Nelson inventa el término Hipertexto para definir una forma de escritura no secuencial que entrelaza el texto y permite trazar al lector su propio camino a través de la información. La mayor expresión de este concepto se encuentra en la actualidad en la World Wide Web de Internet.

- **1968: Douglas Engelbart demuestra el primer sistema gráfico.** Primera demostración pública de un sistema operativo con ventanas por entre las cuales se navegaba mediante un teclado y un ratón. Douglas Engelbart había creado el Augmentation Research Center en el seno del Stanford Research Institute, un centro de investigación basado en la necesidad de incrementar el intelecto humano. En este centro nacerían los principales conceptos con los que trabajan todos los sistemas operativos gráficos actuales ­las ventanas, el ratón, el trabajo en red, el correo electrónico­ y en los que se inspiraría el PARC, nutrido en parte con algunos investigadores procedentes del SRI de Engelbart, como por ejemplo Bill English, coautor junto con Engelbart del invento del ratón.

- **1970: Xerox crea el PARC.** Xerox funda el Palo Alto Research Center, un centro que aglutinaría a los cerebros informáticos más destacados del mundo y del que surgirían buena parte de los conceptos con los que trabajan los ordenadores modernos. La función de este centro no era poca: inventar el futuro.

- **1970: Alan Kay funda el Learning Research Group.** En este centro fundado en el PARC (Palo Alto Research Center), Kay estudiará la forma de facilitar el uso de los ordenadores y de implementar su visión de lo que debería ser un ordenador personal, el DynaBook.

- **1971: Nace el microprocesador.** Intel inventa el microprocesador también denominado circuito integrado o chip y que permitiría el nacimiento de la informática personal. En 1972, el primer chip capaz de procesar 8 bits, suficiente para representar números y letras, sería el disparo de salida. Con la posibilidad de colocar todos los circuitos en un sólo chip y la capacidad de manejar números y letras nacería la cuarta generación de ordenadores.

- **1971: Alan Kay imagina la metáfora del escritorio.** Buscando la manera de reducir la hostilidad que generaban los interfaces de los ordenadores del momento, Alan Kay inventaría la idea de representar un escritorio de trabajo en la pantalla como forma de interactuar con ellos. Tras sucesivas mejoras y lavados de cara, puede decirse que actualmente aún trabajamos basados en esta metáfora.

- **1973: Nace el Alto en el PARC.** El PARC plasma todos sus inventos en un primer ordenador que denominará el Alto pero cuya comercialización será inviable. El Alto será el primer ordenador con un interface gráfico del usuario pero no saldría del laboratorio.

- **1975: Aparece el Altair 8800.** Primer ordenador personal que sería un éxito comercial. Se vendía en forma de kit, es decir, con todos sus componentes por separado. El usuario tenía que montarlo y hacerse sus propios programas, y aunque no disponía de un interface gráfico ni nada que se le pareciese, tuvo el honor de ser el primer ordenador personal en comercializarse con éxito y motivaría a toda una generación de futuros informáticos.

- **1975: Nace el Homebrew Computer Club.** Club formado por jóvenes entusiastas de la tecnología nacido a raíz de la aparición del Altair. De su seno saldrían algunos de los grandes empresarios e impulsores de la siguiente generación informática, como los creadores de Microsoft o Apple Computer, entre otros.

- **1978: Aparece VisiCalc, la primera hoja de cálculo.** Dan Bricklin y Bob Frankston desarrollan el primer programa en plasmar la noción de la metáfora. Se trataba de una hoja de cálculo diseñada inicialmente para el Apple II y que ayudaría a vender un montón de estas máquinas y a introducir el ordenador personal en las pequeñas y medianas empresas.

- **1981: Aparece el Star.** El PARC lanza su segundo producto en forma de un ordenador que pretendía superar los problemas del Alto pero que tampoco conseguiría rentabilidad comercial. El Star era aún excesivamente caro para lo avanzado de su planteamiento pero definiría buena parte de los conceptos utilizados en el diseño posterior del interface gráfico del usuario.

- **1981: IBM entra en el mercado de los ordenadores personales.** IBM lanza el PC, siglas de "personal computer" pero que pasarían a ser sinónimo del ordenador personal basado en un procesador Intel. El gigante azul, como se conoce popularmente a esta empresa, decidió entrar en el mercado del ordenador personal ante la erosión que su principal área de negocio , la de los mainframes, estaba sufriendo. Sin embargo, el lanzamiento de una arquitectura abierta con un sistema operativo licenciable, el MS-DOS, llevó a la explosión de un mercado de compatibles IBM, los clónicos, que a partir de la segunda década de los ochenta reducirían enormemente la cuota de mercado del propio creador del PC.

- **1983. Apple presenta el Lisa.** Apple lanza su primer intento de revolucionar la informática personal. El Lisa sería el primer ordenador en adoptar gran parte de las innovaciones desarrolladas en el PARC de Xerox. Sin embargo, su elevado precio y adelantado planteamiento no permitió que triunfara como merecía.

- **1984: Aparece el Macintosh de Apple.** Apple revolucionó el mercado de la informática personal con este ordenador que ha sido fuente de inspiración para el resto de la industria. El Macintosh fue el primer ordenador con un interface gráfico del usuario en triunfar en el mercado. Su facilidad de uso y encanto lo convirtieron en un producto único y revolucionario. El Macintosh no sólo implementaría un interface gráfico como el inventado en el PARC sino que sería además un sistema "human-centered", centrado en el individuo, intuitivo y humano.

- **1985: Aparece el Atari ST.** Atari lanza el primer ordenador con interface MIDI (Musical Instrument Digital Interface) incorporado que se convierte automáticamente en un estándar en el sector de la informática musical. El ST incorporaba además del interface MIDI, gráficos en color y ratón por menos de 400 dólares.

- **1985: Commodore presenta el Amiga 1000.** El Amiga fue el primer ordenador personal multitarea (permitía realizar más de una tarea simultáneamente) y, junto con el Atari ST, puede considerarse el introductor de la multimedia en el mercado doméstico.

- **1987: Nace la primera herramienta multimedia: HyperCard.** Bill Atkinson desarrolla HyperCard para el Macintosh. HyperCard era un nuevo tipo de aplicación, un entorno de información único y útil para buscar y guardar todo tipo de información (texto, imágenes, sonido, etc.) y conectarla entre sí. Fue la primera herramienta de autor cuando este término aún no se había ni acuñado.

- **1987: IBM presenta el OS/2.** Sistema operativo con un interface gráfico desarrollado por Microsoft para IBM. Durante meses se polemizó sobre su retraso en ser lanzado al mercado pues Microsoft estaba trabajando paralelamente en su propio sistema gráfico, Windows, que finalmente apareció primero aunque no triunfaría en el mercado hasta la versión 3.1. El OS/2 se demostraría más potente y sofisticado que el propio Windows.

- **1989: Time Berners-Lee diseña la World Wide Web.** Berners-Lee, ingeniero inglés de software, creó la WWW para acceder al contenido de Internet a través de un interface gráfico y facilitar así el intercambio de información entre científicos. Este ingeniero, que trabajaba en el CERN en Ginebra, fue el creador de los estándars URL, HTML y HTTP entre otros.

- **1990: Microsoft lanza Windows 3.0.** Primera versión del interface gráfico de Microsoft en popularizarse. Windows aparecería por primera vez en 1987 pero su excesiva lentitud y gran inestabilidad no permitirían que se extendiese en el mercado ni que los desarrolladores apostaran por él. Con la versión 3.0, un rediseño completo de la anterior con un interface mucho más potente, Windows empieza su camino hacia el éxito (a los cincuenta días de salir al mercado ya había conseguido vender un millón de copias). La siguiente versión, la 3.1, sería su consolidación definitiva y la más extendida hasta la actualidad en que está siendo rápidamente sustituida por Windows 95.

- **1993: Aparece Mosaic.** Mosaic fue el primer browser u hojeador de la World Wide Web. Su aparición gratuita en el mercado supuso la popularización mundial de esta gran red y el comienzo de su acceso masivo y universal. Fue desarrollado por Marc Anderseen y sus compañeros del NCSA que más tarde fundarían la empresa Netscape Inc. popular actualmente por su browser Netscape Navigator.

- **1995: Microsoft lanza Windows 95.** Versión mejorada de Windows que se parece aún más si cabe al sistema operativo del Macintosh. Su lanzamiento en agosto de 1995 se enmarcó en una inmensa campaña de márqueting que sorprendió incluso a sus propios creadores.

- **1997: Apple lanza el Mac OS 8.** La última versión del sistema operativo de Apple no gozó de la campaña de márqueting de Windows 95 pero le ha situado otro paso por delante del anterior, a pesar de todos los problemas por los que ha pasado la compañía.

Fin Cronología.<br>
[Volver a Indice](#indice)

<br><br>

## BIBLIOGRAFIA

- **Babbage, Charles.** "Passages from the Life of a Philosopher".<br>Martin Campbell-Kelly Ed., NJ: IEEE Press, 1994.

- **Bewley, W.L. y T.L. Roberts, D. Schroit y W.L. Verplank.** "Human factor on the design of Xerox's 8010 Star office workstation".<br>En Proceedings of CHI’83, - **1983.

- **Borst, Arno.** "The Ordering of Time: From the Ancient Computus to the Modern Computer".<br>Chicago, IL: University of Chicago Press, 1993.
 
- **Burke, Colin.** "Information and Secrecy: Vannevar Bush, Ultra, and the Other Memex".<br>Metuchen, NJ: Scarecrow Press, 1994.

- **Bush, Vannevar.** "As we may think".<br>En The Atlantic Monthly, julio 1945.

- **Buxton, H.W.** "Memoir of the Life and Labours of the Late Charles Babbage Esq".<br>MIT Press, Cambridge, Massachusets, 1988.

- **Caddes, Carolyn.** "Portraits of Success: Impressions of Silicon Valley Pioneers".<br>Tioga Publishing Co., Palo Alto, CA, 1986.

- **Clapp, Doug y otros.** "The Macintosh Reader".<br>Random House Electronic Publishing, New York, 1992.

- **Clough, Bryan y Paul Mungo.** "Los piratas del chip. La mafia informática al desnudo".<br>Ediciones B, Barcelona 1992.

- **Cringely, Robert X.** "Accidental Empires".<br>Williams Patrick/Addison Wesley, Reading MA, 1992.

- **Engelbart, D.C. y W.K. English.** "A Research Center for Augmenting Human Intellect".<br>En Proceedings of the FJCC 33 (1968).

- **Engelbart, D.C.** "A conceptual Framework for the Augmentation of Man's Intellect".<br>En Vistas in Information Handling. Londres, VI Spartan Books, 1963.

- **Greenia, Mark W.** "Computers and Computing: History of Computing, A Chronology of the People and Machines that Made Computer History".<br>(En disquete), Lexikon services publishing, Mayo 1995.

- **Henderson, T.B. y D.F. Cobb, G.B. Cobb.** "Spreadsheet Software from Visicalc to 1-2-3".<br>Indianapolis, Que Corp, 1983.

- **Higgins, Steve.** "3COM founder reflects on past, future".<br>En PC Week, junio 18, 1990.
 
- **Hyman, Anthony.** "Charles Babbage: Pioneer of the Computer".<br>Prenceton University Press, Princeton, 1983.
 
- **Johnson, Jeff y otros.** "The Xerox Star: A retrospective".<br>En Computer 22, nº 9 (setiembre 1989).

- **Kay, A.** "Inventing the Future (Computer Industry)".<br>En AI Business: The Commercial Uses of Artificial Intelligence, Cambridge, MA: MIT Press, 1984.
 
- **Kerns, D.T. y David A. Nadler.** "Prophets in the Dark".<br>New York, HarperCollins Publishers, 1992.
 
- **Lazere, Cathy, y Dennis Sasha.** "Out of their minds: the lives and discoveries of 15 great computer scientists".<br>Dub-Copernicus, New York, 1995.
 
- **Lee, J.A.N.** "Computer Pioneers".<br>Los Alamitos, Ca: IEEE Computer Society Press, 1994.

- **Levitus Bob y Michael Fraase.** "Guide to the Macintosh underground. Mac culture from the inside".<br>Hayden Books, Indianapolis, 1993.

- **Levy, Steven.** "Hackers: Heroes of the Computer Revolution".<br>Anchor Press/Doubleday, Garden City, NY, 1984.

- **Levy, Steven.** "Insanely Great".<br>Penguin Books, New York, 1994.

- **Linzmayer, Owen.** "The Mac Bathroom reader".<br>SYBEX, San Francisco, 1994.

- **Metcalfe, Robert M.** "How Ethernet Was Invented".<br>En Annals of the History of Computing, 16:4 (1994).

- **Nelson, Ted.** "Computer Lib/Dream Machines".<br>Microsoft Press, 1987.

- **Nelson, Ted.** "Literary Maachines".<br>Edición electrónica de OWL International, Inc. Bellevue Washington, 1987.

- **Nelson, Ted.** "On the Xanadu project".<br>En Byte volumen 15 (setiembre 1990).
 
- **Norman, Donald A.** "The Design of Everyday Things".<br>Basic Books, 1988.

- **Nyce, James M. y Paul Kahn.** "From Memex to Hypertext: Vannevar Bush and the Mind's Machine".<br>En Academic Press, 1991.
 
- **Moseley, Maboth.** "Irascible Genius: a Life of Charles Babbage, Inventor".<br>Hutchinson, Londres, 1964.
 
- **Pitta, Julie.** "Who and What Made Park an Industry Legend".<br>En Los Angeles Times, September 13, 1995.

- **Preece, Jenny.** "Human-Computer Interaction".<br>Addison-Wesley, 1994.

- **Prevost Babbage, Heny.** "Babbage's Calculating Engines: A Collection of Papers".<br>Tomash, Los Angeles 1982.

- **Ryan, Bob.** "Dynabook Revisited with Alan Kay".<br>En Byte, vol 16, febrero 1991.
 
- **Saffo, Paul.** "The Alto: Today's Technology Yesterday".<br>En Personal Computing, June 1989.

- **Santa Clara Valley Historical Association.** "The making of Silicon Valley: A one hundred year renaissance".<br>Ward Winslow Editor, 1995.
 
- **Shneiderman, B.** "Designing the User Interface: Strategies for Effective Human-Computer Interaction".<br>2ª Edición, Addison-Wesley, 1992.

- **Slater, Robert.** "Portraits in Silicon".<br>MIT Press, Cambridge MA, 1987.
 
- **Shirley, Robin.** "Altair and After: The Original PC Revolution".<br>En Computer Resurrection: The Bulletin of the Computer Conservation Society, 5 (1993).
 
- **Smith, D.C. y otros.** "Designing the Star User Interface".<br>En Byte , Abril 1982.
 
- **Smith, D.C. y otros.** "The Star User Interface: An overview".<br>En AFIPS, 1982, National Computer Conference.

- **Smith, Douglas K. y Robert C. Alexander.** "Fumbling the Future: How Xerox Invented, Then Ignored, the First Personal Computer".<br>Morrow, New York, 1988.

- **Sutherland, Ivan E.** "Sketchpad - A Man-Machine Graphical Communication System".<br>En MIT Lincoln Laboratory Technical Report 196, 1963.

- **Tesler, Larry.** "The Legacy of the Lisa".<br>En MacWorld (Setiembre 1985).

- **Tognazzini, Bruce.** "Tog on software dessign".<br>Addison Wesley Developers Press, New York, 1996.

- **Zachary, G. Pascal.** "The Godfather".<br>En Wired, noviembre 1997.
 
- **Zientara, Marguerite.** "History of Computing: a Biographical Portrait of the Visionaries who Shaped the Destiny of the Computer Industry".<br>CW Communications Inc., Framingham, Massachusetts, 1981.
 
- **Young, Jeffrey S.** "Steve Jobs: The Journey is the Reward".<br>Scott, Foresman and Co., Glenview IL. 1988.
 
### En Internet:

- "As we may think" de Vannebar Bush. [link](https://web.archive.org/web/20160207203613/http://iibi.unam.mx/~voutssasmt/documentos/Vannevar_Bush_Como%20podriamos%20_Pensar_JV.pdf)

- "Project World Wide Web", el proyecto del CERN a iniciativa de Tim Berners-Lee. [link](https://info.cern.ch/hypertext/WWW/TheProject.html)

- "Human factor on the design of Xerox's 8010 Star office workstation", los principios de trabajo de W. L. Bewley, T. L. Roberts, D. Schroit y W.L. Verplank en el PARC. [link](http://billverplank.com/bewley83.pdf)

- "Xanadu", el proyecto basado en el hipertexto de Ted Nelson. [link](https://www.xanadu.net/)

- "Richmond Journal of Law & Technology" publicación con abundante información sobre el litigio entre Apple y Microsoft por el diseño del interface de Windows

Fin Bibliografía.<br>
[Volver a Indice](#indice)

<br><br>
